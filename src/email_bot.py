import imaplib
import email
from email.header import decode_header
import smtplib
from email.mime.text import MIMEText
import datetime
import os
import json
import hashlib
import time
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
from bs4 import BeautifulSoup

# --- æ ¸å¿ƒé…ç½®åŒº ---
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
API_KEY = os.environ.get("LLM_API_KEY")
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"
HISTORY_FILE = "data/history.json"

# ç¡…åŸºæµåŠ¨é…ç½®
BASE_URL = "https://api.siliconflow.cn/v1"
MODEL_NAME = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"

# é‚®ä»¶ç™½åå•
TARGET_SUBJECTS = ["æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", "new research", "Stork"]

# åˆå§‹åŒ– API å®¢æˆ·ç«¯
client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
cr = Crossref()

# --- è¾…åŠ©å‡½æ•° ---

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(history_list):
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history_list, f, indent=2, ensure_ascii=False)

def get_unique_id(source_data):
    if source_data.get("id"):
        return source_data["id"]
    elif source_data.get("url"):
        return hashlib.md5(source_data["url"].encode()).hexdigest()
    return None

def get_oa_link_from_doi(doi):
    """åˆ©ç”¨ Unpaywall API æŸ¥æ‰¾ DOI æ˜¯å¦æœ‰å…è´¹ PDF"""
    try:
        email = "bot@example.com"
        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email={email}", timeout=5)
        data = r.json()
        if data.get('is_oa') and data.get('best_oa_location'):
            return data['best_oa_location']['url_for_pdf']
    except:
        pass
    return None

# --- å¤šç›®æ ‡æå–å™¨ ---
def detect_and_extract_all(text):
    results = []
    seen_ids = set() 

    # 1. ArXiv ID
    for match in re.finditer(r"(?:arXiv ID:|arxiv\.org/abs/)\s*(\d+\.\d+)", text, re.IGNORECASE):
        aid = match.group(1)
        if aid not in seen_ids:
            results.append({
                "type": "arxiv",
                "id": aid,
                "url": f"https://arxiv.org/pdf/{aid}.pdf"
            })
            seen_ids.add(aid)

    # 2. DOI
    for match in re.finditer(r"doi:\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.IGNORECASE):
        doi = match.group(1)
        if doi not in seen_ids:
            oa_url = get_oa_link_from_doi(doi)
            results.append({
                "type": "doi",
                "id": doi,
                "url": oa_url 
            })
            seen_ids.add(doi)

    # 3. Direct PDF
    for match in re.finditer(r'(https?://[^\s]+\.pdf)', text, re.IGNORECASE):
        url = match.group(1)
        if any(x in url for x in seen_ids): continue
        
        url_hash = hashlib.md5(url.encode()).hexdigest()
        if url_hash not in seen_ids:
            results.append({
                "type": "direct_pdf",
                "id": None, 
                "url": url
            })
            seen_ids.add(url_hash)

    return results

def fetch_content(source_data):
    content = ""
    source_type = "Full Text"

    # A. PDF ä¸‹è½½
    if source_data["url"] and source_data["url"].endswith(".pdf"):
        print(f"    ğŸ“¥ [ä¸‹è½½] {source_data['url']}")
        time.sleep(3) 
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            r = requests.get(source_data["url"], headers=headers, timeout=60)
            if r.status_code == 200:
                with open("temp.pdf", "wb") as f:
                    f.write(r.content)
                content = pymupdf4llm.to_markdown("temp.pdf")
                os.remove("temp.pdf")
                return content, "PDF Full Text"
        except Exception as e:
            print(f"    âš ï¸ PDF ä¸‹è½½å¤±è´¥: {e}")

    # B. DOI æ‘˜è¦æŠ“å–
    if source_data["type"] == "doi":
        print(f"    â„¹ï¸ [å…ƒæ•°æ®] å°è¯•æŠ“å–æ‘˜è¦ DOI: {source_data['id']}")
        try:
            work = cr.works(ids=source_data["id"])
            title = work['message'].get('title', [''])[0]
            abstract = work['message'].get('abstract', 'æ— æ‘˜è¦ä¿¡æ¯')
            abstract = re.sub(r'<[^>]+>', '', abstract)
            content = f"# {title}\n\n## Abstract\n{abstract}"
            return content, "Abstract Only"
        except:
            pass
            
    return None, "Unknown"

# --- æ ¸å¿ƒä¿®æ”¹ï¼šå‡çº§ç‰ˆåˆ†æå‡½æ•° ---
def analyze_with_llm(content, content_type, source_url=""):
    """
    ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„é«˜çº§å­¦æœ¯ Prompt è¿›è¡Œåˆ†æ
    """
    prompt = f"""
    è¯·ä½œä¸ºæˆ‘çš„å­¦æœ¯åŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹æä¾›çš„æ–‡çŒ®å†…å®¹æ‰§è¡Œä»»åŠ¡ã€‚
    
    ã€æ–‡çŒ®å†…å®¹æ¥æºã€‘ï¼š{content_type}
    ã€å·²çŸ¥é“¾æ¥ã€‘ï¼š{source_url}

    è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼ˆè¯·è¾“å‡º Markdown æ ¼å¼ï¼‰ï¼š

    1. **ç¡®è®¤å¹¶å¤è¿°æ–‡çŒ®åŸºæœ¬ä¿¡æ¯**ï¼š
       - ä»æ–‡ä¸­æå–å¹¶è¡¥å…¨ï¼šæ ‡é¢˜ã€ä½œè€…ã€æœŸåˆŠ/ä¼šè®®ï¼ˆå¦‚ç¼©å†™è¯·è¡¥å…¨ï¼‰ã€å¹´ä»½ã€å…³é”®è¯ã€‚
    
    2. **ç ”ç©¶é¢†åŸŸä¸å½±å“åŠ›æ¨æ–­**ï¼š
       - æ¨æ–­æ–‡çŒ®çš„ç ”ç©¶é¢†åŸŸå’Œå¯èƒ½çš„å½±å“åŠ›ã€‚

    3. **ç ”ç©¶ç°çŠ¶ä¸ç¼ºå£**ï¼š
       - æ¸…æ™°é˜è¿°æœ¬ç ”ç©¶é¢†åŸŸçš„ç°çŠ¶å’Œæœ¬æ–‡è¦è§£å†³çš„å…·ä½“ç ”ç©¶ç¼ºå£æˆ–é—®é¢˜ã€‚

    4. **å…³é”®æŠ€æœ¯ä¸åˆ›æ–°**ï¼š
       - è¯¦ç»†è¯´æ˜æœ¬æ–‡é‡‡ç”¨çš„å…³é”®æŠ€æœ¯ã€å®éªŒè®¾è®¡æˆ–ç†è®ºæ¡†æ¶, å¹¶æ˜ç¡®å…¶åˆ›æ–°ä¹‹å¤„ã€‚

    5. **æ ¸å¿ƒç»“è®º**ï¼š
       - åˆ†ç‚¹åˆ—å‡ºæœ€é‡è¦çš„å®è¯ç»“æœå’Œç ”ç©¶ç»“è®ºã€‚

    6. **æœ¯è¯­è§£é‡Š**ï¼š
       - è§£é‡Šæ–‡ä¸­å¯èƒ½å¯¹éä¸“ä¸šè¯»è€…æ„æˆéšœç¢çš„2-3ä¸ªä¸“ä¸šæœ¯è¯­æˆ–æ¦‚å¿µã€‚

    7. **ä¼˜åŠ¿ä¸è´¡çŒ®**ï¼š
       - åˆ†ææœ¬ç ”ç©¶çš„ä¸»è¦ä¼˜åŠ¿å’Œå¯¹é¢†åŸŸçš„è´¡çŒ®ã€‚

    8. **å±€é™æ€§ä¸æœªæ¥æ–¹å‘**ï¼š
       - æ‰¹åˆ¤æ€§åœ°è®¨è®ºæœ¬ç ”ç©¶å¯èƒ½å­˜åœ¨çš„å±€é™æ€§(å¦‚æ ·æœ¬é‡ã€æ–¹æ³•å‡è®¾ç­‰), å¹¶æå‡ºæœªæ¥å¯èƒ½çš„ç ”ç©¶æ–¹å‘ã€‚

    9. **ç›¸å…³æ–‡çŒ®æ¨è**ï¼š
       - åŸºäºä½ çš„çŸ¥è¯†åº“ï¼Œæ¨è3-5ç¯‡ä¸æœ¬æ–‡çŒ®é«˜åº¦ç›¸å…³çš„åŸºç¡€æ€§æ–‡çŒ®æˆ–åç»­è·Ÿè¿›ç ”ç©¶ , å¹¶ç®€è¦è¯´æ˜å…³è”æ€§ã€‚

    10. **å­¦æœ¯æœç´¢æ¨¡æ‹Ÿ**ï¼š
        - åˆ©ç”¨ä½ çš„çŸ¥è¯†åº“æ¨¡æ‹Ÿå­¦æœ¯æ•°æ®åº“æœç´¢ï¼Œåˆ—å‡ºæœ¬æ–‡çš„æ ¸å¿ƒå¼•ç”¨ç½‘ç»œã€‚

    11. **DOIä¸é“¾æ¥**ï¼š
        - å¿…é¡»æä¾›å®Œæ•´çš„DOIå·åŠå¯ç›´è¾¾çš„DOIé“¾æ¥ã€‚
        - å¦‚æœæ— æ³•æ‰¾åˆ°DOIï¼Œè¯·æä¾›æ›¿ä»£çš„å®˜æ–¹æ¥æºé“¾æ¥ (å¦‚æœŸåˆŠä¸»é¡µã€arXivé“¾æ¥) ,å¹¶è§£é‡ŠåŸå› ã€‚ï¼ˆå‚è€ƒå·²çŸ¥é“¾æ¥ï¼š{source_url}ï¼‰

    12. **é‡åŒ–åˆ†ææå–**ï¼ˆå¦‚æœé€‚ç”¨ï¼‰ï¼š
        - å¦‚æœè¯¥è®ºæ–‡ä½¿ç”¨é‡åŒ–æ–¹æ³•ï¼Œè¯·ä¸“é—¨åˆ—å‡ºï¼š**Data/Dataset**ã€**å˜é‡**ã€**æ¨¡å‹**ã€**ç»Ÿè®¡æ–¹æ³•**ã€**æ•°æ®æ¥æº**ã€**æ•°æ®å¤„ç†æ–¹æ³•**å’Œ**æ•°æ®ç»“æœ**ã€‚

    ---
    **ä»¥ä¸‹æ˜¯æ–‡çŒ®å†…å®¹ï¼š**
    {content[:50000]} 
    ---
    """
    try:
        completion = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"LLM åˆ†æå‡ºé”™: {e}"

# --- é‚®ä»¶å¤„ç†éƒ¨åˆ† ---

def connect_imap():
    mail = imaplib.IMAP4_SSL(IMAP_SERVER)
    mail.login(EMAIL_USER, EMAIL_PASS)
    return mail

def get_emails_from_today():
    mail = connect_imap()
    mail.select("inbox")
    date_str = (datetime.date.today() - datetime.timedelta(days=1)).strftime("%d-%b-%Y")
    status, messages = mail.search(None, f'(SINCE "{date_str}")')
    
    email_ids = messages[0].split()
    target_emails = []
    
    print(f"ğŸ” æ‰«æåˆ° {len(email_ids)} å°è¿‘æœŸé‚®ä»¶...")
    
    for e_id in email_ids:
        try:
            _, msg_data = mail.fetch(e_id, "(RFC822)")
            for response_part in msg_data:
                if isinstance(response_part, tuple):
                    msg = email.message_from_bytes(response_part[1])
                    subject, encoding = decode_header(msg["Subject"])[0]
                    if isinstance(subject, bytes):
                        subject = subject.decode(encoding if encoding else "utf-8")
                    
                    if any(keyword.lower() in subject.lower() for keyword in TARGET_SUBJECTS):
                        print(f"  âœ‰ï¸ [å‘½ä¸­é‚®ä»¶] {subject}")
                        target_emails.append(msg)
        except:
            continue
    return target_emails

def extract_body(msg):
    if msg.is_multipart():
        for part in msg.walk():
            ctype = part.get_content_type()
            if ctype == "text/plain" and "attachment" not in str(part.get("Content-Disposition")):
                return part.get_payload(decode=True).decode()
            elif ctype == "text/html": 
                html = part.get_payload(decode=True).decode()
                return html
    else:
        return msg.get_payload(decode=True).decode()
    return ""

def send_daily_report(report_content):
    msg = MIMEText(report_content, "markdown", "utf-8")
    msg["Subject"] = f"ğŸ¤– AI æ–‡çŒ®æ·±åº¦åˆ†ææ—¥æŠ¥ (å…± {report_content.count('# 1. **ç¡®è®¤')} ç¯‡) - {datetime.date.today()}"
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER 

    with smtplib.SMTP_SSL(SMTP_SERVER, 465) as server:
        server.login(EMAIL_USER, EMAIL_PASS)
        server.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
    print("ğŸ“§ æ±‡æ€»é‚®ä»¶å·²å‘é€ï¼")

def main():
    processed_ids = load_history()
    emails = get_emails_from_today()
    
    if not emails:
        print("ä»Šå¤©æ²¡æœ‰ç›¸å…³é‚®ä»¶ã€‚")
        return

    daily_report = "# ğŸ“… ä»Šæ—¥æ–‡çŒ®æ·±åº¦åˆ†æ\n\n"
    total_new_count = 0
    
    for msg in emails:
        subject = decode_header(msg["Subject"])[0][0]
        if isinstance(subject, bytes): subject = subject.decode()
        
        body = extract_body(msg)
        source_list = detect_and_extract_all(body)
        
        if not source_list:
            continue
            
        print(f"    ğŸ” é‚®ä»¶å†…å‘ç° {len(source_list)} ç¯‡æ½œåœ¨æ–‡çŒ®...")

        for source_data in source_list:
            unique_id = get_unique_id(source_data)
            
            if unique_id in processed_ids:
                print(f"    â­ï¸ [è·³è¿‡] å·²åˆ†æè¿‡: {unique_id}")
                continue

            print(f"    ğŸš€ [åˆ†æ] ID: {unique_id}")
            content, ctype = fetch_content(source_data)
            
            if content:
                # ä¼ å…¥ source_url ä»¥ä¾¿ LLM å¡«å†™ç¬¬11ç‚¹
                analysis = analyze_with_llm(content, ctype, source_url=source_data.get('url', ''))
                
                paper_title = source_data.get('id', 'Paper Analysis')
                daily_report += f"## ğŸ“‘ æ–‡çŒ® ID: {paper_title}\n\n{analysis}\n\n---\n\n"
                processed_ids.append(unique_id)
                total_new_count += 1
            else:
                print(f"    âŒ ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡ã€‚")
    
    if total_new_count > 0:
        send_daily_report(daily_report)
        save_history(processed_ids)
        print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±æ›´æ–° {total_new_count} ç¯‡æ–‡çŒ®ã€‚")
    else:
        print("â˜• æ‰€æœ‰å†…å®¹éƒ½å·²åˆ†æè¿‡ã€‚")

if __name__ == "__main__":
    main()
