import os
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
import time
import hashlib
import json
import shutil
import zipfile
import socket
import imaplib
import email
import smtplib
import datetime
from datetime import timedelta
from email.header import decode_header
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from urllib.parse import unquote, urlparse
import markdown

# --- ğŸ› ï¸ é…ç½®åŒº ---
LLM_API_KEY = os.environ.get("LLM_API_KEY")
LLM_BASE_URL = "https://api.siliconflow.cn/v1"
LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")

EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"

# ğŸŸ¢ æ¯æ¬¡å¤„ç†ä¸Šé™ï¼ˆé˜²æ­¢è¶…æ—¶ï¼‰
BATCH_SIZE = 20
MAX_RETRIES = 3 # å¤±è´¥é‡è¯•æ¬¡æ•°ä¸Šé™

TARGET_SUBJECTS = [
    "æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", 
    "new research", "Stork", "ScienceDirect", "Chinese politics", 
    "Imperial history", "Causal inference", "new results", "The Accounting Review",
    "recommendations available", "Table of Contents"
]

DATA_DIR = "data"
DB_FILE = os.path.join(DATA_DIR, "papers_database.json") # ğŸŸ¢ ç»Ÿä¸€æ•°æ®åº“
DOWNLOAD_DIR = "downloads"
MAX_EMAIL_ZIP_SIZE = 18 * 1024 * 1024 
socket.setdefaulttimeout(30)

client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
cr = Crossref()
DOMAIN_LAST_ACCESSED = {}

# --- ğŸ“š æ•°æ®åº“ç®¡ç†ç±» (æ ¸å¿ƒä¼˜åŒ–) ---
class PaperDB:
    def __init__(self, filepath):
        self.filepath = filepath
        self.data = self._load()

    def _load(self):
        if os.path.exists(self.filepath):
            try:
                with open(self.filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except: pass
        return {}

    def save(self):
        os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
        with open(self.filepath, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)

    def add_new(self, pid, metadata):
        if pid not in self.data:
            self.data[pid] = {
                **metadata,
                "status": "NEW", # åˆå§‹çŠ¶æ€
                "retry_count": 0,
                "created_at": str(datetime.datetime.now()),
                "history": []
            }
            return True
        return False

    def update_status(self, pid, status, extra_data=None):
        if pid in self.data:
            self.data[pid]["status"] = status
            self.data[pid]["updated_at"] = str(datetime.datetime.now())
            if extra_data:
                self.data[pid].update(extra_data)
            self.save()

    def get_pending_downloads(self, limit=BATCH_SIZE):
        # è·å– NEW æˆ–è€… ä¸‹è½½å¤±è´¥ä¸”é‡è¯•æ¬¡æ•°æœªè¶…æ ‡çš„
        candidates = []
        for pid, item in self.data.items():
            if item["status"] == "NEW":
                candidates.append(item)
            elif item["status"] == "DOWNLOAD_FAILED" and item.get("retry_count", 0) < MAX_RETRIES:
                candidates.append(item)
        return candidates[:limit]

    def get_pending_analysis(self, limit=BATCH_SIZE):
        # è·å– DOWNLOADED æˆ–è€… åˆ†æå¤±è´¥ä¸”é‡è¯•æ¬¡æ•°æœªè¶…æ ‡çš„
        candidates = []
        for pid, item in self.data.items():
            if item["status"] == "DOWNLOADED":
                candidates.append(item)
            elif item["status"] == "ANALYSIS_FAILED" and item.get("retry_count", 0) < MAX_RETRIES:
                candidates.append(item)
        return candidates[:limit]

    def increment_retry(self, pid):
        if pid in self.data:
            self.data[pid]["retry_count"] = self.data[pid].get("retry_count", 0) + 1
            self.save()

# --- ğŸ§  æ ¸å¿ƒåŠŸèƒ½ ---

def translate_title(text):
    if not text or len(text) < 5 or "Unknown" in text: return ""
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": f"è¯·å°†ä»¥ä¸‹å­¦æœ¯è®ºæ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼ˆä»…è¾“å‡ºç¿»è¯‘åçš„æ–‡æœ¬ï¼‰ï¼š{text}"}],
            temperature=0.1
        )
        return completion.choices[0].message.content.strip()
    except: return ""

def get_metadata_safe(source_data):
    title = source_data.get('title', '')
    if title and "Unknown" not in title: return title
    s_id = source_data.get('id', '')
    if source_data.get('type') == 'arxiv': return f"ArXiv Paper {s_id}"
    return title or "Unknown Title"

def extract_titles_from_text(text):
    print("    ğŸ§  [æ™ºèƒ½æå–] æ­£åœ¨åˆ†æé‚®ä»¶æ­£æ–‡æå–æ ‡é¢˜...")
    prompt = f"Extract academic paper titles from the text below. Return ONLY a JSON list of strings. Text: {text[:3000]}"
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME, messages=[{"role": "user", "content": prompt}], temperature=0.1
        )
        content = completion.choices[0].message.content.strip().replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except: return []

def search_doi_by_title(title):
    print(f"    ğŸ” [Crossref] æœç´¢ DOI: {title[:30]}...")
    try:
        res = cr.works(query=title, limit=1)
        if res['message']['items']:
            item = res['message']['items'][0]
            return item.get('DOI'), item.get('title', [title])[0]
    except: pass
    return None, None

def get_oa_link(doi):
    try:
        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email=bot@example.com", timeout=10)
        data = r.json()
        if data.get('is_oa') and data.get('best_oa_location'):
            return data['best_oa_location']['url_for_pdf']
    except: pass
    return None

def extract_body(msg):
    text = ""
    urls = set()
    def find_urls(t): return [u.rstrip('.,;)]}') for u in re.findall(r'(https?://[^\s"\'<>]+)', t)]
    
    if msg.is_multipart():
        for part in msg.walk():
            try:
                payload = part.get_payload(decode=True)
                if not payload: continue
                pt = payload.decode(errors='ignore')
                if "attachment" not in str(part.get("Content-Disposition")):
                    if part.get_content_type() == "text/html":
                        urls.update(re.findall(r'href=["\']([^"\']+)["\']', pt, re.IGNORECASE))
                        text += re.sub('<[^<]+?>', ' ', pt) + "\n"
                    else: text += pt + "\n"
                urls.update(find_urls(pt))
            except: continue
    else:
        try:
            pt = msg.get_payload(decode=True).decode(errors='ignore')
            text += pt
            urls.update(find_urls(pt))
        except: pass
    return text, list(urls)

def detect_sources(text, urls):
    sources = []
    seen = set()
    
    # ArXiv
    for m in re.finditer(r"(?:arXiv:|arxiv\.org/abs/|arxiv\.org/pdf/)\s*(\d{4}\.\d{4,5})", text, re.IGNORECASE):
        aid = m.group(1)
        if aid not in seen:
            sources.append({"type": "arxiv", "id": aid, "url": f"https://arxiv.org/pdf/{aid}.pdf"})
            seen.add(aid)
    
    # DOI
    for m in re.finditer(r"(?:doi:|doi\.org/)\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.IGNORECASE):
        doi = m.group(1)
        if doi not in seen:
            link = get_oa_link(doi)
            sources.append({"type": "doi", "id": doi, "url": link}) # urlå¯èƒ½ä¸ºNone
            seen.add(doi)

    # Direct Links
    block = ['muse.jhu.edu', 'scholar.google.com/scholar_share', 'google.com/url']
    for link in urls:
        try:
            l = unquote(link).lower()
            if any(x in l for x in block): continue
            if l.endswith('.pdf') or 'viewcontent.cgi' in l:
                lid = hashlib.md5(l.encode()).hexdigest()[:10]
                if lid not in seen:
                    sources.append({"type": "pdf_link", "id": f"link_{lid}", "url": link})
                    seen.add(lid)
        except: continue
    return sources

def polite_wait(url):
    if not url: return
    dom = urlparse(url).netloc
    last = DOMAIN_LAST_ACCESSED.get(dom, 0)
    if time.time() - last < 5: time.sleep(5)
    DOMAIN_LAST_ACCESSED[dom] = time.time()

def get_safe_filename(pid, save_dir):
    safe_name = re.sub(r'[\\/*?:"<>|]', '_', pid)
    return os.path.join(save_dir, f"{safe_name}.pdf")

def fetch_content(item, save_dir):
    url = item.get('url')
    if not url: return None, "No URL", None
    
    polite_wait(url)
    print(f"    ğŸ” [ä¸‹è½½] {url[:50]}...")
    
    try:
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=30, stream=True)
        if r.status_code == 429: return None, "Rate Limit", None
        
        # æ£€æŸ¥æ˜¯å¦çœŸæ˜¯PDF
        if 'application/pdf' not in r.headers.get('Content-Type', '').lower() and not url.endswith('.pdf'):
             return None, "Not PDF", None

        fname = get_safe_filename(item['id'], save_dir)
        with open(fname, "wb") as f:
            for chunk in r.iter_content(8192): f.write(chunk)
        
        if os.path.getsize(fname) < 2000:
            os.remove(fname)
            return None, "File Too Small", None
            
        try:
            content = pymupdf4llm.to_markdown(fname)
            if len(content) < 500:
                os.remove(fname)
                return None, "Content Empty", None
            return content, "PDF Full Text", fname
        except:
            return None, "Parse Error", None
            
    except Exception as e:
        return None, str(e), None

def analyze_with_llm(content, ctype):
    prompt = f"""ä½ æ˜¯ä¸€åå­¦æœ¯ç ”ç©¶åŠ©ç†ã€‚è¯·ç”¨ã€ä¸­æ–‡ã€‘åˆ†æä»¥ä¸‹æ–‡çŒ®ã€‚
    â—é‡è¦ï¼šç¬¬ä¸€è¡ŒåŠ¡å¿…è¾“å‡ºçœŸå®è‹±æ–‡æ ‡é¢˜ï¼Œæ ¼å¼ "TITLE: <Title>"ã€‚
    ä»»åŠ¡ï¼š
    1. æå–çœŸå®æ ‡é¢˜ã€‚
    2. æ·±åº¦åˆ†æèƒŒæ™¯ã€é—®é¢˜ã€æ–¹æ³•ã€ç»“è®ºã€åˆ›æ–°ç‚¹ã€‚
    3. é‡åˆ°å›¾è¡¨æ—¶æ’å…¥ 

[Image of X]
ã€‚
    4. è¾“å‡º Markdownã€‚

    æ¥æºï¼š{ctype}
    å†…å®¹ï¼š{content[:50000]}
    """
    try:
        res = client.chat.completions.create(
            model=LLM_MODEL_NAME, messages=[{"role": "user", "content": prompt}], temperature=0.3
        )
        txt = res.choices[0].message.content.strip()
        
        real_title = "Unknown"
        body = txt
        match = re.match(r"^TITLE:\s*(.*)", txt, re.IGNORECASE)
        if match:
            real_title = match.group(1).strip()
            body = txt.split('\n', 1)[1].strip()
        return real_title, body
    except Exception as e: return None, f"Error: {e}"

def send_email(subject, body, attach_files=[]):
    html = markdown.markdown(body, extensions=['extra'])
    # æ›¿æ¢ Image tag
    html = re.sub(r'\]+)\]', r'<div style="background:#eef;padding:10px;margin:10px 0;border:1px dashed #ccc;text-align:center;color:#666">ğŸ–¼ï¸ å›¾ç¤ºå»ºè®®ï¼š\1</div>', html)
    
    full_html = f"""
    <html>
    <body style="font-family:sans-serif;max-width:800px;margin:auto;padding:20px;">
        <div style="background:#2c3e50;color:white;padding:20px;border-radius:8px;">
            <h1 style="margin:0">{subject}</h1>
            <p>{datetime.date.today()}</p>
        </div>
        {html}
        <hr>
        <p style="text-align:center;color:#888;font-size:12px">AI Research Assistant</p>
    </body>
    </html>
    """
    
    msg = MIMEMultipart()
    msg["Subject"] = subject
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER
    msg.attach(MIMEText(full_html, "html", "utf-8"))
    
    for fpath in attach_files:
        if os.path.exists(fpath):
            try:
                with open(fpath, "rb") as f:
                    part = MIMEApplication(f.read(), Name=os.path.basename(fpath))
                    part['Content-Disposition'] = f'attachment; filename="{os.path.basename(fpath)}"'
                    msg.attach(part)
            except: pass
            
    try:
        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as s:
            s.login(EMAIL_USER, EMAIL_PASS)
            s.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
        return True
    except Exception as e:
        print(f"é‚®ä»¶å¤±è´¥: {e}")
        return False

# --- ğŸš€ ä¸»æµç¨‹ ---

def run():
    print(f"ğŸ¬ å¯åŠ¨: {datetime.datetime.now()}")
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    
    db = PaperDB(DB_FILE)
    print(f"ğŸ“š æ•°æ®åº“åŠ è½½å®Œæ¯•ï¼Œå…± {len(db.data)} æ¡è®°å½•")

    # --- 1. æ‰«æé‚®ä»¶ (ç”Ÿäº§è€…) ---
    mail = imaplib.IMAP4_SSL(IMAP_SERVER)
    mail.login(EMAIL_USER, EMAIL_PASS)
    mail.select("inbox")
    
    since = (datetime.date.today() - timedelta(days=2)).strftime("%d-%b-%Y")
    _, data = mail.search(None, f'(SINCE "{since}")')
    
    for eid in data[0].split():
        try:
            _, h = mail.fetch(eid, "(BODY.PEEK[HEADER])")
            subj = decode_header(email.message_from_bytes(h[0][1])["Subject"])[0][0]
            if isinstance(subj, bytes): subj = subj.decode()
            
            if not any(k.lower() in subj.lower() for k in TARGET_SUBJECTS): continue
            print(f"ğŸ¯ å‘½ä¸­: {subj[:20]}...")
            
            _, m = mail.fetch(eid, "(RFC822)")
            body, urls = extract_body(email.message_from_bytes(m[0][1]))
            sources = detect_sources(body, urls)
            
            # å¦‚æœæ²¡æ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•LLMåæŸ¥
            if not sources:
                titles = extract_titles_from_text(body)
                for t in titles:
                    doi, full = search_doi_by_title(t)
                    if doi: sources.append({"type": "doi", "id": doi, "url": get_oa_link(doi), "title": full})

            for s in sources:
                # ç»Ÿä¸€ ID ç”Ÿæˆ
                pid = s.get('id') or hashlib.md5(s.get('url','').encode()).hexdigest()[:10]
                s['id'] = pid
                if 'title' not in s: s['title'] = get_metadata_safe(s)
                
                # æ·»åŠ åˆ°æ•°æ®åº“ (NEW)
                if db.add_new(pid, s):
                    print(f"    â• å…¥åº“: {pid}")
                    
        except Exception as e: print(f"æ‰«æé”™è¯¯: {e}")

    # --- 2. å¤„ç†ä¸‹è½½ (æ¶ˆè´¹è€… 1) ---
    to_download = db.get_pending_downloads(BATCH_SIZE)
    print(f"ğŸ“¥ å¾…ä¸‹è½½é˜Ÿåˆ—: {len(to_download)} ç¯‡")
    
    for item in to_download:
        pid = item['id']
        print(f"Processing Download: {pid}")
        content, ctype, path = fetch_content(item, DOWNLOAD_DIR)
        
        if path:
            # ä¸‹è½½æˆåŠŸ
            db.update_status(pid, "DOWNLOADED", {"local_path": path, "content_type": ctype})
        else:
            # ä¸‹è½½å¤±è´¥
            print(f"    âŒ ä¸‹è½½å¤±è´¥: {ctype}")
            db.increment_retry(pid)
            db.update_status(pid, "DOWNLOAD_FAILED", {"error": ctype})

    # --- 3. å¤„ç†åˆ†æ (æ¶ˆè´¹è€… 2) ---
    # æ³¨æ„ï¼šè¿™é‡Œä¼šé‡æ–°è·å– DOWNLOADED çŠ¶æ€çš„ï¼ŒåŒ…æ‹¬åˆšåˆšä¸‹è½½æˆåŠŸçš„
    to_analyze = db.get_pending_analysis(BATCH_SIZE) 
    print(f"ğŸ¤– å¾…åˆ†æé˜Ÿåˆ—: {len(to_analyze)} ç¯‡")
    
    new_reports = []
    attachments = []
    
    for item in to_analyze:
        pid = item['id']
        print(f"Processing Analysis: {pid}")
        
        # å¿…é¡»ç¡®ä¿æ–‡ä»¶å­˜åœ¨ (GitHub Actions æ¯æ¬¡æ˜¯æ–°çš„ï¼Œæ‰€ä»¥å¿…é¡»æ˜¯åˆšæ‰ä¸‹è½½çš„)
        # å¦‚æœæ˜¯ä¹‹å‰è¿è¡Œä¸‹è½½çš„ï¼Œä½†åœ¨å½“å‰ç¯å¢ƒé‡Œæ²¡æœ‰ï¼Œéœ€è¦é‡æ–°ä¸‹è½½
        local_path = get_safe_filename(pid, DOWNLOAD_DIR)
        if not os.path.exists(local_path):
            print("    âš ï¸ æœ¬åœ°æ–‡ä»¶ç¼ºå¤± (å¯èƒ½æ˜¯ä¸Šæ¬¡è¿è¡Œä¸‹è½½çš„)ï¼Œé‡æ–°ä¸‹è½½...")
            content, ctype, local_path = fetch_content(item, DOWNLOAD_DIR)
            if not local_path:
                print("    âŒ é‡è¯•ä¸‹è½½å¤±è´¥")
                db.update_status(pid, "DOWNLOAD_FAILED") # å›é€€çŠ¶æ€
                continue
        
        # è¯»å–å†…å®¹
        try:
            content = pymupdf4llm.to_markdown(local_path)
        except:
            print("    âŒ æ–‡ä»¶æ— æ³•è¯»å–")
            db.update_status(pid, "ANALYSIS_FAILED")
            continue

        # LLM åˆ†æ
        real_title, analysis = analyze_with_llm(content, "PDF")
        
        if analysis and "Error" not in analysis:
            trans_title = translate_title(real_title)
            
            # ç”Ÿæˆå¡ç‰‡ HTML
            card = f"""
            <div style="background:white;padding:20px;margin-bottom:20px;border-radius:10px;border:1px solid #eee;box-shadow:0 2px 5px rgba(0,0,0,0.05);">
                <div style="font-size:18px;font-weight:bold;color:#2c3e50;border-bottom:2px solid #3498db;padding-bottom:10px;">{real_title}</div>
                <div style="background:#f0f7ff;padding:8px;margin:10px 0;border-left:4px solid #3498db;color:#555;font-weight:bold;">{trans_title}</div>
                <div>{analysis}</div>
            </div>
            """
            new_reports.append(card)
            attachments.append(local_path)
            
            # æ›´æ–°çŠ¶æ€ä¸º ANALYZED
            db.update_status(pid, "ANALYSIS_FAILED" if "Error" in analysis else "ANALYZED", {
                "real_title": real_title,
                "trans_title": trans_title
            })
        else:
            db.increment_retry(pid)
            db.update_status(pid, "ANALYSIS_FAILED")

    # --- 4. å‘é€é‚®ä»¶ ---
    if new_reports:
        # åˆ†åŒ…å‘é€
        zips = []
        curr_zip, curr_size = [], 0
        for f in attachments:
            sz = os.path.getsize(f)
            if curr_size + sz > MAX_EMAIL_ZIP_SIZE:
                zips.append(curr_zip)
                curr_zip, curr_size = [f], sz
            else:
                curr_zip.append(f)
                curr_size += sz
        if curr_zip: zips.append(curr_zip)
        
        full_body = "\n".join(new_reports)
        
        for i, zfiles in enumerate(zips):
            zname = f"papers_{i+1}.zip"
            with zipfile.ZipFile(zname, 'w', zipfile.ZIP_DEFLATED) as zf:
                for f in zfiles: zf.write(f, os.path.basename(f))
            
            subj = f"ğŸ¤– AI æ—¥æŠ¥ (Part {i+1}/{len(zips)})"
            body = full_body if i==0 else "<h3>ğŸ“ é™„ä»¶è¡¥å‘</h3>"
            
            send_email(subj, body, zname)
            if os.path.exists(zname): os.remove(zname)
            time.sleep(5)
    else:
        print("â˜• æœ¬æ¬¡æ— æ–°åˆ†æç»“æœ")

    print("âœ… å®Œæˆ")

if __name__ == "__main__":
    run()
