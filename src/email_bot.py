import os
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
import time
import hashlib
import json
import zipfile
import socket
import imaplib
import email
import smtplib
import datetime
import logging
from datetime import timedelta
from email.header import decode_header
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from urllib.parse import unquote, urlparse, parse_qs
import markdown
from tenacity import retry, stop_after_attempt, wait_exponential

# --- é…ç½®æ—¥å¿— ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å…¨å±€å˜é‡ ---
LLM_API_KEY = os.environ.get("LLM_API_KEY")
LLM_BASE_URL = "https://api.siliconflow.cn/v1"
LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"

SCHEDULER_MODE = False
LOOP_INTERVAL_HOURS = 4
BATCH_SIZE = 20
MAX_RETRIES = 3

TARGET_SUBJECTS = [
    "æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", "new research", 
    "Stork", "ScienceDirect", "Chinese politics", "Imperial history", 
    "Causal inference", "new results", "The Accounting Review", 
    "recommendations available", "Table of Contents"
]

DATA_DIR = "data"
DB_FILE = os.path.join(DATA_DIR, "papers_database.json")
DOWNLOAD_DIR = "downloads"
MAX_EMAIL_ZIP_SIZE = 18 * 1024 * 1024 
socket.setdefaulttimeout(30)

client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
cr = Crossref()
DOMAIN_LAST_ACCESSED = {}

# --- è¾…åŠ©å‡½æ•° ---
def clean_google_url(url):
    """æ¸…æ´— Google è·³è½¬é“¾æ¥"""
    try:
        url = unquote(url)
        if "google" in url and ("url=" in url or "q=" in url):
            parsed = urlparse(url)
            qs = parse_qs(parsed.query)
            if 'url' in qs: 
                return unquote(qs['url'][0])
            if 'q' in qs: 
                return unquote(qs['q'][0])
    except Exception as e:
        logger.debug(f"URL æ¸…æ´—å¼‚å¸¸: {e}")
    return url

# --- æ•°æ®åº“ç±» ---
class PaperDB:
    def __init__(self, filepath):
        self.filepath = filepath
        self.data = self._load()

    def _load(self):
        if os.path.exists(self.filepath):
            try:
                with open(self.filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½æ•°æ®åº“å¤±è´¥: {e}")
        return {}

    def save(self):
        try:
            os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
            with open(self.filepath, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®åº“å¤±è´¥: {e}")

    def add_new(self, pid, meta):
        if pid not in self.data:
            self.data[pid] = {
                **meta, 
                "status": "NEW", 
                "retry": 0, 
                "created_at": str(datetime.datetime.now())
            }
            self.save()
            return True
        return False

    def update_status(self, pid, status, extra=None):
        if pid in self.data:
            self.data[pid]["status"] = status
            self.data[pid]["updated_at"] = str(datetime.datetime.now())
            if extra: 
                self.data[pid].update(extra)
            self.save()

    def get_pending_downloads(self, limit=BATCH_SIZE):
        res = []
        for pid, item in self.data.items():
            if item["status"] == "NEW":
                res.append(item)
            elif item["status"] == "DOWNLOAD_FAILED" and item.get("retry", 0) < MAX_RETRIES:
                res.append(item)
        return res[:limit]

    def get_pending_analysis(self, limit=BATCH_SIZE):
        res = []
        for pid, item in self.data.items():
            if item["status"] in ["DOWNLOADED", "ABSTRACT_ONLY"]:
                res.append(item)
            elif item["status"] == "ANALYSIS_FAILED" and item.get("retry", 0) < MAX_RETRIES:
                res.append(item)
        return res[:limit]

    def inc_retry(self, pid):
        if pid in self.data:
            self.data[pid]["retry"] = self.data[pid].get("retry", 0) + 1
            self.save()

# --- æ ¸å¿ƒé€»è¾‘ ---

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10), reraise=False)
def translate_title(text):
    """ç¿»è¯‘æ ‡é¢˜"""
    if not text or len(text) < 5 or "Unknown" in text: 
        return ""
    res = client.chat.completions.create(
        model=LLM_MODEL_NAME,
        messages=[{"role": "user", "content": f"è¯·å°†ä»¥ä¸‹å­¦æœ¯è®ºæ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼ˆä»…è¾“å‡ºç¿»è¯‘åçš„æ–‡æœ¬ï¼‰ï¼š{text}"}],
        temperature=0.1
    )
    return res.choices[0].message.content.strip()

def get_meta_safe(src):
    """å®‰å…¨è·å–å…ƒæ•°æ®æ ‡é¢˜"""
    t = src.get('title', '')
    if t and "Unknown" not in t: 
        return t
    if src.get('type') == 'arxiv': 
        return f"ArXiv {src.get('id')}"
    return "Unknown Title"

def extract_titles(text):
    """ä»æ–‡æœ¬ä¸­æå–æ ‡é¢˜"""
    logger.info("    ğŸ§  [æ™ºèƒ½æå–] åˆ†æé‚®ä»¶æ ‡é¢˜...")
    try:
        res = client.chat.completions.create(
            model=LLM_MODEL_NAME, 
            messages=[{"role": "user", "content": f"Extract academic paper titles from the text below. Return ONLY a JSON list of strings. Text: {text[:3000]}"}],
            temperature=0.1
        )
        content = res.choices[0].message.content.strip()
        content = content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        logger.warning(f"æ ‡é¢˜æå–å¤±è´¥: {e}")
        return []

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=4, max=20))
def search_doi(title):
    """é€šè¿‡æ ‡é¢˜æœç´¢ DOI"""
    logger.info(f"    ğŸ” [Crossref] æœç´¢ DOI: {title[:30]}...")
    res = cr.works(query=title, limit=1)
    if res['message']['items']:
        it = res['message']['items'][0]
        return it.get('DOI'), it.get('title', [title])[0]
    return None, None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=4, max=10))
def get_oa_link(doi):
    """è·å–å¼€æ”¾è·å–é“¾æ¥"""
    r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email=bot@example.com", timeout=10)
    if r.status_code == 200:
        d = r.json()
        if d.get('is_oa') and d.get('best_oa_location'): 
            return d['best_oa_location']['url_for_pdf']
    return None

def extract_body_urls(msg):
    """æå–é‚®ä»¶æ­£æ–‡å’Œé“¾æ¥"""
    text = ""
    urls = set()
    
    def grep_url(t): 
        return [u.rstrip('.,;)]}') for u in re.findall(r'(https?://[^\s"\'<>]+)', t)]
    
    if msg.is_multipart():
        for p in msg.walk():
            try:
                payload = p.get_payload(decode=True)
                if not payload: 
                    continue
                content = payload.decode(errors='ignore')
                
                if p.get_content_type() == "text/html":
                    urls.update(re.findall(r'href=["\']([^"\']+)["\']', content, re.IGNORECASE))
                    text += re.sub('<[^<]+?>', ' ', content) + "\n"
                elif p.get_content_type() == "text/plain":
                    text += content + "\n"
                    
                urls.update(grep_url(content))
            except:
                continue
    else:
        try:
            payload = msg.get_payload(decode=True).decode(errors='ignore')
            text += payload
            urls.update(grep_url(payload))
        except:
            pass
    
    return text, list(urls)

def detect_sources(text, urls):
    """æ£€æµ‹æ–‡çŒ®æº"""
    srcs = []
    seen = set()
    
    # ArXiv
    for m in re.finditer(r"(?:arXiv:|arxiv\.org/abs/|arxiv\.org/pdf/)\s*(\d{4}\.\d{4,5})", text, re.I):
        aid = m.group(1)
        if aid not in seen:
            srcs.append({
                "type": "arxiv", 
                "id": aid, 
                "url": f"https://arxiv.org/pdf/{aid}.pdf"
            })
            seen.add(aid)
            
    # DOI
    for m in re.finditer(r"(?:doi:|doi\.org/)\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.I):
        doi = m.group(1)
        if doi not in seen:
            try: 
                link = get_oa_link(doi)
            except: 
                link = None
            srcs.append({"type": "doi", "id": doi, "url": link})
            seen.add(doi)
            
    # Links
    blocked = ['unsubscribe', 'twitter.com', 'facebook.com', 'muse.jhu.edu', 'sciencedirect.com/science/article/pii']
    
    for link in urls:
        try:
            clink = clean_google_url(link)
            if not clink: 
                continue
            lower = clink.lower()
            
            if any(x in lower for x in blocked): 
                continue
            
            if lower.endswith('.pdf') or 'viewcontent.cgi' in lower:
                lid = hashlib.md5(clink.encode()).hexdigest()[:10]
                if lid not in seen:
                    srcs.append({"type": "pdf_link", "id": f"link_{lid}", "url": clink})
                    seen.add(lid)
        except:
            continue
    
    return srcs

def get_path(pid):
    """è·å–å®‰å…¨çš„æ–‡ä»¶è·¯å¾„"""
    safe_name = re.sub(r'[\\/*?:"<>|]', '_', pid)
    return os.path.join(DOWNLOAD_DIR, f"{safe_name}.pdf")

def fetch_content(item):
    """ä¸‹è½½æ–‡çŒ®å†…å®¹"""
    url = item.get('url')
    if url:
        url = clean_google_url(url)
    
    if not url:
        if item.get("type") == "doi": 
            logger.info("    â„¹ï¸ æ—  PDF é“¾æ¥ï¼Œå°è¯•æŠ“å–æ‘˜è¦...")
            return fetch_abstract(item)
        return None, "No URL", None
        
    logger.info(f"    ğŸ” [ä¸‹è½½] {url[:50]}...")
    
    try:
        r = requests.get(
            url, 
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0"}, 
            timeout=30, 
            stream=True
        )
        
        if r.status_code == 429: 
            return None, "Rate Limit", None
        
        ct = r.headers.get('Content-Type', '').lower()
        if 'application/pdf' not in ct and not url.lower().endswith('.pdf'):
            logger.warning(f"    âš ï¸ å“åº”é PDF ({ct})ï¼Œå°è¯•æ‘˜è¦è¡¥æ•‘...")
            if item.get("type") == "doi": 
                return fetch_abstract(item)
            return None, "Not PDF", None
            
        fp = get_path(item['id'])
        with open(fp, "wb") as f:
            for chunk in r.iter_content(8192): 
                f.write(chunk)
            
        if os.path.getsize(fp) < 2000:
            logger.warning("    âš ï¸ æ–‡ä»¶è¿‡å°ï¼Œå°è¯•æ‘˜è¦è¡¥æ•‘...")
            os.remove(fp)
            if item.get("type") == "doi": 
                return fetch_abstract(item)
            return None, "Too Small", None
            
        try:
            txt = pymupdf4llm.to_markdown(fp)
            if len(txt) < 500:
                os.remove(fp)
                if item.get("type") == "doi": 
                    return fetch_abstract(item)
                return None, "Empty", None
            return txt, "PDF", fp
        except Exception as e:
            logger.warning(f"    PDF è§£æå¤±è´¥: {e}")
            if item.get("type") == "doi": 
                return fetch_abstract(item)
            return None, "Parse Error", None
            
    except Exception as e:
        logger.error(f"    ä¸‹è½½å¼‚å¸¸: {e}")
        if item.get("type") == "doi": 
            return fetch_abstract(item)
        return None, str(e), None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
def fetch_abstract(item):
    """è·å–æ‘˜è¦ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰"""
    w = cr.works(ids=item["id"])
    t = w['message'].get('title', [''])[0]
    a = re.sub(r'<[^>]+>', '', w['message'].get('abstract', 'æ— æ‘˜è¦'))
    return f"TITLE: {t}\n\nABSTRACT: {a}", "ABSTRACT_ONLY", None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=5, max=30))
def analyze(txt, ctype):
    """LLM åˆ†ææ–‡çŒ®"""
    if ctype == "ABSTRACT_ONLY":
        prompt = f"""ä½ æ˜¯å­¦æœ¯ç ”ç©¶åŠ©ç†ã€‚ä»¥ä¸‹æ˜¯æ–‡çŒ®çš„æ ‡é¢˜å’Œæ‘˜è¦ï¼ˆæœªè·å–å…¨æ–‡ï¼‰ã€‚

è¯·ä»…æ ¹æ®æ‘˜è¦è¿›è¡Œç®€è¦åˆ†æï¼š
1. ç¬¬ä¸€è¡Œè¾“å‡ºçœŸå®è‹±æ–‡æ ‡é¢˜ï¼Œæ ¼å¼: TITLE: <è‹±æ–‡æ ‡é¢˜>
2. æ€»ç»“æ ¸å¿ƒå†…å®¹ï¼ˆèƒŒæ™¯ã€æ–¹æ³•ã€ç»“è®ºï¼‰
3. æ˜ç¡®æ ‡æ³¨ã€ä»…åŸºäºæ‘˜è¦åˆ†æã€‘
4. è¾“å‡º Markdown æ ¼å¼

å†…å®¹ï¼š
{txt[:3000]}
"""
    else:
        prompt = f"""ä½ æ˜¯å­¦æœ¯ç ”ç©¶åŠ©ç†ã€‚è¯·ç”¨ä¸­æ–‡æ·±åº¦åˆ†æä»¥ä¸‹æ–‡çŒ®å…¨æ–‡ã€‚

â—é‡è¦ï¼šç¬¬ä¸€è¡ŒåŠ¡å¿…è¾“å‡ºçœŸå®è‹±æ–‡æ ‡é¢˜ï¼Œæ ¼å¼ "TITLE: <Title>"

ä»»åŠ¡ï¼š
1. æå–çœŸå®æ ‡é¢˜
2. æ·±åº¦åˆ†æèƒŒæ™¯ã€é—®é¢˜ã€æ–¹æ³•ã€ç»“è®ºã€åˆ›æ–°ç‚¹
3. è¾“å‡º Markdown æ ¼å¼
4. ä¸è¦åŒ…å«å›¾ç‰‡å ä½ç¬¦

æ¥æºç±»å‹ï¼š{ctype}
å†…å®¹ï¼š{txt[:50000]}
"""
    
    res = client.chat.completions.create(
        model=LLM_MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    raw = res.choices[0].message.content.strip()
    
    title = "Unknown"
    body = raw
    m = re.match(r"^TITLE:\s*(.*)", raw, re.I)
    if m:
        title = m.group(1).strip()
        parts = raw.split('\n', 1)
        body = parts[1].strip() if len(parts) > 1 else ""
    
    return title, body

def send_mail(subj, md_body, files=None):
    """å‘é€é‚®ä»¶"""
    if files is None:
        files = []
    
    html = markdown.markdown(md_body, extensions=['extra'])
    
    full_html = f"""
    <html>
    <body style="font-family:sans-serif;max-width:800px;margin:auto;padding:20px">
        <div style="background:#2c3e50;color:white;padding:20px;border-radius:8px">
            <h1 style="margin:0">{subj}</h1>
            <p>{datetime.date.today()}</p>
        </div>
        {html}
        <hr>
        <p style="text-align:center;color:#888;font-size:12px">AI Research Assistant</p>
    </body>
    </html>
    """
    
    msg = MIMEMultipart()
    msg["Subject"] = subj
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER
    msg.attach(MIMEText(full_html, "html", "utf-8"))
    
    for f in files:
        if os.path.exists(f):
            try:
                with open(f, "rb") as fp:
                    part = MIMEApplication(fp.read(), Name=os.path.basename(f))
                    part['Content-Disposition'] = f'attachment; filename="{os.path.basename(f)}"'
                    msg.attach(part)
            except Exception as e:
                logger.warning(f"é™„ä»¶å¤„ç†å¤±è´¥: {e}")
            
    try:
        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as s:
            s.login(EMAIL_USER, EMAIL_PASS)
            s.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
        logger.info("âœ… é‚®ä»¶å·²å‘é€")
        return True
    except Exception as e:
        logger.error(f"é‚®ä»¶å‘é€å¤±è´¥: {e}")
        return False

# --- ä¸»ç¨‹åº ---
def run():
    """ä¸»è¿è¡Œå‡½æ•°"""
    logger.info(f"ğŸ¬ ä»»åŠ¡å¼€å§‹: {datetime.datetime.now()}")
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    
    db = PaperDB(DB_FILE)
    logger.info(f"ğŸ“š æ•°æ®åº“è®°å½•æ•°: {len(db.data)}")

    # ========== 1. æ‰«æé‚®ä»¶ ==========
    try:
        m = imaplib.IMAP4_SSL(IMAP_SERVER)
        m.login(EMAIL_USER, EMAIL_PASS)
        m.select("inbox")
        
        since_date = (datetime.date.today() - timedelta(days=2)).strftime("%d-%b-%Y")
        _, data = m.search(None, f'(SINCE "{since_date}")')
        
        if data[0]:
            for eid in data[0].split():
                try:
                    _, h = m.fetch(eid, "(BODY.PEEK[HEADER])")
                    subj = decode_header(email.message_from_bytes(h[0][1])["Subject"])[0][0]
                    if isinstance(subj, bytes): 
                        subj = subj.decode()
                    
                    if not any(k.lower() in subj.lower() for k in TARGET_SUBJECTS): 
                        continue
                    
                    logger.info(f"ğŸ¯ å‘½ä¸­é‚®ä»¶: {subj[:20]}...")
                    
                    _, b = m.fetch(eid, "(RFC822)")
                    msg = email.message_from_bytes(b[0][1])
                    txt, urls = extract_body_urls(msg)
                    srcs = detect_sources(txt, urls)
                    
                    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æºï¼Œå°è¯•æ™ºèƒ½æå–
                    if not srcs:
                        ts = extract_titles(txt)
                        for t in ts:
                            try:
                                doi, full = search_doi(t)
                                if doi: 
                                    oa_link = get_oa_link(doi)
                                    srcs.append({
                                        "type": "doi", 
                                        "id": doi, 
                                        "url": oa_link,
                                        "title": full
                                    })
                            except Exception as e:
                                logger.warning(f"DOI æœç´¢å¤±è´¥: {e}")
                            
                    for s in srcs:
                        pid = s.get('id') or hashlib.md5(s.get('url', '').encode()).hexdigest()[:10]
                        s['id'] = pid
                        if 'title' not in s: 
                            s['title'] = get_meta_safe(s)
                        if db.add_new(pid, s): 
                            logger.info(f"    â• æ–°å¢: {pid}")
                            
                except Exception as e: 
                    logger.error(f"é‚®ä»¶è§£æé”™è¯¯: {e}")
                    
    except Exception as e: 
        logger.error(f"IMAP è¿æ¥é”™è¯¯: {e}", exc_info=True)

    # ========== 2. ä¸‹è½½æ–‡çŒ® ==========
    pend_dl = db.get_pending_downloads(BATCH_SIZE)
    logger.info(f"ğŸ“¥ å¾…ä¸‹è½½é˜Ÿåˆ—: {len(pend_dl)}")
    
    for item in pend_dl:
        pid = item['id']
        logger.info(f"Processing Download: {pid}")
        
        res, type_, path = fetch_content(item)
        
        if type_ == "PDF":
            db.update_status(pid, "DOWNLOADED", {
                "local_path": path, 
                "content_type": type_
            })
        elif type_ == "ABSTRACT_ONLY":
            db.update_status(pid, "ABSTRACT_ONLY", {
                "abstract_content": res, 
                "content_type": type_
            })
        else:
            logger.warning(f"    âŒ ä¸‹è½½å¤±è´¥: {type_}")
            db.inc_retry(pid)
            db.update_status(pid, "DOWNLOAD_FAILED", {"error": type_})

    # ========== 3. åˆ†ææ–‡çŒ® ==========
    pend_an = db.get_pending_analysis(BATCH_SIZE)
    logger.info(f"ğŸ¤– å¾…åˆ†æé˜Ÿåˆ—: {len(pend_an)}")
    
    reports, atts = [], []
    
    for item in pend_an:
        pid = item['id']
        logger.info(f"Processing Analysis: {pid}")
        
        txt, ctype = "", item.get("content_type", "Unknown")
        
        if item["status"] == "DOWNLOADED":
            fp = get_path(pid)
            if not os.path.exists(fp):
                logger.info("    âš ï¸ æœ¬åœ°æ–‡ä»¶ç¼ºå¤±ï¼Œé‡æ–°ä¸‹è½½...")
                txt_new, ctype_new, fp_new = fetch_content(item)
                if not fp_new: 
                    db.update_status(pid, "DOWNLOAD_FAILED")
                    continue
                fp = fp_new
                
            try: 
                txt = pymupdf4llm.to_markdown(fp)
            except Exception as e:
                logger.error(f"Markdown æå–å¤±è´¥: {e}")
                db.update_status(pid, "ANALYSIS_FAILED")
                continue
                
            atts.append(fp)
            
        elif item["status"] == "ABSTRACT_ONLY":
            txt = item.get("abstract_content", "")
            if not txt:
                try: 
                    txt, _, _ = fetch_abstract(item)
                except Exception as e:
                    logger.error(f"æ‘˜è¦è·å–å¤±è´¥: {e}")
                    db.inc_retry(pid)
                    continue
        
        try:
            rt, ans = analyze(txt, ctype)
            tt = translate_title(rt) or "ç¿»è¯‘å¤±è´¥"
            
            badge = ""
            if ctype == "ABSTRACT_ONLY":
                badge = "<span style='background:#fff3cd;color:#856404;padding:2px 6px;border-radius:4px;font-size:12px;margin-left:10px'>âš ï¸ ä»…æ‘˜è¦åˆ†æ</span>"
            
            card = f"""
<div style="background:white;padding:20px;margin-bottom:20px;border-radius:10px;border:1px solid #eee;box-shadow:0 2px 5px rgba(0,0,0,0.05)">
    <div style="font-size:18px;font-weight:bold;color:#2c3e50;border-bottom:2px solid #3498db;padding-bottom:10px">{rt} {badge}</div>
    <div style="background:#f0f7ff;padding:8px;margin:10px 0;border-left:4px solid #3498db;color:#555;font-weight:bold">{tt}</div>
    <div>{ans}</div>
</div>
"""
            reports.append(card)
            db.update_status(pid, "ANALYZED", {
                "real_title": rt, 
                "trans_title": tt
            })
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {e}", exc_info=True)
            db.inc_retry(pid)
            db.update_status(pid, "ANALYSIS_FAILED")

    # ========== 4. å‘é€é‚®ä»¶ ==========
    if reports:
        body = "\n".join(reports)
        
        # åˆ†å·æ‰“åŒ…é™„ä»¶
        zips = []
        cz, csz = [], 0
        for f in atts:
            s = os.path.getsize(f)
            if csz + s > MAX_EMAIL_ZIP_SIZE:
                zips.append(cz)
                cz, csz = [f], s
            else:
                cz.append(f)
                csz += s
        if cz: 
            zips.append(cz)
        
        # å‘é€é‚®ä»¶
        if not zips:
            send_mail(f"ğŸ¤– AI æ—¥æŠ¥ (æ–°:{len(reports)})", body)
        else:
            for i, zf in enumerate(zips):
                zn = f"papers_{i+1}.zip"
                try:
                    with zipfile.ZipFile(zn, 'w', zipfile.ZIP_DEFLATED) as z:
                        for f in zf: 
                            z.write(f, os.path.basename(f))
                    
                    subj = f"ğŸ¤– AI æ—¥æŠ¥ (Part {i+1}/{len(zips)})"
                    b = body if i == 0 else "<h3>ğŸ“ é™„ä»¶è¡¥å‘</h3>"
                    
                    send_mail(subj, b, [zn])
                    
                    if os.path.exists(zn): 
                        os.remove(zn)
                    
                    time.sleep(5)
                    
                except Exception as e:
                    logger.error(f"ZIP å¤„ç†å¤±è´¥: {e}")
    else:
        logger.info("â˜• æœ¬æ¬¡æ— æ–°åˆ†æç»“æœ")
    
    logger.info("âœ… ä»»åŠ¡å®Œæˆ")

if __name__ == "__main__":
    if SCHEDULER_MODE:
        logger.info("ğŸ”„ å¯åŠ¨å¾ªç¯æ¨¡å¼...")
        while True:
            try: 
                run()
            except Exception as e:
                logger.exception("ä»»åŠ¡å´©æºƒ")
            time.sleep(LOOP_INTERVAL_HOURS * 3600)
    else:
        run()
