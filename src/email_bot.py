import os
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
import time
import hashlib
import json
import shutil
import zipfile
import socket
import imaplib
import email
import smtplib
import datetime
from datetime import timedelta
import random
from email.header import decode_header
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from urllib.parse import unquote, urlparse
import markdown

# --- ğŸ› ï¸ 1. æ ¸å¿ƒé…ç½®åŒº ---
LLM_API_KEY = os.environ.get("LLM_API_KEY")
LLM_BASE_URL = "https://api.siliconflow.cn/v1"
LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")

EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"

# ğŸŸ¢ å¼€å…³ï¼šæ˜¯å¦å¼€å¯4å°æ—¶å¾ªç¯æ¨¡å¼
# å¦‚æœåœ¨ GitHub Actions è¿è¡Œï¼Œå»ºè®®è®¾ä¸º False (ä½¿ç”¨ .yml å®šæ—¶)
# å¦‚æœåœ¨æœ¬åœ°ç”µè„‘é•¿é©»è¿è¡Œï¼Œè®¾ä¸º True
SCHEDULER_MODE = False 
LOOP_INTERVAL_HOURS = 4

# ç›‘æ§å…³é”®è¯
TARGET_SUBJECTS = [
    "æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", 
    "new research", "Stork", "ScienceDirect", "Chinese politics", 
    "Imperial history", "Causal inference", "new results", "The Accounting Review",
    "recommendations available", "Table of Contents"
]

# ğŸŸ¢ æ•°æ®è®°å½•æ–‡ä»¶è·¯å¾„
DATA_DIR = "data"
HISTORY_0_FILE = os.path.join(DATA_DIR, "history0_scanned.json")   # æ‰€æœ‰æ‰«æåˆ°çš„
HISTORY_3_FILE = os.path.join(DATA_DIR, "history3_downloaded.json") # ä¸‹è½½æˆåŠŸçš„
HISTORY_2_FILE = os.path.join(DATA_DIR, "history2_analyzed.json")   # åˆ†ææˆåŠŸçš„

DOWNLOAD_DIR = "downloads"
MAX_ATTACHMENT_SIZE = 19 * 1024 * 1024
MAX_EMAIL_ZIP_SIZE = 18 * 1024 * 1024 

socket.setdefaulttimeout(30)

client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
cr = Crossref()

DOMAIN_LAST_ACCESSED = {}

# --- ğŸ¨ é‚®ä»¶æ ·å¼ ---
EMAIL_CSS = """
<style>
    body { font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; max-width: 800px; margin: 0 auto; padding: 20px; }
    h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; font-size: 24px; }
    h2 { color: #e67e22; margin-top: 30px; font-size: 20px; border-left: 5px solid #e67e22; padding-left: 10px; background-color: #fdf2e9; }
    h3 { color: #34495e; font-size: 18px; margin-top: 25px; }
    .image-placeholder { background-color: #e8f6f3; border: 1px dashed #1abc9c; color: #16a085; padding: 15px; text-align: center; border-radius: 5px; margin: 20px 0; font-style: italic; }
    .failed-section { background-color: #fff0f0; padding: 15px; border-radius: 5px; border: 1px solid #ffcccc; margin-top: 30px; }
    .failed-item { margin-bottom: 15px; border-bottom: 1px dashed #eee; padding-bottom: 10px; }
    .warning-box { background-color: #fff3cd; color: #856404; padding: 10px; border: 1px solid #ffeeba; border-radius: 5px; margin-top: 20px; font-weight: bold; }
</style>
"""

# --- ğŸ§  2. æ ¸å¿ƒæ¨¡å— ---

def translate_title(text):
    """ç®€å•ç¿»è¯‘æ ‡é¢˜ï¼ˆç”¨äºè®°å½•ï¼‰"""
    if not text or len(text) < 5: return ""
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": f"Translate this academic title to Chinese (only output the translated text): {text}"}],
            temperature=0.1
        )
        return completion.choices[0].message.content.strip()
    except: return ""

def get_metadata_safe(source_data):
    """å°è¯•è·å–æ ‡é¢˜ï¼Œä½†ä¸å¼ºæ±‚ç¿»è¯‘ï¼ˆä¸ºäº†é€Ÿåº¦ï¼‰"""
    title = source_data.get('title', '')
    if title: return title

    s_id = source_data.get('id', '')
    s_type = source_data.get('type', '')
    
    if s_type == 'doi':
        try:
            w = cr.works(ids=s_id)
            title = w['message'].get('title', [''])[0]
        except: pass
    elif s_type == 'arxiv':
        title = f"ArXiv Paper {s_id}"
    
    return title or "Unknown Title"

def get_oa_link_from_doi(doi):
    try:
        email_addr = "bot@example.com"
        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email={email_addr}", timeout=15)
        data = r.json()
        if data.get('is_oa') and data.get('best_oa_location'):
            return data['best_oa_location']['url_for_pdf']
    except: pass
    return None

def extract_titles_from_text(text):
    print("    ğŸ§  [æ™ºèƒ½æå–] æ­£åœ¨åˆ†æé‚®ä»¶æ­£æ–‡æå–æ ‡é¢˜...")
    prompt = f"""
    You are a research assistant. Extract the titles of academic papers from the email text below.
    Rules:
    1. Ignore generic text like "Table of Contents", "Read the full article", "Unsubscribe".
    2. Return ONLY a JSON list of strings. Example: ["Title 1", "Title 2"].
    3. Do not output Markdown.
    
    Email Text:
    {text[:5000]}
    """
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        content = completion.choices[0].message.content.strip()
        content = content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        print(f"    âš ï¸ æ ‡é¢˜æå–å¤±è´¥: {e}")
        return []

def search_doi_by_title(title):
    print(f"    ğŸ” [Crossref] æœç´¢ DOI: {title[:40]}...")
    try:
        results = cr.works(query=title, limit=1)
        if results['message']['items']:
            item = results['message']['items'][0]
            # è¿”å› DOI å’Œ å®˜æ–¹æ ‡é¢˜
            return item.get('DOI'), item.get('title', [title])[0]
    except Exception as e:
        print(f"    âŒ DOI æœç´¢å¤±è´¥: {e}")
    return None, None

def extract_body(msg):
    body_text = ""
    extracted_urls = set()
    
    def find_urls_in_text(text):
        urls = re.findall(r'(https?://[^\s"\'<>]+)', text)
        return [u.rstrip('.,;)]}') for u in urls]

    if msg.is_multipart():
        for part in msg.walk():
            content_type = part.get_content_type()
            disposition = str(part.get("Content-Disposition"))
            try:
                payload = part.get_payload(decode=True)
                if not payload: continue
                part_text = payload.decode(errors='ignore')
                if "attachment" not in disposition:
                    if content_type == "text/html":
                        hrefs = re.findall(r'href=["\']([^"\']+)["\']', part_text, re.IGNORECASE)
                        extracted_urls.update(hrefs)
                        clean_text = re.sub('<[^<]+?>', ' ', part_text)
                        body_text += clean_text + "\n"
                    else:
                        body_text += part_text + "\n"
                extracted_urls.update(find_urls_in_text(part_text))
            except: continue
    else:
        try:
            payload = msg.get_payload(decode=True)
            if payload:
                text = payload.decode(errors='ignore')
                body_text += text
                extracted_urls.update(find_urls_in_text(text))
        except: pass
    return body_text, list(extracted_urls)

def detect_and_extract_all(text, all_links=None):
    results = []
    seen_ids = set()
    
    for match in re.finditer(r"(?:arXiv:|arxiv\.org/abs/|arxiv\.org/pdf/)\s*(\d{4}\.\d{4,5})", text, re.IGNORECASE):
        aid = match.group(1)
        if aid not in seen_ids:
            results.append({"type": "arxiv", "id": aid, "url": f"https://arxiv.org/pdf/{aid}.pdf"})
            seen_ids.add(aid)
    
    for match in re.finditer(r"(?:doi:|doi\.org/)\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.IGNORECASE):
        doi = match.group(1)
        if doi not in seen_ids:
            oa_url = get_oa_link_from_doi(doi)
            results.append({"type": "doi", "id": doi, "url": oa_url})
            seen_ids.add(doi)
    
    ACADEMIC_DOMAINS = [
        'emerald.com', 'researchgate.net', 'wiley.com', 'sciencedirect.com', 
        'springer.com', 'tandfonline.com', 'sagepub.com', 'jstor.org', 'oup.com', 
        'cambridge.org', 'egrove.olemiss.edu'
    ]
    BLOCKED_DOMAINS = ['muse.jhu.edu', 'sciencedirect.com/science/article/pii']
    
    if all_links:
        for link in all_links:
            try:
                link = unquote(link)
                link_lower = link.lower()
                if any(x in link_lower for x in ['unsubscribe', 'privacy', 'manage', 'twitter', 'facebook']): continue
                if any(blk in link_lower for blk in BLOCKED_DOMAINS): continue

                is_pdf = link_lower.endswith('.pdf') or '/pdf/' in link_lower
                is_repo_pdf = 'viewcontent.cgi' in link_lower
                is_content_pdf = 'content/pdf' in link_lower
                is_academic_web = any(d in link_lower for d in ACADEMIC_DOMAINS)
                is_edu = '.edu' in urlparse(link).netloc
                
                if is_pdf or is_repo_pdf or is_content_pdf or is_academic_web or is_edu:
                    link_hash = hashlib.md5(link.encode()).hexdigest()[:10]
                    if link_hash not in seen_ids:
                        if "scholar_url?url=" in link:
                            match = re.search(r'url=([^&]+)', link)
                            if match: link = unquote(match.group(1))
                        source_type = "direct_pdf" if (is_pdf or is_repo_pdf) else "academic_web"
                        results.append({"type": source_type, "id": f"link_{link_hash}", "url": link})
                        seen_ids.add(link_hash)
            except: continue
    return results

def polite_wait(url):
    try:
        if not url: return
        domain = urlparse(url).netloc
        last_time = DOMAIN_LAST_ACCESSED.get(domain, 0)
        cooldown = 5 + random.uniform(1, 3)
        if time.time() - last_time < cooldown: time.sleep(cooldown)
        DOMAIN_LAST_ACCESSED[domain] = time.time()
    except: pass

def fetch_content(source_data, save_dir=None):
    if source_data.get("type") == "arxiv": time.sleep(3)
    url = source_data.get("url")
    if not url: 
        if source_data.get("type") == "doi": return fetch_abstract_only(source_data)
        return None, "No URL", None

    polite_wait(url)
    print(f"    ğŸ” [ä¸‹è½½] å°è¯•è®¿é—®: {url[:50]}...")
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36", "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"}

    try:
        r = requests.get(url, headers=headers, timeout=30, allow_redirects=True, stream=True)
        if r.status_code == 429:
            print("    ğŸ›‘ [429] è¯·æ±‚è¿‡å¤šï¼Œå†·å´ 60ç§’...")
            time.sleep(60)
            return None, "Rate Limited", None
            
        final_url = r.url
        content_type = r.headers.get('Content-Type', '').lower()
        is_pdf_response = ('application/pdf' in content_type or final_url.endswith('.pdf') or 'viewcontent.cgi' in final_url)

        if is_pdf_response:
            print("    ğŸ“¥ ç¡®è®¤ PDFï¼Œä¸‹è½½ä¸­...")
            file_id = source_data.get('id') or hashlib.md5(url.encode()).hexdigest()[:10]
            safe_name = re.sub(r'[\\/*?:"<>|]', '_', file_id)
            filename = os.path.join(save_dir, f"{safe_name}.pdf")
            with open(filename, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192): f.write(chunk)
            
            if os.path.getsize(filename) < 2000:
                print(f"    âš ï¸ æ–‡ä»¶è¿‡å°ï¼Œè·³è¿‡ã€‚")
                os.remove(filename)
                return None, "Fake PDF", None

            try:
                content = pymupdf4llm.to_markdown(filename)
                if len(content) < 500:
                    print(f"    âš ï¸ è§£æå†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡ã€‚")
                    os.remove(filename)
                    return None, "Content Too Short", None
                print(f"    âœ… PDF è§£ææˆåŠŸï¼Œé•¿åº¦: {len(content)}")
                return content, "PDF Full Text", filename
            except: return None, "PDF Error", None

        elif 'text/html' in content_type:
            print("    ğŸŒ æ£€æµ‹åˆ°ç½‘é¡µï¼Œæå–æ­£æ–‡...")
            html_content = ""
            for chunk in r.iter_content(chunk_size=8192):
                html_content += chunk.decode(errors='ignore')
                if len(html_content) > 200000: break 
            text_content = re.sub(r'<[^<]+?>', '\n', html_content)
            text_content = re.sub(r'\n+', '\n', text_content).strip()
            if len(text_content) < 500:
                print(f"    âš ï¸ ç½‘é¡µå†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡ã€‚")
                return None, "Content Too Short", None
            print(f"    âœ… ç½‘é¡µæ–‡æœ¬æå–æˆåŠŸ")
            return text_content, "Web Page Text", None

    except Exception as e:
        print(f"    âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
        if source_data.get("type") == "doi": return fetch_abstract_only(source_data)

    if source_data.get("type") == "doi": return fetch_abstract_only(source_data)
    return None, "Unknown", None

def fetch_abstract_only(source_data):
    try:
        print(f"    ğŸ“š [ä¿åº•] è·å– Crossref æ‘˜è¦...")
        work = cr.works(ids=source_data["id"])
        title = work['message'].get('title', [''])[0]
        abstract = re.sub(r'<[^>]+>', '', work['message'].get('abstract', 'æ— æ‘˜è¦'))
        content = f"# {title}\n\n## Abstract\n{abstract}"
        return content, "Abstract Only", None
    except: return None, "Error", None

def analyze_with_llm(content, content_type, source_url=""):
    prompt = f"""è¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡çŒ®ã€‚æ¥æºï¼š{content_type}ã€‚åœ¨è§£é‡Šæœºåˆ¶æ—¶æ’å…¥ 

[Image of X]
 æ ‡ç­¾ã€‚è¾“å‡º Markdownã€‚\n---\n{content[:50000]}"""
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"LLM åˆ†æå‡ºé”™: {e}"

def generate_failed_report(failed_list):
    if not failed_list: return ""
    report = "\n\n<div class='failed-section'><h2>âš ï¸ æœªèƒ½è·å–å…¨æ–‡çš„æ–‡çŒ® (Skipped/Failed)</h2>"
    report += "<p>ä»¥ä¸‹æ–‡çŒ®å› åçˆ¬è™«éªŒè¯ã€æ–‡ä»¶è¿‡å°æˆ–ä¸‹è½½å¤±è´¥æœªèƒ½è‡ªåŠ¨åˆ†æï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹ï¼š</p>"
    for src in failed_list:
        url = src.get('url', 'No URL')
        s_id = src.get('id', 'Unknown ID')
        sType = src.get('type', 'Unknown')
        title = src.get('title', s_id) 
        report += f"<div class='failed-item'><h3>âŒ {title}</h3><ul><li><strong>URL:</strong> <a href='{url}'>{url}</a></li><li><strong>Type:</strong> {sType}</li></ul></div>"
    report += "</div>"
    return report

# --- ğŸ“§ 3. è¾…åŠ©ä¸æ•°æ®ç®¡ç† ---

def load_json(filepath):
    if os.path.exists(filepath):
        try:
            with open(filepath, "r", encoding="utf-8") as f: return json.load(f)
        except: return []
    return []

def save_json(data, filepath):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f: json.dump(data, f, indent=2, ensure_ascii=False)

def append_to_history(new_items, filepath):
    """è¿½åŠ æ–°è®°å½•åˆ°å†å²æ–‡ä»¶ï¼ˆå»é‡ï¼‰"""
    history = load_json(filepath)
    existing_ids = {item['id'] for item in history}
    added_count = 0
    for item in new_items:
        if item['id'] not in existing_ids:
            history.append(item)
            existing_ids.add(item['id'])
            added_count += 1
    if added_count > 0:
        save_json(history, filepath)
    return added_count

def get_unique_id(source_data):
    return source_data.get("id") or hashlib.md5(source_data.get("url", "").encode()).hexdigest()

def send_email_with_attachment(subject, body_markdown, attachment_zip=None):
    try: html_content = markdown.markdown(body_markdown, extensions=['extra', 'tables', 'fenced_code'])
    except: html_content = body_markdown
    try:
        def replacer(match): return f'<div class="image-placeholder">ğŸ–¼ï¸ å›¾ç¤ºå»ºè®®ï¼š{match.group(1)}</div>'
        html_content = re.sub(r'\]+)\]', replacer, html_content)
    except: pass
    
    final_html = f"<!DOCTYPE html><html><head><meta charset='UTF-8'>{EMAIL_CSS}</head><body>{html_content}<hr><p style='text-align:center;color:#888;font-size:12px;'>Generated by AI Research Assistant | {datetime.date.today()}</p></body></html>"
    msg = MIMEMultipart()
    msg["Subject"] = subject
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER
    msg.attach(MIMEText(final_html, "html", "utf-8"))
    
    if attachment_zip and os.path.exists(attachment_zip):
        if os.path.getsize(attachment_zip) > MAX_EMAIL_ZIP_SIZE:
            print("âš ï¸ é™„ä»¶è¿‡å¤§ (åˆ‡ç‰‡åä¾ç„¶è¿‡å¤§)ï¼Œè·³è¿‡é™„ä»¶å‘é€ã€‚")
            attach_note = f"<div class='warning-box'>âš ï¸ é™„ä»¶è¿‡å¤§ ({os.path.getsize(attachment_zip)/1024/1024:.1f}MB)ï¼Œå·²è‡ªåŠ¨ç§»é™¤ã€‚</div>"
            final_html = final_html.replace("<body>", f"<body>{attach_note}")
            msg = MIMEMultipart()
            msg["Subject"] = subject
            msg["From"] = EMAIL_USER
            msg["To"] = EMAIL_USER
            msg.attach(MIMEText(final_html, "html", "utf-8"))
        else:
            try:
                with open(attachment_zip, "rb") as f:
                    part = MIMEApplication(f.read(), Name=os.path.basename(attachment_zip))
                    part['Content-Disposition'] = f'attachment; filename="{os.path.basename(attachment_zip)}"'
                    msg.attach(part)
            except: pass
    
    try:
        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as server:
            server.login(EMAIL_USER, EMAIL_PASS)
            server.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
        return True
    except Exception as e:
        print(f"å‘é€å¤±è´¥: {e}")
        return False

# --- ğŸš€ 4. ä¸»ä»»åŠ¡æµç¨‹ ---

def run_task():
    print(f"ğŸ¬ ä»»åŠ¡å¯åŠ¨: {datetime.datetime.now()}")
    if os.path.exists(DOWNLOAD_DIR): shutil.rmtree(DOWNLOAD_DIR)
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True) 
    
    history0 = load_json(HISTORY_0_FILE)
    processed_ids = {item['id'] for item in history0}
    
    mail = imaplib.IMAP4_SSL(IMAP_SERVER)
    mail.login(EMAIL_USER, EMAIL_PASS)
    mail.select("inbox")
    
    # ğŸŸ¢ 2. 48å°æ—¶çª—å£
    date_criteria = (datetime.date.today() - timedelta(days=2)).strftime("%d-%b-%Y")
    print(f"ğŸ” æœç´¢ {date_criteria} ä¹‹åçš„é‚®ä»¶...")
    _, data = mail.search(None, f'(SINCE "{date_criteria}")')
    email_list = data[0].split()
    print(f"ğŸ“¨ æ£€ç´¢åˆ° {len(email_list)} å°å€™é€‰é‚®ä»¶")
    
    pending_sources = []
    
    for idx, e_id in enumerate(email_list):
        try:
            time.sleep(1) # åŸºç¡€é˜²å°
            _, header_data = mail.fetch(e_id, "(BODY.PEEK[HEADER])")
            msg_header = email.message_from_bytes(header_data[0][1])
            subj, enc = decode_header(msg_header["Subject"])[0]
            subj = subj.decode(enc or 'utf-8') if isinstance(subj, bytes) else subj
            
            if not any(k.lower() in subj.lower() for k in TARGET_SUBJECTS): continue
            
            print(f"ğŸ¯ å‘½ä¸­: {subj[:30]}...")
            _, m_data = mail.fetch(e_id, "(RFC822)")
            msg = email.message_from_bytes(m_data[0][1])
            
            body_text, all_urls = extract_body(msg)
            print(f"    ğŸ“ é“¾æ¥æ•°: {len(all_urls)}")
            
            sources = detect_and_extract_all(body_text, all_urls)
            
            if not sources:
                print("    ğŸ’¡ å¯ç”¨ LLM æ ‡é¢˜åæŸ¥...")
                titles = extract_titles_from_text(body_text)
                for t in titles:
                    doi, full_title = search_doi_by_title(t)
                    if doi:
                        print(f"    âœ… åæŸ¥ DOI: {doi}")
                        oa_url = get_oa_link_from_doi(doi)
                        sources.append({"type": "doi", "id": doi, "url": oa_url, "title": full_title})
                        time.sleep(1)

            # ğŸŸ¢ è®°å½•åˆ° History 0
            new_scanned = []
            for s in sources:
                u_id = get_unique_id(s)
                s['id'] = u_id
                if 'title' not in s: s['title'] = get_metadata_safe(s)
                
                if u_id not in processed_ids:
                    pending_sources.append(s)
                    new_scanned.append({
                        "id": u_id,
                        "type": s.get('type'),
                        "url": s.get('url'),
                        "title": s.get('title'),
                        "trans_title": "",
                        "timestamp": str(datetime.datetime.now())
                    })
                    processed_ids.add(u_id)
            
            if new_scanned:
                append_to_history(new_scanned, HISTORY_0_FILE)

        except Exception as e:
            print(f"âš ï¸ é”™è¯¯: {e}")
            continue

    MAX_PAPERS = 15
    to_process = pending_sources[:MAX_PAPERS]
    
    if not to_process:
        print("â˜• æ— æ–°æ–‡çŒ®å¤„ç†ã€‚")
        try: mail.logout() 
        except: pass
        return

    print(f"ğŸ“‘ å‡†å¤‡åˆ†æ {len(to_process)} ç¯‡æ–‡çŒ®...")
    report_body, all_files, total_new, failed = "", [], 0, []
    
    history3_records = []
    history2_records = []

    for src in to_process:
        print(f"ğŸ“ å¤„ç†: {src.get('id')}")
        
        # ç¿»è¯‘æ ‡é¢˜
        if not src.get('trans_title') and src.get('title'):
             src['trans_title'] = translate_title(src['title'])
             print(f"    ğŸ‡¨ğŸ‡³ æ ‡é¢˜ç¿»è¯‘: {src['trans_title'][:20]}...")

        content, ctype, path = fetch_content(src, save_dir=DOWNLOAD_DIR)
        
        if path: 
            all_files.append(path)
            # ğŸŸ¢ 3. è®°å½•ä¸‹è½½æˆåŠŸ (History 3)
            history3_records.append({
                "id": src['id'], "title": src.get('title'), 
                "trans_title": src.get('trans_title'), "timestamp": str(datetime.datetime.now())
            })
        
        if content:
            print("ğŸ¤– AI åˆ†æä¸­...")
            ans = analyze_with_llm(content, ctype, src.get('url'))
            if "LLM åˆ†æå‡ºé”™" not in ans:
                report_body += f"## ğŸ“‘ {src.get('title', src['id'])}\n**{src.get('trans_title', '')}**\n\n{ans}\n\n---\n\n"
                total_new += 1
                
                # ğŸŸ¢ 4. è®°å½•åˆ†ææˆåŠŸ (History 2)
                history2_records.append({
                    "id": src['id'], "title": src.get('title'),
                    "trans_title": src.get('trans_title'), "analysis_summary": ans[:100]+"...",
                    "timestamp": str(datetime.datetime.now())
                })
                continue
        failed.append(src)
    
    append_to_history(history3_records, HISTORY_3_FILE)
    append_to_history(history2_records, HISTORY_2_FILE)

    failed_report = generate_failed_report(failed)
    final_report = f"# ğŸ“… æ–‡çŒ®æ—¥æŠ¥ {datetime.date.today()}\n\n" + report_body + failed_report
    
    file_batches = []
    current_batch = []
    current_size = 0
    for f in all_files:
        try:
            f_size = os.path.getsize(f)
            if current_size + f_size > MAX_EMAIL_ZIP_SIZE:
                if current_batch: file_batches.append(current_batch)
                current_batch = [f]
                current_size = f_size
            else:
                current_batch.append(f)
                current_size += f_size
        except: pass
    if current_batch: file_batches.append(current_batch)
    
    if not file_batches:
        if total_new > 0 or failed:
            print("ğŸ“¨ å‘é€çº¯æ–‡æœ¬æŠ¥å‘Š...")
            send_email_with_attachment(f"ğŸ¤– AI å­¦æœ¯æ—¥æŠ¥ (æ–°:{total_new})", final_report, None)
    else:
        print(f"ğŸ“¨ é™„ä»¶è¿‡å¤§ï¼Œåˆ† {len(file_batches)} å°å‘é€...")
        for i, batch in enumerate(file_batches):
            zip_name = f"papers_part_{i+1}.zip"
            with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zf:
                for f in batch: zf.write(f, os.path.basename(f))
            
            if i == 0:
                subject = f"ğŸ¤– AI å­¦æœ¯æ—¥æŠ¥ (æ–°:{total_new}) - Part 1"
                body = final_report
            else:
                subject = f"ğŸ¤– AI å­¦æœ¯æ—¥æŠ¥ (é™„ä»¶ Part {i+1}) - {datetime.date.today()}"
                body = f"# ğŸ“ é™„ä»¶è¡¥å‘ (Part {i+1})\n\nè¿™æ˜¯åç»­çš„ PDF é™„ä»¶åŒ…ã€‚"
            
            send_email_with_attachment(subject, body, zip_name)
            if os.path.exists(zip_name): os.remove(zip_name)
            time.sleep(10)
    
    try: mail.logout()
    except: pass
    print("âœ… æœ¬æ¬¡ä»»åŠ¡å®Œæˆ")

def main():
    if SCHEDULER_MODE:
        print(f"ğŸ”„ å·²å¯åŠ¨å¾ªç¯æ¨¡å¼ï¼Œæ¯ {LOOP_INTERVAL_HOURS} å°æ—¶è¿è¡Œä¸€æ¬¡...")
        while True:
            try:
                run_task()
            except Exception as e:
                print(f"âŒ ä»»åŠ¡å´©æºƒ: {e}")
            
            print(f"ğŸ’¤ ä¼‘çœ  {LOOP_INTERVAL_HOURS} å°æ—¶...")
            time.sleep(LOOP_INTERVAL_HOURS * 3600)
    else:
        run_task()

if __name__ == "__main__":
    main()
