import os
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
import time
import hashlib
import json
import shutil
import zipfile
import socket
import imaplib
import email
import smtplib
import datetime
import random
from email.header import decode_header
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from urllib.parse import unquote, urlparse
import markdown

# --- ğŸ› ï¸ 1. æ ¸å¿ƒé…ç½®åŒº ---
LLM_API_KEY = os.environ.get("LLM_API_KEY")
LLM_BASE_URL = "https://api.siliconflow.cn/v1"
LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")

EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"

# å¢åŠ  ScienceDirect ç›¸å…³å…³é”®è¯
TARGET_SUBJECTS = [
    "æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", 
    "new research", "Stork", "ScienceDirect", "Chinese politics", 
    "Imperial history", "Causal inference", "new results", "The Accounting Review",
    "recommendations available", "Table of Contents"
]

HISTORY_FILE = "data/history.json"
DOWNLOAD_DIR = "downloads"
MAX_ATTACHMENT_SIZE = 19 * 1024 * 1024
socket.setdefaulttimeout(30)

client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
cr = Crossref()

DOMAIN_LAST_ACCESSED = {}

# --- ğŸ¨ é‚®ä»¶æ ·å¼ç¾åŒ– ---
EMAIL_CSS = """
<style>
    body { font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; max-width: 800px; margin: 0 auto; padding: 20px; }
    h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; font-size: 24px; }
    h2 { color: #e67e22; margin-top: 30px; font-size: 20px; border-left: 5px solid #e67e22; padding-left: 10px; background-color: #fdf2e9; }
    .image-placeholder { background-color: #e8f6f3; border: 1px dashed #1abc9c; color: #16a085; padding: 15px; text-align: center; border-radius: 5px; margin: 20px 0; font-style: italic; }
</style>
"""

# --- ğŸ§  2. æ ¸å¿ƒæ¨¡å— ---

def get_oa_link_from_doi(doi):
    """é€šè¿‡ DOI æŸ¥æ‰¾æ— éªŒè¯ç çš„å…è´¹ PDF"""
    try:
        email_addr = "bot@example.com"
        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email={email_addr}", timeout=15)
        data = r.json()
        if data.get('is_oa') and data.get('best_oa_location'):
            return data['best_oa_location']['url_for_pdf']
    except: 
        pass
    return None

def extract_titles_from_text(text):
    """å¼ºåŠ›æ¨¡å¼ï¼šè®© LLM ä»é‚®ä»¶æ­£æ–‡ä¸­æå–æ ‡é¢˜"""
    print("    ğŸ§  [æ™ºèƒ½æå–] æ­£åœ¨åˆ†æé‚®ä»¶æ­£æ–‡æå–æ ‡é¢˜...")
    prompt = f"""
    You are a research assistant. Extract the titles of academic papers from the email text below.
    
    Rules:
    1. Ignore "Table of Contents", "Obituary", "Read the full article", journal names, or author names.
    2. Ignore generic text like "New recommendations available" or "Visit ScienceDirect".
    3. Return ONLY a JSON list of strings. Example: ["Title 1", "Title 2"].
    4. Do not output Markdown.
    
    Email Text:
    {text[:6000]}
    """
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        content = completion.choices[0].message.content.strip()
        content = content.replace("```json", "").replace("```", "").strip()
        titles = json.loads(content)
        print(f"    ğŸ§  æå–åˆ° {len(titles)} ä¸ªæ ‡é¢˜")
        return titles
    except Exception as e:
        print(f"    âš ï¸ æ ‡é¢˜æå–å¤±è´¥: {e}")
        return []

def search_doi_by_title(title):
    """é€šè¿‡æ ‡é¢˜åæŸ¥ DOI"""
    print(f"    ğŸ” [Crossref] æœç´¢ DOI: {title[:40]}...")
    try:
        # ä½¿ç”¨ habanero æœç´¢ï¼Œåªå–ç¬¬ä¸€æ¡
        results = cr.works(query=title, limit=1)
        if results['message']['items']:
            item = results['message']['items'][0]
            # ç®€å•çš„éªŒè¯ï¼šå¦‚æœæ‰¾åˆ°çš„æ ‡é¢˜ä¸æœç´¢çš„æ ‡é¢˜ç›¸ä¼¼åº¦å¤ªä½ï¼Œè¿™é‡Œä¸åšå¤æ‚æ ¡éªŒï¼Œå‡è®¾ Crossref å¤Ÿå‡†
            return item.get('DOI')
    except Exception as e:
        print(f"    âŒ DOI æœç´¢å¤±è´¥: {e}")
    return None

def extract_body(msg):
    body_text = ""
    extracted_urls = set()
    
    # ç®€å•çš„ URL æå–æ­£åˆ™
    def find_urls_in_text(text):
        urls = re.findall(r'(https?://[^\s"\'<>]+)', text)
        return [u.rstrip('.,;)]}') for u in urls]

    if msg.is_multipart():
        for part in msg.walk():
            content_type = part.get_content_type()
            disposition = str(part.get("Content-Disposition"))
            try:
                payload = part.get_payload(decode=True)
                if not payload: continue
                part_text = payload.decode(errors='ignore')
                
                if "attachment" not in disposition:
                    if content_type == "text/html":
                        # æå– href
                        hrefs = re.findall(r'href=["\']([^"\']+)["\']', part_text, re.IGNORECASE)
                        extracted_urls.update(hrefs)
                        # æ¸…ç† HTML æ ‡ç­¾åªç•™æ–‡æœ¬
                        clean_text = re.sub('<[^<]+?>', ' ', part_text)
                        body_text += clean_text + "\n"
                    else:
                        body_text += part_text + "\n"
                
                extracted_urls.update(find_urls_in_text(part_text))
            except: continue
    else:
        try:
            payload = msg.get_payload(decode=True)
            if payload:
                text = payload.decode(errors='ignore')
                body_text += text
                extracted_urls.update(find_urls_in_text(text))
        except: pass
    
    return body_text, list(extracted_urls)

def detect_and_extract_all(text, html_links=None):
    results = []
    seen_ids = set()
    
    # 1. æ£€æµ‹ ArXiv
    for match in re.finditer(r"(?:arXiv:|arxiv\.org/abs/|arxiv\.org/pdf/)\s*(\d{4}\.\d{4,5})", text, re.IGNORECASE):
        aid = match.group(1)
        if aid not in seen_ids:
            results.append({"type": "arxiv", "id": aid, "url": f"https://arxiv.org/pdf/{aid}.pdf"})
            seen_ids.add(aid)
    
    # 2. æ£€æµ‹ DOI
    for match in re.finditer(r"(?:doi:|doi\.org/)\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.IGNORECASE):
        doi = match.group(1)
        if doi not in seen_ids:
            oa_url = get_oa_link_from_doi(doi)
            results.append({"type": "doi", "id": doi, "url": oa_url})
            seen_ids.add(doi)
    
    # 3. å¤„ç†é“¾æ¥
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ•…æ„è·³è¿‡ muse.jhu.edu å’Œ sciencedirect.com çš„ç›´é“¾
    # å› ä¸ºå®ƒä»¬æœ‰é˜²ç«å¢™ï¼Œç›´æ¥æŠ“å– 100% ä¼šå¤±è´¥ï¼Œä¸å¦‚ç›´æ¥ç”¨æ ‡é¢˜åæŸ¥
    BLOCKED_DOMAINS = ['muse.jhu.edu', 'sciencedirect.com', 'linkinghub.elsevier.com']
    
    if html_links:
        for link in html_links:
            try:
                link = unquote(link)
                link_lower = link.lower()

                # è·³è¿‡æ— å…³é“¾æ¥
                if any(x in link_lower for x in ['unsubscribe', 'privacy', 'manage', 'twitter', 'facebook']):
                    continue
                
                # å¦‚æœæ˜¯å·²çŸ¥ä¼šè¢«å¢™çš„åŸŸåï¼Œç›´æ¥è·³è¿‡ï¼Œå¼ºåˆ¶èµ°æ ‡é¢˜åæŸ¥é€»è¾‘
                if any(blk in link_lower for blk in BLOCKED_DOMAINS):
                    continue

                # åªæœ‰æ˜ç¡®æ˜¯ PDF çš„æ‰å°è¯•ç›´è¿
                is_pdf = link_lower.endswith('.pdf') or '/pdf/' in link_lower
                
                if is_pdf:
                    link_hash = hashlib.md5(link.encode()).hexdigest()[:10]
                    if link_hash not in seen_ids:
                        results.append({
                            "type": "direct_pdf",
                            "id": f"link_{link_hash}",
                            "url": link
                        })
                        seen_ids.add(link_hash)
            except: continue
    
    return results

def polite_wait(url):
    """ç®€å•çš„ç¤¼è²Œè®¿é—®å»¶è¿Ÿ"""
    try:
        if not url: return
        domain = urlparse(url).netloc
        last_time = DOMAIN_LAST_ACCESSED.get(domain, 0)
        cooldown = 5 + random.uniform(1, 3)
        if time.time() - last_time < cooldown:
            time.sleep(cooldown)
        DOMAIN_LAST_ACCESSED[domain] = time.time()
    except: pass

def fetch_content(source_data, save_dir=None):
    if source_data.get("type") == "arxiv":
        time.sleep(3)

    url = source_data.get("url")
    if not url: 
        # å¦‚æœæ˜¯ DOI ç±»å‹ä¸”æ²¡æœ‰ URLï¼Œç›´æ¥å°è¯•è·å–æ‘˜è¦
        if source_data.get("type") == "doi":
            return fetch_abstract_only(source_data)
        return None, "No URL", None

    polite_wait(url)
    print(f"    ğŸ” [ä¸‹è½½] å°è¯•è®¿é—®: {url[:50]}...")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    try:
        r = requests.get(url, headers=headers, timeout=30, allow_redirects=True, stream=True)
        if r.status_code == 429:
            time.sleep(60)
            return None, "Rate Limited", None
            
        final_url = r.url
        content_type = r.headers.get('Content-Type', '').lower()
        
        # å¦‚æœæ˜¯ PDF
        if 'application/pdf' in content_type or final_url.endswith('.pdf'):
            file_id = source_data.get('id') or hashlib.md5(url.encode()).hexdigest()[:10]
            safe_name = re.sub(r'[\\/*?:"<>|]', '_', file_id)
            filename = os.path.join(save_dir, f"{safe_name}.pdf")
            
            with open(filename, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            try:
                content = pymupdf4llm.to_markdown(filename)
                print(f"    âœ… PDF ä¸‹è½½å¹¶è§£ææˆåŠŸ")
                return content, "PDF Full Text", filename
            except: 
                return None, "PDF Error", None

        # å¦‚æœä¸æ˜¯ PDF (æ¯”å¦‚è¢«æ‹¦æˆªäº†ï¼Œæˆ–è€…åªæ˜¯ç½‘é¡µ)ï¼Œå¯¹äº DOI æˆ‘ä»¬æœ‰å¤‡é€‰æ–¹æ¡ˆ
        if source_data.get("type") == "doi":
            print("    âš ï¸ ç›´è¿å¤±è´¥æˆ–éPDFï¼Œè½¬ä¸ºè·å– Crossref æ‘˜è¦...")
            return fetch_abstract_only(source_data)

    except Exception as e:
        print(f"    âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
        if source_data.get("type") == "doi":
            return fetch_abstract_only(source_data)

    return None, "Unknown", None

def fetch_abstract_only(source_data):
    """åªè·å–æ‘˜è¦ä½œä¸ºä¿åº•"""
    try:
        print(f"    ğŸ“š [ä¿åº•] æ­£åœ¨ä» Crossref è·å–æ‘˜è¦...")
        work = cr.works(ids=source_data["id"])
        title = work['message'].get('title', [''])[0]
        abstract = re.sub(r'<[^>]+>', '', work['message'].get('abstract', 'ï¼ˆæœªæ‰¾åˆ°æ‘˜è¦ä¿¡æ¯ï¼‰'))
        content = f"# {title}\n\n## Abstract\n{abstract}"
        return content, "Abstract Only", None
    except Exception as e:
        print(f"    âŒ æ‘˜è¦è·å–å¤±è´¥: {e}")
        return None, "Error", None

def analyze_with_llm(content, content_type, source_url=""):
    prompt = f"""è¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡çŒ®ã€‚æ¥æºï¼š{content_type}ã€‚åœ¨è§£é‡Šæœºåˆ¶æ—¶æ’å…¥  æ ‡ç­¾ã€‚è¾“å‡º Markdownã€‚\n---\n{content[:50000]}"""
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"LLM åˆ†æå‡ºé”™: {e}"

# --- ğŸ“§ 3. è¾…åŠ©åŠŸèƒ½ ---

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f: return json.load(f)
        except: return []
    return []

def save_history(history_list):
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f: json.dump(history_list, f, indent=2, ensure_ascii=False)

def get_unique_id(source_data):
    return source_data.get("id") or hashlib.md5(source_data.get("url", "").encode()).hexdigest()

def send_email_with_attachment(subject, body_markdown, attachment_zip=None):
    try:
        html_content = markdown.markdown(body_markdown, extensions=['extra', 'tables', 'fenced_code'])
    except: html_content = body_markdown
    
    # ä¿®å¤æ­£åˆ™è¯­æ³•
    pattern = r"\]+)\]"
    replacement = r'<div class="image-placeholder">ğŸ–¼ï¸ å›¾ç¤ºå»ºè®®ï¼š\1</div>'
    html_content = re.sub(pattern, replacement, html_content)
    
    final_html = f"""
<!DOCTYPE html>
<html>
<head><meta charset="UTF-8">{EMAIL_CSS}</head>
<body>
    {html_content}
    <hr>
    <p style="text-align:center; color:#888; font-size:12px;">Generated by AI Research Assistant | {datetime.date.today()}</p>
</body>
</html>
"""
    msg = MIMEMultipart()
    msg["Subject"] = subject
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER
    msg.attach(MIMEText(final_html, "html", "utf-8"))
    
    if attachment_zip and os.path.exists(attachment_zip):
        try:
            with open(attachment_zip, "rb") as f:
                part = MIMEApplication(f.read(), Name=os.path.basename(attachment_zip))
                part['Content-Disposition'] = f'attachment; filename="{os.path.basename(attachment_zip)}"'
                msg.attach(part)
        except: pass
    
    try:
        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as server:
            server.login(EMAIL_USER, EMAIL_PASS)
            server.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
        return True
    except Exception as e:
        print(f"å‘é€å¤±è´¥: {e}")
        return False

# --- ğŸš€ 4. ä¸»é€»è¾‘ ---

def main():
    print("ğŸ¬ ç¨‹åºå¯åŠ¨ä¸­...")
    if os.path.exists(DOWNLOAD_DIR): shutil.rmtree(DOWNLOAD_DIR)
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    
    processed_ids = load_history()
    
    # ç™»å½• IMAP
    mail = imaplib.IMAP4_SSL(IMAP_SERVER)
    mail.login(EMAIL_USER, EMAIL_PASS)
    mail.select("inbox")
    
    # æœç´¢é‚®ä»¶
    date_str = (datetime.date.today() - datetime.timedelta(days=1)).strftime("%d-%b-%Y")
    _, data = mail.search(None, f'(SINCE "{date_str}")')
    email_list = data[0].split()
    print(f"ğŸ“¨ æ£€ç´¢åˆ° {len(email_list)} å°è¿‘æœŸé‚®ä»¶")
    
    pending_sources = []
    
    for idx, e_id in enumerate(email_list):
        try:
            _, header_data = mail.fetch(e_id, "(BODY.PEEK[HEADER])")
            msg_header = email.message_from_bytes(header_data[0][1])
            subj, enc = decode_header(msg_header["Subject"])[0]
            subj = subj.decode(enc or 'utf-8') if isinstance(subj, bytes) else subj
            
            if not any(k.lower() in subj.lower() for k in TARGET_SUBJECTS):
                continue
            
            print(f"ğŸ¯ å‘½ä¸­å…³é”®è¯: {subj[:30]}...")
            _, m_data = mail.fetch(e_id, "(RFC822)")
            msg = email.message_from_bytes(m_data[0][1])
            
            body_text, html_links = extract_body(msg)
            
            # 1. å°è¯•å¸¸è§„æå–
            sources = detect_and_extract_all(body_text, html_links)
            
            # 2. ğŸŸ¢ å¼ºåˆ¶ä¿åº•æœºåˆ¶ï¼šå¦‚æœæ²¡æ‰¾åˆ°æœ‰æ•ˆ PDF (é’ˆå¯¹ ScienceDirect/MUSE)
            if not sources:
                print("    ğŸ’¡ æœªæ‰¾åˆ°ç›´æ¥ PDFï¼Œå¯ç”¨ LLM æ ‡é¢˜åæŸ¥æ¨¡å¼...")
                titles = extract_titles_from_text(body_text)
                for t in titles:
                    found_doi = search_doi_by_title(t)
                    if found_doi:
                        print(f"    âœ… åæŸ¥ DOI: {found_doi}")
                        # ä¼˜å…ˆå°è¯• Unpaywall çš„ OA é“¾æ¥
                        oa_url = get_oa_link_from_doi(found_doi)
                        sources.append({"type": "doi", "id": found_doi, "url": oa_url})
                        time.sleep(1)

            for s in sources:
                if get_unique_id(s) not in processed_ids:
                    pending_sources.append(s)
                    
        except Exception as e:
            print(f"âš ï¸ é‚®ä»¶è§£æé”™è¯¯: {e}")
            continue

    MAX_PAPERS = 15
    to_process = pending_sources[:MAX_PAPERS]
    
    if not to_process:
        print("â˜• æ— æ–°æ–‡çŒ®ã€‚")
        return

    print(f"ğŸ“‘ å‡†å¤‡åˆ†æ {len(to_process)} ç¯‡æ–‡çŒ®...")
    report_body, all_files, total_new, failed = "", [], 0, []
    
    for src in to_process:
        print(f"ğŸ“ å¤„ç†: {src.get('id', 'Doc')}")
        content, ctype, path = fetch_content(src, save_dir=DOWNLOAD_DIR)
        
        if path: all_files.append(path)
        
        if content:
            print("ğŸ¤– AI åˆ†æä¸­...")
            ans = analyze_with_llm(content, ctype, src.get('url'))
            if "LLM åˆ†æå‡ºé”™" not in ans:
                report_body += f"## ğŸ“‘ {src.get('id', 'Paper')}\n\n{ans}\n\n---\n\n"
                processed_ids.append(get_unique_id(src))
                total_new += 1
                continue
        failed.append(src)
    
    # å‘é€é‚®ä»¶
    final_report = f"# ğŸ“… æ–‡çŒ®æ—¥æŠ¥ {datetime.date.today()}\n\n" + report_body
    if total_new > 0 or failed:
        print("ğŸ“¨ å‘é€é‚®ä»¶ä¸­...")
        zip_file = "papers.zip" if all_files else None
        if zip_file:
            with zipfile.ZipFile(zip_file, 'w') as zf:
                for f in all_files: zf.write(f, os.path.basename(f))
        
        send_email_with_attachment(f"ğŸ¤– AI å­¦æœ¯æ—¥æŠ¥ (æ–°:{total_new})", final_report, zip_file)
        if zip_file and os.path.exists(zip_file): os.remove(zip_file)
    
    save_history(processed_ids)
    print("ğŸ‰ å®Œæˆï¼")

if __name__ == "__main__":
    main()
