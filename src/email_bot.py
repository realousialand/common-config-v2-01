import os
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
import time
import hashlib
import json
import shutil
import zipfile
import socket
import imaplib
import email
import smtplib
import datetime
import logging
from datetime import timedelta
from email.header import decode_header
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from urllib.parse import unquote, urlparse, parse_qs
import markdown
from bs4 import BeautifulSoup
from tenacity import retry, stop_after_attempt, wait_exponential

# --- é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

LLM_API_KEY = os.environ.get("LLM_API_KEY")
LLM_BASE_URL = "https://api.siliconflow.cn/v1"
LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"
SCHEDULER_MODE = False
LOOP_INTERVAL_HOURS = 4
BATCH_SIZE = 20
MAX_RETRIES = 3
TARGET_SUBJECTS = ["æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", "new research", "Stork", "ScienceDirect", "Chinese politics", "Imperial history", "Causal inference", "new results", "The Accounting Review", "recommendations available", "Table of Contents"]
DATA_DIR = "data"
DB_FILE = os.path.join(DATA_DIR, "papers_database.json")
DOWNLOAD_DIR = "downloads"
MAX_EMAIL_ZIP_SIZE = 18 * 1024 * 1024 
socket.setdefaulttimeout(30)
client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
cr = Crossref()
DOMAIN_LAST_ACCESSED = {}

# --- è¾…åŠ© ---
def clean_google_url(url):
    try:
        url = unquote(url)
        if "google" in url and ("url=" in url or "q=" in url):
            parsed = urlparse(url)
            qs = parse_qs(parsed.query)
            if 'url' in qs: return unquote(qs['url'][0])
            if 'q' in qs: return unquote(qs['q'][0])
    except: pass
    return url

def is_valid_academic_text(text):
    if not text or len(text) < 500: return False
    junk_triggers = ["access denied", "security check", "human verification", "cloudflare", "403 forbidden", "404 not found", "robot", "captcha", "please enable cookies"]
    head = text[:1000].lower()
    if any(t in head for t in junk_triggers): return False
    return True

def startup_check():
    logger.info("ğŸ”§ å¯åŠ¨è‡ªæ£€...")
    try:
        tag = chr(91) + "Image of Graph" + chr(93)
        test_str = "Test " + tag
        if "Image" not in test_str: raise ValueError("String Error")
        url = "https://www.google.com/url?q=https://arxiv.org/pdf/1.pdf"
        if "arxiv.org" not in clean_google_url(url): raise ValueError("URL Clean Error")
        logger.info("âœ… è‡ªæ£€é€šè¿‡")
    except Exception as e:
        logger.critical(f"âŒ è‡ªæ£€å¤±è´¥: {e}")
        exit(1)

# --- æ•°æ®åº“ ---
class PaperDB:
    def __init__(self, filepath):
        self.filepath = filepath
        self.data = self._load()

    def _load(self):
        if os.path.exists(self.filepath):
            try:
                with open(self.filepath, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                    if isinstance(content, list): 
                        logger.warning("âš ï¸ ä¿®å¤æ—§ç‰ˆæ•°æ®åº“æ ¼å¼ List->Dict")
                        new_data = {}
                        for item in content:
                            if isinstance(item, dict) and 'id' in item: new_data[item['id']] = item
                        return new_data
                    if isinstance(content, dict): return content
            except: pass
        return {}

    def save(self):
        try:
            os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
            with open(self.filepath, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, indent=2, ensure_ascii=False)
        except Exception as e: logger.error(f"ä¿å­˜å¤±è´¥: {e}")

    def add_new(self, pid, meta):
        if not isinstance(self.data, dict): self.data = {}
        if pid not in self.data:
            self.data[pid] = {**meta, "status": "NEW", "retry": 0, "ts": str(datetime.datetime.now())}
            self.save()
            return True
        return False

    def update_status(self, pid, status, extra=None):
        if pid in self.data:
            self.data[pid]["status"] = status
            if extra: self.data[pid].update(extra)
            self.save()

    def get_pending_downloads(self, limit=BATCH_SIZE):
        res = []
        if not isinstance(self.data, dict): return res
        for pid, item in self.data.items():
            if item["status"] == "NEW": res.append(item)
            elif item["status"] == "DOWNLOAD_FAILED" and item.get("retry", 0) < MAX_RETRIES: res.append(item)
        return res[:limit]

    def get_pending_analysis(self, limit=BATCH_SIZE):
        res = []
        if not isinstance(self.data, dict): return res
        for pid, item in self.data.items():
            if item["status"] in ["DOWNLOADED", "ABSTRACT_ONLY"]: res.append(item)
            elif item["status"] == "ANALYSIS_FAILED" and item.get("retry", 0) < MAX_RETRIES: res.append(item)
        return res[:limit]

    def inc_retry(self, pid):
        if pid in self.data:
            self.data[pid]["retry"] = self.data[pid].get("retry", 0) + 1
            self.save()

# --- æ ¸å¿ƒ ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10), reraise=False)
def translate_title(text):
    if not text or len(text) < 5 or "Unknown" in text: return ""
    try:
        res = client.chat.completions.create(
            model=LLM_MODEL_NAME, messages=[{"role": "user", "content": f"Translate title to Chinese: {text}"}], temperature=0.1
        )
        return res.choices[0].message.content.strip()
    except: return ""

def get_meta_safe(src):
    t = src.get('title', '')
    if t and "Unknown" not in t: return t
    if src.get('type') == 'arxiv': return f"ArXiv {src.get('id')}"
    return "Unknown Title"

def extract_titles(text):
    logger.info("    ğŸ§  [æ™ºèƒ½æå–] æå–æ ‡é¢˜...")
    try:
        res = client.chat.completions.create(
            model=LLM_MODEL_NAME, 
            messages=[{"role": "user", "content": f"Extract academic titles as JSON list. Text: {text[:3000]}"}], temperature=0.1
        )
        return json.loads(res.choices[0].message.content.strip().replace("```json", "").replace("```", "").strip())
    except: return []

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=4, max=20))
def search_doi(title):
    logger.info(f"    ğŸ” [Crossref] {title[:20]}...")
    res = cr.works(query=title, limit=1)
    if res['message']['items']:
        it = res['message']['items'][0]
        return it.get('DOI'), it.get('title', [title])[0]
    return None, None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=4, max=10))
def get_oa_link(doi):
    try:
        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email=bot@example.com", timeout=10)
        if r.status_code == 200:
            d = r.json()
            if d.get('is_oa') and d.get('best_oa_location'): return d['best_oa_location']['url_for_pdf']
    except: pass
    return None

def extract_body_urls(msg):
    text = ""
    urls = set()
    def grep_url(t): return [u.rstrip('.,;)]}') for u in re.findall(r'(https?://[^\s"\'<>]+)', t)]
    if msg.is_multipart():
        for p in msg.walk():
            try:
                payload = p.get_payload(decode=True)
                if not payload: continue
                pt = payload.decode(errors='ignore')
                if p.get_content_type() == "text/html":
                    urls.update(re.findall(r'href=["\']([^"\']+)["\']', pt, re.IGNORECASE))
                    text += re.sub('<[^<]+?>', ' ', pt) + "\n"
                else: text += pt + "\n"
                urls.update(grep_url(pt))
            except: continue
    else:
        try:
            pt = msg.get_payload(decode=True).decode(errors='ignore')
            text += pt
            urls.update(grep_url(pt))
        except: pass
    return text, list(urls)

def detect_sources(text, urls):
    srcs = []
    seen = set()
    for m in re.finditer(r"(?:arXiv:|arxiv\.org/abs/|arxiv\.org/pdf/)\s*(\d{4}\.\d{4,5})", text, re.I):
        if m.group(1) not in seen:
            srcs.append({"type": "arxiv", "id": m.group(1), "url": f"https://arxiv.org/pdf/{m.group(1)}.pdf"})
            seen.add(m.group(1))
    for m in re.finditer(r"(?:doi:|doi\.org/)\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.I):
        doi = m.group(1)
        if doi not in seen:
            try: link = get_oa_link(doi)
            except: link = None
            srcs.append({"type": "doi", "id": doi, "url": link})
            seen.add(doi)
    for link in urls:
        try:
            clink = clean_google_url(link)
            if not clink: continue
            lower = clink.lower()
            if any(x in lower for x in ['unsubscribe', 'twitter', 'facebook']): continue
            if lower.endswith('.pdf') or 'viewcontent.cgi' in lower:
                lid = hashlib.md5(clink.encode()).hexdigest()[:10]
                if lid not in seen:
                    srcs.append({"type": "pdf_link", "id": f"link_{lid}", "url": clink})
                    seen.add(lid)
        except: continue
    return srcs

def get_path(pid):
    safe = re.sub(r'[\\/*?:"<>|]', '_', pid)
    return os.path.join(DOWNLOAD_DIR, f"{safe}.pdf")

# ğŸŸ¢ V23.2 æ™ºèƒ½å—…æ¢å¢å¼ºç‰ˆ
def sniff_real_pdf_link(initial_url, html_content):
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 1. Stork ä¸“ç”¨ ID
        stork_btn = soup.find('a', id='full_text_available_anchor', href=True)
        if stork_btn: return stork_btn['href']

        # 2. å…ƒæ•°æ®
        meta_pdf = soup.find('meta', {'name': 'citation_pdf_url'})
        if meta_pdf and meta_pdf.get('content'): return meta_pdf['content']
            
        # 3. å¹¿è°±ç‰¹å¾æœç´¢ (Class/ID/Href/Text)
        for a in soup.find_all('a', href=True):
            href = a['href'].lower()
            
            # è·å–æ ‡ç­¾å†…çš„æ‰€æœ‰æ–‡å­—ï¼ˆåŒ…æ‹¬éšè—çš„ã€spané‡Œçš„ï¼‰
            text = a.get_text(" ", strip=True).lower()
            
            # è·å– class å±æ€§å­—ç¬¦ä¸²
            classes = " ".join(a.get('class', [])).lower()
            
            # åˆ¤å®šæ¡ä»¶ï¼š
            # A. é“¾æ¥æœ¬èº«å«æœ‰ .pdf æˆ– /article-pdf/ (æœŸåˆŠå¸¸ç”¨è·¯å¾„)
            is_pdf_link = '.pdf' in href or '/article-pdf/' in href
            
            # B. ä¸Šä¸‹æ–‡æš—ç¤ºè¿™æ˜¯ä¸‹è½½æŒ‰é’® (æ–‡å­—æˆ–æ ·å¼åŒ…å« pdf/download)
            is_download_btn = 'pdf' in text or 'download' in text or 'full text' in text or 'pdf' in classes or 'download' in classes
            
            if is_pdf_link and is_download_btn:
                # ä¿®å¤ç›¸å¯¹è·¯å¾„
                if href.startswith('/'):
                    parsed = urlparse(initial_url)
                    return f"{parsed.scheme}://{parsed.netloc}{a['href']}"
                return a['href']
                
    except Exception as e:
        logger.warning(f"    âš ï¸ å—…æ¢å¤±è´¥: {e}")
    return None

def fetch_content(item):
    url = clean_google_url(item.get('url'))
    if not url:
        if item.get("type") == "doi": return fetch_abstract(item)
        return None, "No URL", None
    
    logger.info(f"    ğŸ” [ä¸‹è½½] {url[:40]}...")
    try:
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}, timeout=30, stream=True, allow_redirects=True)
        
        if r.status_code == 429: return None, "Rate Limit", None
        
        final_url = r.url
        ct = r.headers.get('Content-Type', '').lower()
        
        if 'application/pdf' in ct or final_url.lower().endswith('.pdf'):
            fp = get_path(item['id'])
            with open(fp, "wb") as f:
                for chunk in r.iter_content(8192): f.write(chunk)
            if os.path.getsize(fp) < 2000:
                os.remove(fp)
                return None, "Too Small", None
            return pymupdf4llm.to_markdown(fp), "PDF", fp
            
        else:
            logger.info("    ğŸ•µï¸ è¿™æ˜¯ä¸€ä¸ªç½‘é¡µï¼Œå°è¯•å—…æ¢ PDF é“¾æ¥...")
            html_text = r.text
            real_pdf_url = sniff_real_pdf_link(final_url, html_text)
            
            if real_pdf_url:
                logger.info(f"    ğŸš€ å—…æ¢æˆåŠŸï¼ŒäºŒæ¬¡ä¸‹è½½: {real_pdf_url[:40]}...")
                r2 = requests.get(real_pdf_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=30, stream=True)
                if 'application/pdf' in r2.headers.get('Content-Type', '').lower():
                    fp = get_path(item['id'])
                    with open(fp, "wb") as f:
                        for chunk in r2.iter_content(8192): f.write(chunk)
                    if os.path.getsize(fp) > 2000:
                        return pymupdf4llm.to_markdown(fp), "PDF", fp
            
            logger.info("    âš ï¸ æ— æ³•ä¸‹è½½ PDFï¼Œè½¬ä¸ºæ‘˜è¦åˆ†æ")
            if item.get("type") == "doi": return fetch_abstract(item)
            return None, "Not PDF", None

    except Exception as e:
        if item.get("type") == "doi": return fetch_abstract(item)
        return None, str(e), None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
def fetch_abstract(item):
    w = cr.works(ids=item["id"])
    t = w['message'].get('title', [''])[0]
    a = re.sub(r'<[^>]+>', '', w['message'].get('abstract', 'æ— æ‘˜è¦'))
    return f"TITLE: {t}\n\nABSTRACT: {a}", "ABSTRACT_ONLY", None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=5, max=30))
def analyze(txt, ctype):
    if ctype == "ABSTRACT_ONLY":
        title_part = "Unknown"
        abstract_part = txt
        m = re.search(r"TITLE:\s*(.*?)\n\nABSTRACT:\s*(.*)", txt, re.DOTALL)
        if m:
            title_part = m.group(1).strip()
            abstract_part = m.group(2).strip()
            
        sys_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ç¿»è¯‘åŠ©æ‰‹ã€‚"
        user_prompt = f"è¯·å°†ä»¥ä¸‹å­¦æœ¯æ‘˜è¦ç¿»è¯‘æˆé€šé¡ºçš„ä¸­æ–‡ï¼ˆä»…è¾“å‡ºç¿»è¯‘å†…å®¹ï¼Œä¸è¦ä»»ä½•å‰ç¼€ï¼‰ï¼š\n\n{abstract_part}"
        try:
            res = client.chat.completions.create(
                model=LLM_MODEL_NAME, messages=[{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}], temperature=0.3
            )
            trans = res.choices[0].message.content.strip()
            return title_part, f"**ã€æ‘˜è¦ç¿»è¯‘ã€‘**\n\n{trans}"
        except:
            return title_part, f"æ‘˜è¦ç¿»è¯‘å¤±è´¥ã€‚åŸæ–‡ï¼š\n{abstract_part[:500]}..."

    sys_prompt = "ä½ æ˜¯ä¸€åå­¦æœ¯ç ”ç©¶åŠ©æ‰‹ã€‚è¯·åŠ¡å¿…ç”¨ã€ä¸­æ–‡ã€‘å›ç­”ã€‚"
    user_prompt = f"""
    # æ ¼å¼é“å¾‹
    ç¬¬ä¸€è¡Œå¿…é¡»ä¸¥æ ¼è¾“å‡ºè‹±æ–‡åŸæ ‡é¢˜ï¼Œæ ¼å¼ï¼šTITLE: <English Title>
    
    # æ‹’ç­”æŒ‡ä»¤
    å¦‚æœæä¾›çš„ Content æ˜¯é”™è¯¯é¡µé¢ã€æ— æ³•è®¿é—®ã€ä¹±ç æˆ–éå­¦æœ¯è®ºæ–‡ï¼Œè¯·ç›´æ¥å›ç­” "INVALID_CONTENT"ã€‚
    
    # ä»»åŠ¡ï¼šåŸºäºæ–‡çŒ®å†…å®¹ï¼Œç”¨ã€ä¸­æ–‡ã€‘æŒ‰ä»¥ä¸‹æ¿å—æ·±å…¥åˆ†æã€‚è¯·ä½¿ç”¨ Markdown åˆ—è¡¨å’ŒåŠ ç²—çªå‡ºé‡ç‚¹ï¼š
    1. **åŸºæœ¬ä¿¡æ¯**ï¼šæ ‡é¢˜ã€ä½œè€…ã€æœŸåˆŠ/ä¼šè®®ï¼ˆå…¨ç§°ï¼‰ã€å¹´ä»½ã€å…³é”®è¯ã€‚
    2. **ç ”ç©¶é¢†åŸŸ**ï¼šæ¨æ–­é¢†åŸŸåŠå½±å“åŠ›ã€‚
    3. **èƒŒæ™¯ä¸ç¼ºå£**ï¼šç°çŠ¶æ˜¯ä»€ä¹ˆï¼Ÿè§£å†³äº†ä»€ä¹ˆå…·ä½“ç¼ºå£ï¼Ÿ
    4. **æ–¹æ³•è®º**ï¼šå…³é”®æŠ€æœ¯ã€å®éªŒè®¾è®¡ã€ç†è®ºæ¡†æ¶ã€åˆ›æ–°ç‚¹ã€‚
    5. **ç»“æœä¸ç»“è®º**ï¼šæ ¸å¿ƒå®è¯ç»“æœã€‚
    6. **æœ¯è¯­è§£é‡Š**ï¼šè§£é‡Š2-3ä¸ªä¸“ä¸šæœ¯è¯­ï¼ˆé¢å‘éä¸“ä¸šè¯»è€…ï¼‰ã€‚
    7. **è´¡çŒ®åˆ†æ**ï¼šä¸»è¦ä¼˜åŠ¿ä¸è´¡çŒ®ã€‚
    8. **å±€é™ä¸æœªæ¥**ï¼šæ ·æœ¬é‡ã€å‡è®¾é™åˆ¶ç­‰ã€‚
    9. **ç›¸å…³æ–‡çŒ®**ï¼šæ¨è3-5ç¯‡åŸºç¡€æˆ–åç»­ç ”ç©¶ã€‚
    10. **æœç´¢å»ºè®®**ï¼šæ•°æ®åº“æœç´¢å…³é”®è¯ã€‚
    11. **é“¾æ¥ä¿¡æ¯**ï¼šæä¾›DOIé“¾æ¥æˆ–å®˜æ–¹é“¾æ¥ã€‚
    12. **é‡åŒ–ç»†èŠ‚**ï¼šï¼ˆè‹¥ä¸ºé‡åŒ–ç ”ç©¶ï¼‰åˆ—å‡ºæ•°æ®/æ•°æ®é›†ã€å˜é‡ã€æ¨¡å‹ã€ç»Ÿè®¡æ–¹æ³•ã€æ•°æ®æ¥æºã€å¤„ç†æ–¹æ³•ã€ç»“æœã€‚

    ç±»å‹: {ctype}
    å†…å®¹: 
    {txt[:45000]}
    """
    res = client.chat.completions.create(
        model=LLM_MODEL_NAME,
        messages=[{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}],
        temperature=0.3
    )
    raw = res.choices[0].message.content.strip()
    
    if "INVALID_CONTENT" in raw:
        raise ValueError("LLMåˆ¤æ–­å†…å®¹æ— æ•ˆ")

    clean = raw.replace("```markdown", "").replace("```", "").strip()
    
    title = "Unknown"
    body = clean
    m = re.search(r"TITLE:\s*(.*)", clean, re.I)
    if m:
        title = m.group(1).strip()
        body = clean.replace(m.group(0), "").strip()
    return title, body

def md_to_styled_html(md_text):
    html = markdown.markdown(md_text, extensions=['extra', 'nl2br'])
    html = re.sub(r'<h3>', '<h3 style="color:#2c3e50; border-bottom:2px solid #3498db; padding-bottom:8px; margin-top:20px;">', html)
    html = re.sub(r'<strong>', '<strong style="background-color:#fff3cd; padding:0 4px; border-radius:3px; color:#333;">', html)
    html = re.sub(r'<ul>', '<ul style="padding-left:20px; color:#444; line-height:1.6;">', html)
    html = re.sub(r'<li>', '<li style="margin-bottom:5px;">', html)
    html = re.sub(r'<p>', '<p style="margin:10px 0; line-height:1.6; color:#333;">', html)
    return html

def send_mail(subj, md_content, files=[]):
    styled_body = md_to_styled_html(md_content)
    full_html = f"""
    <!DOCTYPE html>
    <html>
    <body style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; background-color: #f4f6f9; padding: 20px; margin: 0;">
        <div style="max-width: 800px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.05); overflow: hidden;">
            <div style="background: #2c3e50; color: white; padding: 20px; text-align: center;">
                <h2 style="margin: 0; font-size: 24px;">{subj}</h2>
                <p style="margin: 5px 0 0 0; opacity: 0.8; font-size: 14px;">{datetime.date.today()}</p>
            </div>
            <div style="padding: 30px;">{styled_body}</div>
            <div style="background: #f8f9fa; padding: 15px; text-align: center; border-top: 1px solid #eee; color: #888; font-size: 12px;">
                Generated by AI Research Assistant ğŸ¤–
            </div>
        </div>
    </body>
    </html>
    """
    
    msg = MIMEMultipart()
    msg["Subject"] = subj
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER
    msg.attach(MIMEText(full_html, "html", "utf-8"))
    
    for f in files:
        if os.path.exists(f):
            try:
                with open(f, "rb") as fp:
                    part = MIMEApplication(fp.read(), Name=os.path.basename(f))
                    part['Content-Disposition'] = f'attachment; filename="{os.path.basename(f)}"'
                    msg.attach(part)
            except: pass
            
    try:
        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as s:
            s.login(EMAIL_USER, EMAIL_PASS)
            s.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
        logger.info(f"âœ… é‚®ä»¶å·²å‘é€: {subj}")
        return True
    except Exception as e:
        logger.error(f"é‚®ä»¶å¤±è´¥: {e}")
        return False

# --- å…¥å£ ---
def run():
    startup_check()
    logger.info(f"ğŸ¬ ä»»åŠ¡å¼€å§‹")
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    
    db = PaperDB(DB_FILE)
    logger.info(f"ğŸ“š æ•°æ®åº“: {type(db.data)}, {len(db.data)} æ¡")

    # 1. æ‰«æ
    try:
        m = imaplib.IMAP4_SSL(IMAP_SERVER)
        m.login(EMAIL_USER, EMAIL_PASS)
        m.select("inbox")
        _, data = m.search(None, f'(SINCE "{(datetime.date.today()-timedelta(days=2)).strftime("%d-%b-%Y")}")')
        if data[0]:
            for eid in data[0].split():
                try:
                    _, h = m.fetch(eid, "(BODY.PEEK[HEADER])")
                    subj = decode_header(email.message_from_bytes(h[0][1])["Subject"])[0][0]
                    if isinstance(subj, bytes): subj = subj.decode()
                    if not any(k.lower() in subj.lower() for k in TARGET_SUBJECTS): continue
                    logger.info(f"ğŸ¯ é‚®ä»¶: {subj[:20]}...")
                    _, b = m.fetch(eid, "(RFC822)")
                    msg = email.message_from_bytes(b[0][1])
                    txt, urls = extract_body_urls(msg)
                    srcs = detect_sources(txt, urls)
                    if not srcs:
                        ts = extract_titles(txt)
                        for t in ts:
                            try:
                                doi, full = search_doi(t)
                                if doi: srcs.append({"type": "doi", "id": doi, "url": get_oa_link(doi)})
                            except: pass
                    for s in srcs:
                        pid = s.get('id') or hashlib.md5(s.get('url','').encode()).hexdigest()[:10]
                        s['id'] = pid
                        if 'title' not in s: s['title'] = get_meta_safe(s)
                        if db.add_new(pid, s): logger.info(f"    â• æ–°å¢: {pid}")
                except: pass
    except Exception as e: logger.error(f"IMAP: {e}")

    # 2. ä¸‹è½½
    pend_dl = db.get_pending_downloads(BATCH_SIZE)
    logger.info(f"ğŸ“¥ å¾…ä¸‹è½½: {len(pend_dl)}")
    for item in pend_dl:
        res, type_, path = fetch_content(item)
        if type_ in ["PDF", "ABSTRACT_ONLY"]:
            db.update_status(item['id'], "DOWNLOADED" if type_=="PDF" else "ABSTRACT_ONLY", 
                           {"local_path": path, "content_type": type_, "abstract_content": res if type_=="ABSTRACT_ONLY" else ""})
        else:
            db.inc_retry(item['id'])
            db.update_status(item['id'], "DOWNLOAD_FAILED")

    # 3. åˆ†æ
    pend_an = db.get_pending_analysis(BATCH_SIZE)
    logger.info(f"ğŸ¤– å¾…åˆ†æ: {len(pend_an)}")
    reports, atts = [], []
    first_sent = False

    for item in pend_an:
        pid = item['id']
        txt, ctype = "", item.get("content_type", "Unknown")
        if item["status"] == "DOWNLOADED":
            fp = get_path(pid)
            if not os.path.exists(fp):
                _, ctype, fp = fetch_content(item)
                if not fp: 
                    db.update_status(pid, "DOWNLOAD_FAILED")
                    continue
            try: txt = pymupdf4llm.to_markdown(fp)
            except: db.update_status(pid, "ANALYSIS_FAILED"); continue
            atts.append(fp)
        elif item["status"] == "ABSTRACT_ONLY":
            txt = item.get("abstract_content", "")
            if not txt:
                try: txt, _, _ = fetch_abstract(item)
                except: db.inc_retry(pid); continue
        
        try:
            logger.info(f"åˆ†æ: {pid}")
            rt, ans = analyze(txt, ctype)
            disp = rt if ("Unknown" not in rt and rt) else item.get('title', 'Unknown')
            tt = translate_title(disp)
            badge = " (ä»…æ‘˜è¦)" if ctype == "ABSTRACT_ONLY" else ""
            
            card = f"""
### {disp} {badge}
> **{tt}**

{ans}
            """
            reports.append(card)
            db.update_status(pid, "ANALYZED", {"real_title": disp})

            if not first_sent:
                logger.info("ğŸš€ é¦–å•å³é€...")
                att_list = [fp] if (item["status"]=="DOWNLOADED" and os.path.exists(fp)) else []
                send_mail(f"âš¡ [é¢„è§ˆ] {disp}", card, att_list)
                first_sent = True

        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {e}")
            db.inc_retry(pid)
            db.update_status(pid, "ANALYSIS_FAILED")

    # 4. å‘é€
    if reports:
        if len(reports) == 1 and first_sent: pass
        else:
            zips = []
            cz, csz = [], 0
            for f in atts:
                s = os.path.getsize(f)
                if csz+s > MAX_EMAIL_ZIP_SIZE: zips.append(cz); cz, csz = [f], s
                else: cz.append(f); csz += s
            if cz: zips.append(cz)
            
            full_md = "\n\n---\n\n".join(reports)
            
            if not zips: send_mail(f"ğŸ¤– AI æ—¥æŠ¥ ({len(reports)})", full_md)
            else:
                for i, zf in enumerate(zips):
                    zn = f"p_{i+1}.zip"
                    with zipfile.ZipFile(zn, 'w', zipfile.ZIP_DEFLATED) as z:
                        for f in zf: z.write(f, os.path.basename(f))
                    send_mail(f"ğŸ¤– AI æ—¥æŠ¥ ({i+1})", full_md if i==0 else "é™„ä»¶", [zn])
                    if os.path.exists(zn): os.remove(zn)
                    time.sleep(5)
    logger.info("âœ… å®Œæˆ")

if __name__ == "__main__":
    if SCHEDULER_MODE:
        while True:
            try: run()
            except: pass
            time.sleep(LOOP_INTERVAL_HOURS * 3600)
    else:
        run()
