import os
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
import time
import hashlib
import json
import shutil
import zipfile
import socket
import imaplib
import email
import smtplib
import datetime
import logging
from datetime import timedelta
from email.header import decode_header
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from urllib.parse import unquote, urlparse, parse_qs
import markdown
from tenacity import retry, stop_after_attempt, wait_exponential

# --- é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

LLM_API_KEY = os.environ.get("LLM_API_KEY")
LLM_BASE_URL = "https://api.siliconflow.cn/v1"
LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"
SCHEDULER_MODE = False
LOOP_INTERVAL_HOURS = 4
BATCH_SIZE = 20
MAX_RETRIES = 3
TARGET_SUBJECTS = ["æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", "new research", "Stork", "ScienceDirect", "Chinese politics", "Imperial history", "Causal inference", "new results", "The Accounting Review", "recommendations available", "Table of Contents"]
DATA_DIR = "data"
DB_FILE = os.path.join(DATA_DIR, "papers_database.json")
DOWNLOAD_DIR = "downloads"
MAX_EMAIL_ZIP_SIZE = 18 * 1024 * 1024 
socket.setdefaulttimeout(30)
client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
cr = Crossref()
DOMAIN_LAST_ACCESSED = {}

# --- è¾…åŠ© ---
def clean_google_url(url):
    try:
        url = unquote(url)
        if "google" in url and ("url=" in url or "q=" in url):
            parsed = urlparse(url)
            qs = parse_qs(parsed.query)
            if 'url' in qs: return unquote(qs['url'][0])
            if 'q' in qs: return unquote(qs['q'][0])
    except: pass
    return url

def startup_check():
    logger.info("ğŸ”§ å¯åŠ¨è‡ªæ£€...")
    try:
        tag = chr(91) + "Image of Graph" + chr(93)
        test_str = "Test " + tag
        if "Image" not in test_str: raise ValueError("String Error")
        url = "https://www.google.com/url?q=https://arxiv.org/pdf/1.pdf"
        if "arxiv.org" not in clean_google_url(url): raise ValueError("URL Clean Error")
        logger.info("âœ… è‡ªæ£€é€šè¿‡")
    except Exception as e:
        logger.critical(f"âŒ è‡ªæ£€å¤±è´¥: {e}")
        exit(1)

# --- æ•°æ®åº“ ---
class PaperDB:
    def __init__(self, filepath):
        self.filepath = filepath
        self.data = self._load()

    def _load(self):
        if os.path.exists(self.filepath):
            try:
                with open(self.filepath, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                    if isinstance(content, list): 
                        logger.warning("âš ï¸ ä¿®å¤æ—§ç‰ˆæ•°æ®åº“æ ¼å¼ List->Dict")
                        new_data = {}
                        for item in content:
                            if isinstance(item, dict) and 'id' in item: new_data[item['id']] = item
                        return new_data
                    if isinstance(content, dict): return content
            except: pass
        return {}

    def save(self):
        try:
            os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
            with open(self.filepath, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, indent=2, ensure_ascii=False)
        except Exception as e: logger.error(f"ä¿å­˜å¤±è´¥: {e}")

    def add_new(self, pid, meta):
        if not isinstance(self.data, dict): self.data = {}
        if pid not in self.data:
            self.data[pid] = {**meta, "status": "NEW", "retry": 0, "ts": str(datetime.datetime.now())}
            self.save()
            return True
        return False

    def update_status(self, pid, status, extra=None):
        if pid in self.data:
            self.data[pid]["status"] = status
            if extra: self.data[pid].update(extra)
            self.save()

    def get_pending_downloads(self, limit=BATCH_SIZE):
        res = []
        if not isinstance(self.data, dict): return res
        for pid, item in self.data.items():
            if item["status"] == "NEW": res.append(item)
            elif item["status"] == "DOWNLOAD_FAILED" and item.get("retry", 0) < MAX_RETRIES: res.append(item)
        return res[:limit]

    def get_pending_analysis(self, limit=BATCH_SIZE):
        res = []
        if not isinstance(self.data, dict): return res
        for pid, item in self.data.items():
            if item["status"] in ["DOWNLOADED", "ABSTRACT_ONLY"]: res.append(item)
            elif item["status"] == "ANALYSIS_FAILED" and item.get("retry", 0) < MAX_RETRIES: res.append(item)
        return res[:limit]

    def inc_retry(self, pid):
        if pid in self.data:
            self.data[pid]["retry"] = self.data[pid].get("retry", 0) + 1
            self.save()

# --- æ ¸å¿ƒ ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10), reraise=False)
def translate_title(text):
    if not text or len(text) < 5 or "Unknown" in text: return ""
    try:
        res = client.chat.completions.create(
            model=LLM_MODEL_NAME, messages=[{"role": "user", "content": f"Translate title to Chinese: {text}"}], temperature=0.1
        )
        return res.choices[0].message.content.strip()
    except: return ""

def get_meta_safe(src):
    t = src.get('title', '')
    if t and "Unknown" not in t: return t
    if src.get('type') == 'arxiv': return f"ArXiv {src.get('id')}"
    return "Unknown Title"

def extract_titles(text):
    logger.info("    ğŸ§  [æ™ºèƒ½æå–] æå–æ ‡é¢˜...")
    try:
        res = client.chat.completions.create(
            model=LLM_MODEL_NAME, 
            messages=[{"role": "user", "content": f"Extract academic titles as JSON list. Text: {text[:3000]}"}], temperature=0.1
        )
        return json.loads(res.choices[0].message.content.strip().replace("```json", "").replace("```", "").strip())
    except: return []

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=4, max=20))
def search_doi(title):
    logger.info(f"    ğŸ” [Crossref] {title[:20]}...")
    res = cr.works(query=title, limit=1)
    if res['message']['items']:
        it = res['message']['items'][0]
        return it.get('DOI'), it.get('title', [title])[0]
    return None, None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=4, max=10))
def get_oa_link(doi):
    try:
        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email=bot@example.com", timeout=10)
        if r.status_code == 200:
            d = r.json()
            if d.get('is_oa') and d.get('best_oa_location'): return d['best_oa_location']['url_for_pdf']
    except: pass
    return None

def extract_body_urls(msg):
    text = ""
    urls = set()
    def grep_url(t): return [u.rstrip('.,;)]}') for u in re.findall(r'(https?://[^\s"\'<>]+)', t)]
    if msg.is_multipart():
        for p in msg.walk():
            try:
                payload = p.get_payload(decode=True)
                if not payload: continue
                pt = payload.decode(errors='ignore')
                if p.get_content_type() == "text/html":
                    urls.update(re.findall(r'href=["\']([^"\']+)["\']', pt, re.IGNORECASE))
                    text += re.sub('<[^<]+?>', ' ', pt) + "\n"
                else: text += pt + "\n"
                urls.update(grep_url(pt))
            except: continue
    else:
        try:
            pt = msg.get_payload(decode=True).decode(errors='ignore')
            text += pt
            urls.update(grep_url(pt))
        except: pass
    return text, list(urls)

def detect_sources(text, urls):
    srcs = []
    seen = set()
    for m in re.finditer(r"(?:arXiv:|arxiv\.org/abs/|arxiv\.org/pdf/)\s*(\d{4}\.\d{4,5})", text, re.I):
        if m.group(1) not in seen:
            srcs.append({"type": "arxiv", "id": m.group(1), "url": f"https://arxiv.org/pdf/{m.group(1)}.pdf"})
            seen.add(m.group(1))
    for m in re.finditer(r"(?:doi:|doi\.org/)\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.I):
        doi = m.group(1)
        if doi not in seen:
            try: link = get_oa_link(doi)
            except: link = None
            srcs.append({"type": "doi", "id": doi, "url": link})
            seen.add(doi)
    for link in urls:
        try:
            clink = clean_google_url(link)
            if not clink: continue
            lower = clink.lower()
            if any(x in lower for x in ['unsubscribe', 'twitter', 'facebook']): continue
            if lower.endswith('.pdf') or 'viewcontent.cgi' in lower:
                lid = hashlib.md5(clink.encode()).hexdigest()[:10]
                if lid not in seen:
                    srcs.append({"type": "pdf_link", "id": f"link_{lid}", "url": clink})
                    seen.add(lid)
        except: continue
    return srcs

def get_path(pid):
    safe = re.sub(r'[\\/*?:"<>|]', '_', pid)
    return os.path.join(DOWNLOAD_DIR, f"{safe}.pdf")

def fetch_content(item):
    url = clean_google_url(item.get('url'))
    if not url:
        if item.get("type") == "doi": return fetch_abstract(item)
        return None, "No URL", None
    logger.info(f"    ğŸ” [ä¸‹è½½] {url[:40]}...")
    try:
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=30, stream=True)
        if r.status_code == 429: return None, "Rate Limit", None
        ct = r.headers.get('Content-Type', '').lower()
        if 'application/pdf' not in ct and not url.lower().endswith('.pdf'):
            if item.get("type") == "doi": return fetch_abstract(item)
            return None, "Not PDF", None
        fp = get_path(item['id'])
        with open(fp, "wb") as f:
            for chunk in r.iter_content(8192): f.write(chunk)
        if os.path.getsize(fp) < 2000:
            os.remove(fp)
            if item.get("type") == "doi": return fetch_abstract(item)
            return None, "Too Small", None
        try:
            txt = pymupdf4llm.to_markdown(fp)
            if len(txt) < 500:
                os.remove(fp)
                if item.get("type") == "doi": return fetch_abstract(item)
                return None, "Empty", None
            return txt, "PDF", fp
        except:
            if item.get("type") == "doi": return fetch_abstract(item)
            return None, "Parse Error", None
    except Exception as e:
        if item.get("type") == "doi": return fetch_abstract(item)
        return None, str(e), None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
def fetch_abstract(item):
    w = cr.works(ids=item["id"])
    t = w['message'].get('title', [''])[0]
    a = re.sub(r'<[^>]+>', '', w['message'].get('abstract', 'æ— æ‘˜è¦'))
    return f"TITLE: {t}\n\nABSTRACT: {a}", "ABSTRACT_ONLY", None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=5, max=30))
def analyze(txt, ctype):
    # ğŸŸ¢ 1. æ‘˜è¦æ¨¡å¼ï¼šä»…ç¿»è¯‘
    if ctype == "ABSTRACT_ONLY":
        # å°è¯•ä» txt ä¸­æå–æ ‡é¢˜å’Œæ‘˜è¦æ­£æ–‡ (æ ¼å¼é€šå¸¸æ˜¯ TITLE: ... \n\n ABSTRACT: ...)
        title_part = "Unknown"
        abstract_part = txt
        m = re.search(r"TITLE:\s*(.*?)\n\nABSTRACT:\s*(.*)", txt, re.DOTALL)
        if m:
            title_part = m.group(1).strip()
            abstract_part = m.group(2).strip()
            
        sys_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ç¿»è¯‘åŠ©æ‰‹ã€‚"
        user_prompt = f"è¯·å°†ä»¥ä¸‹å­¦æœ¯æ‘˜è¦ç¿»è¯‘æˆé€šé¡ºçš„ä¸­æ–‡ï¼ˆä»…è¾“å‡ºç¿»è¯‘å†…å®¹ï¼Œä¸è¦ä»»ä½•å‰ç¼€ï¼‰ï¼š\n\n{abstract_part}"
        
        try:
            res = client.chat.completions.create(
                model=LLM_MODEL_NAME, messages=[{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}], temperature=0.3
            )
            trans = res.choices[0].message.content.strip()
            # è¿”å›ç»“æ„ï¼š(è‹±æ–‡æ ‡é¢˜, ç¿»è¯‘åçš„ä¸­æ–‡æ‘˜è¦)
            return title_part, f"**ã€æ‘˜è¦ç¿»è¯‘ã€‘**\n{trans}"
        except:
            return title_part, f"æ‘˜è¦ç¿»è¯‘å¤±è´¥ã€‚åŸæ–‡ï¼š\n{abstract_part[:500]}..."

    # ğŸŸ¢ 2. å…¨æ–‡æ¨¡å¼ï¼š13ç‚¹æ·±åº¦åˆ†æï¼ˆä¸­æ–‡å¼ºåˆ¶ï¼‰
    sys_prompt = "ä½ æ˜¯ä¸€åå­¦æœ¯ç ”ç©¶åŠ©æ‰‹ã€‚è¯·åŠ¡å¿…ç”¨ã€ä¸­æ–‡ã€‘å›ç­”ã€‚"
    user_prompt = f"""
    # æ ¼å¼é“å¾‹
    ç¬¬ä¸€è¡Œå¿…é¡»ä¸¥æ ¼è¾“å‡ºè‹±æ–‡åŸæ ‡é¢˜ï¼Œæ ¼å¼ï¼šTITLE: <English Title>
    
    # ä»»åŠ¡ï¼šåŸºäºæ–‡çŒ®å†…å®¹ï¼Œç”¨ã€ä¸­æ–‡ã€‘æŒ‰ä»¥ä¸‹æ¿å—æ·±å…¥åˆ†æï¼š
    1. **åŸºæœ¬ä¿¡æ¯**ï¼šæ ‡é¢˜ã€ä½œè€…ã€æœŸåˆŠ/ä¼šè®®ï¼ˆå…¨ç§°ï¼‰ã€å¹´ä»½ã€å…³é”®è¯ã€‚
    2. **ç ”ç©¶é¢†åŸŸ**ï¼šæ¨æ–­é¢†åŸŸåŠå½±å“åŠ›ã€‚
    3. **èƒŒæ™¯ä¸ç¼ºå£**ï¼šç°çŠ¶æ˜¯ä»€ä¹ˆï¼Ÿè§£å†³äº†ä»€ä¹ˆå…·ä½“ç¼ºå£ï¼Ÿ
    4. **æ–¹æ³•è®º**ï¼šå…³é”®æŠ€æœ¯ã€å®éªŒè®¾è®¡ã€ç†è®ºæ¡†æ¶ã€åˆ›æ–°ç‚¹ã€‚
    5. **ç»“æœä¸ç»“è®º**ï¼šæ ¸å¿ƒå®è¯ç»“æœã€‚
    6. **æœ¯è¯­è§£é‡Š**ï¼šè§£é‡Š2-3ä¸ªä¸“ä¸šæœ¯è¯­ï¼ˆé¢å‘éä¸“ä¸šè¯»è€…ï¼‰ã€‚
    7. **è´¡çŒ®åˆ†æ**ï¼šä¸»è¦ä¼˜åŠ¿ä¸è´¡çŒ®ã€‚
    8. **å±€é™ä¸æœªæ¥**ï¼šæ ·æœ¬é‡ã€å‡è®¾é™åˆ¶ç­‰ã€‚
    9. **ç›¸å…³æ–‡çŒ®**ï¼šæ¨è3-5ç¯‡åŸºç¡€æˆ–åç»­ç ”ç©¶ã€‚
    10. **æœç´¢å»ºè®®**ï¼šæ•°æ®åº“æœç´¢å…³é”®è¯ã€‚
    11. **é“¾æ¥ä¿¡æ¯**ï¼šæä¾›DOIé“¾æ¥æˆ–å®˜æ–¹é“¾æ¥ã€‚
    12. **é‡åŒ–ç»†èŠ‚**ï¼šï¼ˆè‹¥ä¸ºé‡åŒ–ç ”ç©¶ï¼‰åˆ—å‡ºæ•°æ®/æ•°æ®é›†ã€å˜é‡ã€æ¨¡å‹ã€ç»Ÿè®¡æ–¹æ³•ã€æ•°æ®æ¥æºã€å¤„ç†æ–¹æ³•ã€ç»“æœã€‚

    ç±»å‹: {ctype}
    å†…å®¹: 
    {txt[:45000]}
    """
    res = client.chat.completions.create(
        model=LLM_MODEL_NAME,
        messages=[{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}],
        temperature=0.3
    )
    raw = res.choices[0].message.content.strip()
    clean = raw.replace("```markdown", "").replace("```", "").strip()
    
    title = "Unknown"
    body = clean
    m = re.search(r"TITLE:\s*(.*)", clean, re.I)
    if m:
        title = m.group(1).strip()
        body = clean.replace(m.group(0), "").strip()
    return title, body

def send_mail(subj, md_body, files=[]):
    html = markdown.markdown(md_body, extensions=['extra'])
    full_html = f"""
    <html><body style="font-family:sans-serif;padding:20px">
    <div style="background:#2c3e50;color:white;padding:15px;border-radius:5px">
        <h2>{subj}</h2><p>{datetime.date.today()}</p>
    </div>
    {html}
    <hr><p style="color:#888;font-size:12px">AI Assistant</p>
    </body></html>
    """
    msg = MIMEMultipart()
    msg["Subject"] = subj
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER
    msg.attach(MIMEText(full_html, "html", "utf-8"))
    
    for f in files:
        if os.path.exists(f):
            try:
                with open(f, "rb") as fp:
                    part = MIMEApplication(fp.read(), Name=os.path.basename(f))
                    part['Content-Disposition'] = f'attachment; filename="{os.path.basename(f)}"'
                    msg.attach(part)
            except: pass
            
    try:
        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as s:
            s.login(EMAIL_USER, EMAIL_PASS)
            s.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
        logger.info("âœ… é‚®ä»¶å·²å‘é€")
        return True
    except Exception as e:
        logger.error(f"é‚®ä»¶å¤±è´¥: {e}")
        return False

# --- å…¥å£ ---
def run():
    startup_check()
    logger.info(f"ğŸ¬ ä»»åŠ¡å¼€å§‹")
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    
    db = PaperDB(DB_FILE)
    logger.info(f"ğŸ“š æ•°æ®åº“: {type(db.data)}, {len(db.data)} æ¡")

    # 1. æ‰«æ
    try:
        m = imaplib.IMAP4_SSL(IMAP_SERVER)
        m.login(EMAIL_USER, EMAIL_PASS)
        m.select("inbox")
        _, data = m.search(None, f'(SINCE "{(datetime.date.today()-timedelta(days=2)).strftime("%d-%b-%Y")}")')
        if data[0]:
            for eid in data[0].split():
                try:
                    _, h = m.fetch(eid, "(BODY.PEEK[HEADER])")
                    subj = decode_header(email.message_from_bytes(h[0][1])["Subject"])[0][0]
                    if isinstance(subj, bytes): subj = subj.decode()
                    if not any(k.lower() in subj.lower() for k in TARGET_SUBJECTS): continue
                    logger.info(f"ğŸ¯ é‚®ä»¶: {subj[:20]}...")
                    _, b = m.fetch(eid, "(RFC822)")
                    msg = email.message_from_bytes(b[0][1])
                    txt, urls = extract_body_urls(msg)
                    srcs = detect_sources(txt, urls)
                    if not srcs:
                        ts = extract_titles(txt)
                        for t in ts:
                            try:
                                doi, full = search_doi(t)
                                if doi: srcs.append({"type": "doi", "id": doi, "url": get_oa_link(doi)})
                            except: pass
                    for s in srcs:
                        pid = s.get('id') or hashlib.md5(s.get('url','').encode()).hexdigest()[:10]
                        s['id'] = pid
                        if 'title' not in s: s['title'] = get_meta_safe(s)
                        if db.add_new(pid, s): logger.info(f"    â• æ–°å¢: {pid}")
                except: pass
    except Exception as e: logger.error(f"IMAP: {e}")

    # 2. ä¸‹è½½
    pend_dl = db.get_pending_downloads(BATCH_SIZE)
    logger.info(f"ğŸ“¥ å¾…ä¸‹è½½: {len(pend_dl)}")
    for item in pend_dl:
        res, type_, path = fetch_content(item)
        if type_ in ["PDF", "ABSTRACT_ONLY"]:
            db.update_status(item['id'], "DOWNLOADED" if type_=="PDF" else "ABSTRACT_ONLY", 
                           {"local_path": path, "content_type": type_, "abstract_content": res if type_=="ABSTRACT_ONLY" else ""})
        else:
            db.inc_retry(item['id'])
            db.update_status(item['id'], "DOWNLOAD_FAILED")

    # 3. åˆ†æ
    pend_an = db.get_pending_analysis(BATCH_SIZE)
    logger.info(f"ğŸ¤– å¾…åˆ†æ: {len(pend_an)}")
    reports, atts = [], []
    for item in pend_an:
        pid = item['id']
        txt, ctype = "", item.get("content_type", "Unknown")
        if item["status"] == "DOWNLOADED":
            fp = get_path(pid)
            if not os.path.exists(fp):
                _, ctype, fp = fetch_content(item)
                if not fp: 
                    db.update_status(pid, "DOWNLOAD_FAILED")
                    continue
            try: txt = pymupdf4llm.to_markdown(fp)
            except: db.update_status(pid, "ANALYSIS_FAILED"); continue
            atts.append(fp)
        elif item["status"] == "ABSTRACT_ONLY":
            txt = item.get("abstract_content", "")
            if not txt:
                try: txt, _, _ = fetch_abstract(item)
                except: db.inc_retry(pid); continue
        
        try:
            logger.info(f"åˆ†æ: {pid}")
            rt, ans = analyze(txt, ctype)
            disp = rt if ("Unknown" not in rt and rt) else item.get('title', 'Unknown')
            tt = translate_title(disp)
            # åªæœ‰å½“ä¸æ˜¯æ‘˜è¦æ¨¡å¼æ—¶ï¼Œæ‰åœ¨UIä¸Šæ ‡è®°ï¼ˆå¦‚æœæ˜¯æ‘˜è¦ï¼Œå†…å®¹æœ¬èº«å·²ç»è¯´æ˜æ˜¯ç¿»è¯‘äº†ï¼‰
            badge = "<span style='background:#eee;padding:2px 5px;font-size:12px'>æ‘˜è¦ç¿»è¯‘</span>" if ctype == "ABSTRACT_ONLY" else ""
            
            card = f"""<div style="border:1px solid #ccc;padding:15px;margin-bottom:20px;">
            <h3>{disp} {badge}</h3>
            <p style="color:#666;font-weight:bold">{tt}</p>
            <div>{ans}</div>
            </div>"""
            reports.append(card)
            db.update_status(pid, "ANALYZED", {"real_title": disp})
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {e}")
            db.inc_retry(pid)
            db.update_status(pid, "ANALYSIS_FAILED")

    # 4. å‘é€
    if reports:
        zips = []
        cz, csz = [], 0
        for f in atts:
            s = os.path.getsize(f)
            if csz+s > MAX_EMAIL_ZIP_SIZE: zips.append(cz); cz, csz = [f], s
            else: cz.append(f); csz += s
        if cz: zips.append(cz)
        
        if not zips: send_mail(f"ğŸ¤– AI æ—¥æŠ¥ ({len(reports)})", "\n".join(reports))
        else:
            for i, zf in enumerate(zips):
                zn = f"p_{i+1}.zip"
                with zipfile.ZipFile(zn, 'w', zipfile.ZIP_DEFLATED) as z:
                    for f in zf: z.write(f, os.path.basename(f))
                send_mail(f"ğŸ¤– AI æ—¥æŠ¥ ({i+1})", "\n".join(reports) if i==0 else "é™„ä»¶", [zn])
                if os.path.exists(zn): os.remove(zn)
                time.sleep(5)
    logger.info("âœ… å®Œæˆ")

if __name__ == "__main__":
    if SCHEDULER_MODE:
        while True:
            try: run()
            except: pass
            time.sleep(LOOP_INTERVAL_HOURS * 3600)
    else:
        run()
