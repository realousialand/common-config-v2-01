import os
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
import time
import hashlib
import json
import shutil
import zipfile
import socket
import imaplib
import email
import smtplib
import datetime
from email.header import decode_header
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication

# --- ğŸ› ï¸ 1. æ ¸å¿ƒé…ç½®åŒº (ç¯å¢ƒå˜é‡ä¼˜å…ˆ) ---
# è¿™é‡Œçš„é…ç½®ç›´æ¥å†™æ­»æˆ–è¯»å–ç¯å¢ƒå˜é‡ï¼Œä¸å†ä¾èµ–å¤–éƒ¨æ–‡ä»¶
LLM_API_KEY = os.environ.get("LLM_API_KEY")  # å¿…å¡«
LLM_BASE_URL = "https://api.siliconflow.cn/v1"
LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")

EMAIL_USER = os.environ.get("EMAIL_USER")     # å¿…å¡«
EMAIL_PASS = os.environ.get("EMAIL_PASS")     # å¿…å¡«
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"

# ç›‘æ§å…³é”®è¯ (æ ¹æ®ä½ çš„åšå£«ç ”ç©¶æ–¹å‘å®šåˆ¶)
TARGET_SUBJECTS = [
    "æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", 
    "new research", "Stork", "ScienceDirect", "Chinese politics", 
    "Imperial history", "Causal inference"
]

# æœ¬åœ°æ–‡ä»¶è·¯å¾„é…ç½®
HISTORY_FILE = "data/history.json"
DOWNLOAD_DIR = "downloads"
MAX_ATTACHMENT_SIZE = 19 * 1024 * 1024  # 19MB
socket.setdefaulttimeout(60)

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
cr = Crossref()

# --- ğŸ§  2. LLM åˆ†ææ ¸å¿ƒæ¨¡å— ---

def get_oa_link_from_doi(doi):
    """åˆ©ç”¨ Unpaywall API æŸ¥æ‰¾ DOI æ˜¯å¦æœ‰å…è´¹ PDF"""
    try:
        email_addr = "bot@example.com"
        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email={email_addr}", timeout=10)
        data = r.json()
        if data.get('is_oa') and data.get('best_oa_location'):
            return data['best_oa_location']['url_for_pdf']
    except:
        pass
    return None

def detect_and_extract_all(text):
    """ä»æ–‡æœ¬ä¸­æå– ArXiv ID, DOI å’Œ PDF é“¾æ¥"""
    results = []
    seen_ids = set() 

    # 1. ArXiv
    for match in re.finditer(r"(?:arXiv ID:|arxiv\.org/abs/)\s*(\d+\.\d+)", text, re.IGNORECASE):
        aid = match.group(1)
        if aid not in seen_ids:
            results.append({"type": "arxiv", "id": aid, "url": f"https://arxiv.org/pdf/{aid}.pdf"})
            seen_ids.add(aid)

    # 2. DOI
    for match in re.finditer(r"doi:\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.IGNORECASE):
        doi = match.group(1)
        if doi not in seen_ids:
            oa_url = get_oa_link_from_doi(doi)
            results.append({"type": "doi", "id": doi, "url": oa_url})
            seen_ids.add(doi)

    # 3. Direct PDF Links
    for match in re.finditer(r'(https?://[^\s]+\.pdf)', text, re.IGNORECASE):
        url = match.group(1)
        if any(x in url for x in seen_ids): continue
        url_hash = hashlib.md5(url.encode()).hexdigest()
        if url_hash not in seen_ids:
            results.append({"type": "direct_pdf", "id": None, "url": url})
            seen_ids.add(url_hash)

    return results

def fetch_content(source_data, save_dir=None):
    """ä¸‹è½½ PDF æˆ–è·å– DOI æ‘˜è¦"""
    # A. PDF ä¸‹è½½
    if source_data.get("url") and source_data["url"].endswith(".pdf"):
        print(f"    ğŸ“¥ [ä¸‹è½½ä¸­] {source_data['url']}")
        time.sleep(2) 
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
            r = requests.get(source_data["url"], headers=headers, timeout=60)
            if r.status_code == 200:
                # ä¿®å¤åçš„ ID ç”Ÿæˆé€»è¾‘ï¼Œé¿å…ä¹‹å‰çš„ SyntaxError
                file_id = source_data.get('id')
                if not file_id:
                    file_id = hashlib.md5(source_data['url'].encode()).hexdigest()
                
                safe_name = re.sub(r'[\\/*?:"<>|]', '_', file_id)
                filename = os.path.join(save_dir, f"{safe_name}.pdf") if save_dir else f"temp_{safe_name}.pdf"

                with open(filename, "wb") as f:
                    f.write(r.content)
                
                # ä½¿ç”¨ pymupdf4llm æå–ä¸º Markdown
                content = pymupdf4llm.to_markdown(filename)
                return content, "PDF Full Text", filename
        except Exception as e:
            print(f"    âš ï¸ PDF ä¸‹è½½å¤±è´¥: {e}")

    # B. DOI æ‘˜è¦ (å¤‡é€‰)
    if source_data["type"] == "doi":
        print(f"    â„¹ï¸ [å…ƒæ•°æ®] å°è¯•æŠ“å– DOI æ‘˜è¦: {source_data['id']}")
        try:
            work = cr.works(ids=source_data["id"])
            title = work['message'].get('title', [''])[0]
            abstract = work['message'].get('abstract', 'æ— æ‘˜è¦ä¿¡æ¯')
            abstract = re.sub(r'<[^>]+>', '', abstract)
            content = f"# {title}\n\n## Abstract\n{abstract}"
            return content, "Abstract Only", None
        except:
            pass
            
    return None, "Unknown", None

def analyze_with_llm(content, content_type, source_url=""):
    """
    LLM åˆ†æå‡½æ•° - åŒ…å«è§†è§‰å¢å¼ºæŒ‡ä»¤
    """
    prompt = f"""
    è¯·ä½œä¸ºæˆ‘çš„å­¦æœ¯åŠ©æ‰‹ï¼ˆä¾§é‡ç¤¾ä¼šç§‘å­¦ä¸å®šé‡ç ”ç©¶ï¼‰ï¼ŒåŸºäºä»¥ä¸‹æä¾›çš„æ–‡çŒ®å†…å®¹æ‰§è¡Œæ·±åº¦åˆ†æã€‚
    ã€æ–‡çŒ®å†…å®¹æ¥æºã€‘ï¼š{content_type}
    ã€å·²çŸ¥é“¾æ¥ã€‘ï¼š{source_url}

    ### ğŸ¨ è§†è§‰å¢å¼ºæŒ‡ä»¤ (Visual Enhancement):
    ä¸ºäº†å¸®åŠ©è¯»è€…ç›´è§‚ç†è§£ï¼Œè¯·åœ¨æè¿°**å¤æ‚ç³»ç»Ÿæ¶æ„ã€ç®—æ³•æµç¨‹ã€å› æœæœºåˆ¶ã€å…³é”®æ•°æ®è¶‹åŠ¿**æˆ–**æŠ½è±¡æ¦‚å¿µ**æ—¶ï¼Œåœ¨æ®µè½åæ’å…¥ 1-2 ä¸ªå›¾ç‰‡æœç´¢æ ‡ç­¾ã€‚
    - **æ ¼å¼**ï¼š`

[Image of X]
`
    - **è¦æ±‚**ï¼šX å¿…é¡»æ˜¯å…·ä½“ã€å‡†ç¡®çš„æœç´¢å…³é”®è¯ï¼ˆè‹±æ–‡ä¸ºä½³ï¼‰ã€‚
    - **ç¤ºä¾‹**ï¼š
      - æåˆ° Transformer æ¶æ„æ—¶ï¼š`

[Image of Transformer architecture diagram]
`
      - æåˆ°åŒé‡å·®åˆ†æ³•è¶‹åŠ¿æ—¶ï¼š``
    - **åŸåˆ™**ï¼šåªåœ¨æœ‰æ•™è‚²/è§£é‡Šæ„ä¹‰æ—¶æ’å…¥ã€‚

    ### ğŸ“ ä»»åŠ¡æ­¥éª¤ï¼ˆè¯·è¾“å‡º Markdown æ ¼å¼ï¼‰ï¼š
    1. **åŸºæœ¬ä¿¡æ¯**ï¼šæ ‡é¢˜ã€ä½œè€…ã€å¹´ä»½ã€æœŸåˆŠã€‚
    2. **ç ”ç©¶èƒŒæ™¯ä¸ç¼ºå£**ï¼šä¸€å¥è¯æ¦‚æ‹¬ã€‚
    3. **æ ¸å¿ƒç†è®ºä¸å‡è®¾**ã€‚
    4. **æ•°æ®ä¸æ–¹æ³• (é‡è¦)**ï¼š
       - æ•°æ®æ¥æº (Dataset)
       - æ ¸å¿ƒå˜é‡ (IV/DV)
       - è¯†åˆ«ç­–ç•¥ (Identification Strategy, å¦‚ IV, DID, RDD ç­‰)
    5. **å…³é”®å®è¯ç»“æœ**ï¼š(è‹¥æ–‡ä¸­åŒ…å« Markdown è¡¨æ ¼ï¼Œè¯·é‡ç‚¹è§£è¯»æ˜¾è‘—æ€§ç³»æ•°)
    6. **ä¸»è¦ç»“è®ºä¸è´¡çŒ®**ã€‚
    7. **å±€é™æ€§ä¸æœªæ¥æ–¹å‘**ã€‚
    8. **

[Image of X]
 æ’å…¥ç‚¹**ï¼šè¯·åœ¨æ­£æ–‡ä¸­è‡ªç„¶ç©¿æ’ä¸Šè¿°æ ‡ç­¾ã€‚

    ---
    {content[:55000]} 
    ---
    """
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        analysis = completion.choices[0].message.content
        analysis = analysis.replace("```markdown", "").replace("```", "").strip()
        return analysis
    except Exception as e:
        return f"LLM åˆ†æå‡ºé”™: {e}"

def simple_translate(text):
    """ç®€å•æ ‡é¢˜ç¿»è¯‘"""
    if not text or len(text) < 5: return text
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": "Translate the title to Chinese."},
                {"role": "user", "content": text}
            ],
            temperature=0.3
        )
        return completion.choices[0].message.content.strip()
    except:
        return text

# --- ğŸ“§ 3. é‚®ä»¶ä¸é™„ä»¶å¤„ç†æ¨¡å— ---

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except: return []
    return []

def save_history(history_list):
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history_list, f, indent=2, ensure_ascii=False)

def get_unique_id(source_data):
    # è¿™é‡Œæ˜¯ä¿®å¤åçš„é€»è¾‘
    if source_data.get("id"):
        return source_data["id"]
    elif source_data.get("url"):
        return hashlib.md5(source_data["url"].encode()).hexdigest()
    return None

def connect_imap():
    mail = imaplib.IMAP4_SSL(IMAP_SERVER)
    mail.login(EMAIL_USER, EMAIL_PASS)
    return mail

def extract_body(msg):
    """é€’å½’æå–é‚®ä»¶æ­£æ–‡ï¼Œç©¿é€ .eml"""
    body_text = ""
    if msg.is_multipart():
        for part in msg.walk():
            ctype = part.get_content_type()
            cdispo = str(part.get("Content-Disposition"))
            if ctype == "text/plain" and "attachment" not in cdispo:
                try: body_text += part.get_payload(decode=True).decode(errors='ignore') + "\n"
                except: pass
            elif ctype == "message/rfc822" or (part.get_filename() and part.get_filename().endswith('.eml')):
                try:
                    payload = part.get_payload(0) if isinstance(part.get_payload(), list) else part.get_payload()
                    if isinstance(payload, email.message.Message):
                        body_text += extract_body(payload)
                except: pass
    else:
        try: body_text += msg.get_payload(decode=True).decode(errors='ignore')
        except: pass
    return body_text

def get_emails_from_today():
    try:
        mail = connect_imap()
        mail.select("inbox")
        # æœç´¢æœ€è¿‘ 24 å°æ—¶
        date_str = (datetime.date.today() - datetime.timedelta(days=1)).strftime("%d-%b-%Y")
        status, messages = mail.search(None, f'(SINCE "{date_str}")')
        
        email_ids = messages[0].split()
        target_emails = []
        print(f"ğŸ” æ‰«æåˆ° {len(email_ids)} å°è¿‘æœŸé‚®ä»¶...")
        
        for e_id in email_ids:
            try:
                _, msg_data = mail.fetch(e_id, "(RFC822)")
                for response_part in msg_data:
                    if isinstance(response_part, tuple):
                        msg = email.message_from_bytes(response_part[1])
                        subject, encoding = decode_header(msg["Subject"])[0]
                        if isinstance(subject, bytes):
                            subject = subject.decode(encoding if encoding else "utf-8")
                        
                        if any(k.lower() in subject.lower() for k in TARGET_SUBJECTS):
                            print(f"  âœ‰ï¸ [å‘½ä¸­] {subject[:30]}...")
                            target_emails.append(msg)
            except: continue
        return target_emails
    except Exception as e:
        print(f"âŒ é‚®ç®±è¿æ¥å¤±è´¥: {e}")
        return []

def batch_files_by_size(file_paths, max_size):
    batches = []
    current_batch = []
    current_batch_size = 0
    for f_path in file_paths:
        if not os.path.exists(f_path): continue
        f_size = os.path.getsize(f_path)
        if f_size > max_size:
            batches.append([f_path])
            continue
        if current_batch_size + f_size > max_size:
            batches.append(current_batch)
            current_batch = [f_path]
            current_batch_size = f_size
        else:
            current_batch.append(f_path)
            current_batch_size += f_size
    if current_batch: batches.append(current_batch)
    return batches

def create_zip_for_batch(batch_files, batch_index):
    zip_name = f"papers_part_{batch_index}.zip"
    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zf:
        for file in batch_files:
            zf.write(file, os.path.basename(file))
    return zip_name

def send_email_with_attachment(subject, body, attachment_zip=None):
    msg = MIMEMultipart()
    msg["Subject"] = subject
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER 
    msg.attach(MIMEText(body, "markdown", "utf-8"))

    if attachment_zip and os.path.exists(attachment_zip):
        try:
            with open(attachment_zip, "rb") as f:
                part = MIMEApplication(f.read(), Name=os.path.basename(attachment_zip))
            part['Content-Disposition'] = f'attachment; filename="{os.path.basename(attachment_zip)}"'
            msg.attach(part)
        except Exception as e:
            print(f"âŒ é™„ä»¶æŒ‚è½½å¤±è´¥: {e}")

    try:
        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as server:
            server.login(EMAIL_USER, EMAIL_PASS)
            server.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
        print(f"ğŸ“§ å·²å‘é€: {subject}")
        return True
    except Exception as e:
        print(f"âŒ å‘é€å¤±è´¥: {e}")
        return False

# --- ğŸš€ 4. ä¸»æ‰§è¡Œé€»è¾‘ ---

def main():
    if os.path.exists(DOWNLOAD_DIR):
        shutil.rmtree(DOWNLOAD_DIR)
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    
    processed_ids = load_history()
    emails = get_emails_from_today()
    
    if not emails:
        print("â˜• æš‚æ— æ–°çš„å­¦æœ¯æ¨é€ã€‚")
        return

    report_body = ""
    failed_papers = []
    total_new = 0
    all_files = []
    
    for msg in emails:
        body = extract_body(msg)
        sources = detect_and_extract_all(body)
        
        if not sources: continue
        print(f"  ğŸ” å‘ç° {len(sources)} ç¯‡æ–‡çŒ®...")
        
        for src in sources:
            uid = get_unique_id(src)
            if uid in processed_ids:
                print(f"    â­ï¸ [è·³è¿‡] {uid[:10]}")
                continue

            print(f"    ğŸš€ [åˆ†æ] {src.get('id', 'Document')}")
            content, ctype, saved_path = fetch_content(src, save_dir=DOWNLOAD_DIR)
            
            if saved_path: all_files.append(saved_path)
            
            if content:
                analysis = analyze_with_llm(content, ctype, src.get('url'))
                if "LLM åˆ†æå‡ºé”™" in analysis:
                    failed_papers.append({"id": src.get('id'), "url": src.get('url'), "reason": "AI Error"})
                else:
                    title = src.get('id', 'Paper')
                    report_body += f"## ğŸ“‘ {title}\n\n{analysis}\n\n---\n\n"
                    processed_ids.append(uid)
                    total_new += 1
                    print(f"    âœ… å®Œæˆ")
            else:
                failed_papers.append({"id": src.get('id'), "url": src.get('url'), "reason": "Download Failed"})

    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    final_report = f"# ğŸ“… æ–‡çŒ®æ—¥æŠ¥ {datetime.date.today()}\n\n"
    if failed_papers:
        final_report += f"## âš ï¸ {len(failed_papers)} ç¯‡å¤„ç†å¤±è´¥\n"
        for fp in failed_papers:
            zh_title = simple_translate(fp['id'])
            final_report += f"- **{zh_title}**\n  - åŸæ–‡: {fp['url']}\n  - åŸå› : {fp['reason']}\n\n"
        final_report += "---\n\n"
    
    if total_new > 0:
        final_report += report_body
    else:
        final_report += "ä»Šæ—¥æ— æˆåŠŸåˆ†æçš„æ–‡çŒ®ã€‚\n"

    # å‘é€
    if total_new > 0 or failed_papers:
        subject = f"ğŸ¤– AI å­¦æœ¯æ—¥æŠ¥ (æˆåŠŸ:{total_new} å¤±è´¥:{len(failed_papers)})"
        if not all_files:
            send_email_with_attachment(subject, final_report)
        else:
            batches = batch_files_by_size(all_files, MAX_ATTACHMENT_SIZE)
            total_batches = len(batches)
            for i, batch in enumerate(batches):
                zip_file = create_zip_for_batch(batch, i+1)
                sub_part = f"{subject} (é™„ä»¶ {i+1}/{total_batches})"
                body_part = final_report if i == 0 else "ğŸ“ è¡¥å……é™„ä»¶..."
                send_email_with_attachment(sub_part, body_part, zip_file)
                if os.path.exists(zip_file): os.remove(zip_file)
        
        save_history(processed_ids)
        print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    else:
        print("æ²¡æœ‰éœ€è¦å‘é€çš„å†…å®¹ã€‚")

if __name__ == "__main__":
    main()
