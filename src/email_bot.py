import os

import re

import requests

import pymupdf4llm

from openai import OpenAI

from habanero import Crossref

import time

import hashlib

import json

import shutil

import zipfile

import socket

import imaplib

import email

import smtplib

import datetime

from email.header import decode_header

from email.mime.text import MIMEText

from email.mime.multipart import MIMEMultipart

from email.mime.application import MIMEApplication

from urllib.parse import unquote

import markdown



# --- ğŸ› ï¸ 1. æ ¸å¿ƒé…ç½®åŒº ---

LLM_API_KEY = os.environ.get("LLM_API_KEY")

LLM_BASE_URL = "https://api.siliconflow.cn/v1"

LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")



EMAIL_USER = os.environ.get("EMAIL_USER")

EMAIL_PASS = os.environ.get("EMAIL_PASS")

IMAP_SERVER = "imap.gmail.com"

SMTP_SERVER = "smtp.gmail.com"



TARGET_SUBJECTS = [

    "æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", 

    "new research", "Stork", "ScienceDirect", "Chinese politics", 

    "Imperial history", "Causal inference", "new results"

]



HISTORY_FILE = "data/history.json"

DOWNLOAD_DIR = "downloads"

MAX_ATTACHMENT_SIZE = 19 * 1024 * 1024

socket.setdefaulttimeout(30)



client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

cr = Crossref()



# --- ğŸ¨ é‚®ä»¶æ ·å¼ç¾åŒ– (CSS) ---

EMAIL_CSS = """

<style>

    body { font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; max-width: 800px; margin: 0 auto; padding: 20px; }

    h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; font-size: 24px; }

    h2 { color: #e67e22; margin-top: 30px; font-size: 20px; border-left: 5px solid #e67e22; padding-left: 10px; background-color: #fdf2e9; }

    h3 { color: #34495e; font-size: 18px; margin-top: 25px; }

    p { margin-bottom: 15px; text-align: justify; }

    strong { color: #c0392b; font-weight: 700; }

    blockquote { border-left: 4px solid #bdc3c7; margin: 0; padding-left: 15px; color: #7f8c8d; background-color: #f9f9f9; padding: 10px; }

    li { margin-bottom: 8px; }

    hr { border: 0; height: 1px; background: #eee; margin: 30px 0; }

    code { background-color: #f4f4f4; padding: 2px 5px; border-radius: 3px; font-family: Monaco, monospace; font-size: 0.9em; color: #e74c3c; }

    .image-placeholder { background-color: #e8f6f3; border: 1px dashed #1abc9c; color: #16a085; padding: 15px; text-align: center; border-radius: 5px; margin: 20px 0; font-style: italic; }

</style>

"""



# --- ğŸ§  2. æ ¸å¿ƒæ¨¡å— ---



def get_oa_link_from_doi(doi):

    try:

        email_addr = "bot@example.com"

        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email={email_addr}", timeout=15)

        data = r.json()

        if data.get('is_oa') and data.get('best_oa_location'):

            return data['best_oa_location']['url_for_pdf']

    except: 

        pass

    return None



def extract_body(msg):

    """ğŸŸ¢ æå–é‚®ä»¶æ­£æ–‡ï¼ˆçº¯æ–‡æœ¬ + HTMLä¸­çš„é“¾æ¥ï¼‰"""

    body_text = ""

    html_links = []

    

    if msg.is_multipart():

        for part in msg.walk():

            content_type = part.get_content_type()

            disposition = str(part.get("Content-Disposition"))

            

            # æå–çº¯æ–‡æœ¬

            if content_type == "text/plain" and "attachment" not in disposition:

                try:

                    body_text += part.get_payload(decode=True).decode(errors='ignore') + "\n"

                except:

                    pass

            

            # ğŸŸ¢ æå– HTML ä¸­çš„æ‰€æœ‰é“¾æ¥

            elif content_type == "text/html" and "attachment" not in disposition:

                try:

                    html_content = part.get_payload(decode=True).decode(errors='ignore')

                    # æå–æ‰€æœ‰ href é“¾æ¥

                    found_links = re.findall(r'href=["\']([^"\']+)["\']', html_content, re.IGNORECASE)

                    html_links.extend(found_links)

                except:

                    pass

    else:

        try:

            body_text += msg.get_payload(decode=True).decode(errors='ignore')

        except:

            pass

    

    return body_text, html_links



def detect_and_extract_all(text, html_links=None):

    """ğŸŸ¢ æ£€æµ‹æ–‡çŒ®æ ‡è¯†ç¬¦ + HTMLé“¾æ¥ä¸­çš„PDF"""

    results = []

    seen_ids = set()

    

    # 1. æ£€æµ‹ ArXiv ID

    for match in re.finditer(r"(?:arXiv:|arxiv\.org/abs/|arxiv\.org/pdf/)\s*(\d{4}\.\d{4,5})", text, re.IGNORECASE):

        aid = match.group(1)

        if aid not in seen_ids:

            results.append({

                "type": "arxiv", 

                "id": aid, 

                "url": f"https://arxiv.org/pdf/{aid}.pdf"

            })

            seen_ids.add(aid)

    

    # 2. æ£€æµ‹ DOI

    for match in re.finditer(r"(?:doi:|doi\.org/)\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.IGNORECASE):

        doi = match.group(1)

        if doi not in seen_ids:

            oa_url = get_oa_link_from_doi(doi)

            results.append({

                "type": "doi", 

                "id": doi, 

                "url": oa_url

            })

            seen_ids.add(doi)

    

    # ğŸŸ¢ 3. å¤„ç† HTML é“¾æ¥ä¸­çš„ PDF

    if html_links:

        for link in html_links:

            try:

                # Google Scholar ç‰¹æ®Šæ ¼å¼: scholar_url?url=å®é™…é“¾æ¥

                if "scholar_url?url=" in link:

                    actual_url = re.search(r'url=([^&]+)', link)

                    if actual_url:

                        pdf_url = unquote(actual_url.group(1))

                        if pdf_url.endswith('.pdf') or '/pdf/' in pdf_url.lower():

                            link_hash = hashlib.md5(pdf_url.encode()).hexdigest()[:10]

                            if link_hash not in seen_ids:

                                results.append({

                                    "type": "scholar_pdf",

                                    "id": f"gs_{link_hash}",

                                    "url": pdf_url

                                })

                                seen_ids.add(link_hash)

                

                # ç›´æ¥ PDF é“¾æ¥

                elif link.endswith('.pdf') or '/pdf/' in link.lower():

                    # è¿‡æ»¤æ‰ä¸€äº›æ— æ•ˆé“¾æ¥

                    if not any(skip in link.lower() for skip in ['unsubscribe', 'privacy', 'terms']):

                        link_hash = hashlib.md5(link.encode()).hexdigest()[:10]

                        if link_hash not in seen_ids:

                            results.append({

                                "type": "direct_pdf",

                                "id": f"pdf_{link_hash}",

                                "url": link

                            })

                            seen_ids.add(link_hash)

            except:

                continue

    

    return results



def fetch_content(source_data, save_dir=None):

    if source_data.get("type") == "arxiv":

        print(f"    â³ [ArXiv] è¯·æ±‚é¢‘ç‡ä¿æŠ¤ï¼Œç­‰å¾… 5s...")

        time.sleep(5)



    if source_data.get("url") and source_data["url"]:

        # ğŸŸ¢ æ”¯æŒæ›´å¤š PDF é“¾æ¥æ ¼å¼

        is_pdf = (

            source_data["url"].endswith(".pdf") or 

            '/pdf/' in source_data["url"].lower() or

            source_data.get("type") in ["arxiv", "scholar_pdf", "direct_pdf"]

        )

        

        if is_pdf:

            print(f"    ğŸ“¥ [ä¸‹è½½] æ­£åœ¨æŠ“å– PDF: {source_data['url'][:60]}...")

            try:

                headers = {

                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",

                    "Accept": "application/pdf,*/*"

                }

                r = requests.get(source_data["url"], headers=headers, timeout=45, allow_redirects=True)

                

                if r.status_code == 200 and len(r.content) > 1000:  # ç¡®ä¿ä¸æ˜¯ç©ºæ–‡ä»¶

                    file_id = source_data.get('id') or hashlib.md5(source_data['url'].encode()).hexdigest()[:10]

                    safe_name = re.sub(r'[\\/*?:"<>|]', '_', file_id)

                    filename = os.path.join(save_dir, f"{safe_name}.pdf") if save_dir else f"temp_{safe_name}.pdf"

                    

                    with open(filename, "wb") as f: 

                        f.write(r.content)

                    

                    content = pymupdf4llm.to_markdown(filename)

                    print(f"    âœ… [æˆåŠŸ] å·²æå– {len(content)} å­—ç¬¦")

                    return content, "PDF Full Text", filename

                else:

                    print(f"    âš ï¸ PDFä¸‹è½½å¤±è´¥: HTTP {r.status_code}, å¤§å° {len(r.content)} bytes")

            except Exception as e:

                print(f"    âš ï¸ ä¸‹è½½ä¸­æ–­: {e}")



    # å¯¹äº DOIï¼Œå°è¯•è·å–æ‘˜è¦

    if source_data.get("type") == "doi":

        try:

            print(f"    ğŸ“š [Crossref] æ­£åœ¨è·å–æ‘˜è¦...")

            work = cr.works(ids=source_data["id"])

            title = work['message'].get('title', [''])[0]

            abstract = re.sub(r'<[^>]+>', '', work['message'].get('abstract', 'æ— æ‘˜è¦'))

            content = f"# {title}\n\n## Abstract\n{abstract}"

            return content, "Abstract Only", None

        except Exception as e:

            print(f"    âš ï¸ Crossref æŸ¥è¯¢å¤±è´¥: {e}")

    

    return None, "Unknown", None



def analyze_with_llm(content, content_type, source_url=""):

    prompt = f"""è¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡çŒ®ã€‚æ¥æºï¼š{content_type}ã€‚åœ¨è§£é‡Šæœºåˆ¶æ—¶æ’å…¥ [Image of X] æ ‡ç­¾ã€‚è¾“å‡º Markdownã€‚\n---\n{content[:50000]}"""

    try:

        completion = client.chat.completions.create(

            model=LLM_MODEL_NAME,

            messages=[{"role": "user", "content": prompt}],

            temperature=0.3

        )

        return completion.choices[0].message.content.strip()

    except Exception as e:

        return f"LLM åˆ†æå‡ºé”™: {e}"



# --- ğŸ“§ 3. è¾…åŠ©åŠŸèƒ½ ---



def load_history():

    if os.path.exists(HISTORY_FILE):

        try:

            with open(HISTORY_FILE, "r", encoding="utf-8") as f: 

                return json.load(f)

        except: 

            return []

    return []



def save_history(history_list):

    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)

    with open(HISTORY_FILE, "w", encoding="utf-8") as f: 

        json.dump(history_list, f, indent=2, ensure_ascii=False)



def get_unique_id(source_data):

    return source_data.get("id") or hashlib.md5(source_data.get("url", "").encode()).hexdigest()



def send_email_with_attachment(subject, body_markdown, attachment_zip=None):

    try:

        html_content = markdown.markdown(body_markdown, extensions=['extra', 'tables', 'fenced_code'])

    except Exception as e:

        print(f"Markdown è½¬æ¢å¤±è´¥: {e}")

        html_content = body_markdown

    

    pattern = r"\[Image of ([^\]]+)\]"

    replacement = r'<div class="image-placeholder">ğŸ–¼ï¸ å›¾ç¤ºå»ºè®®ï¼š\1</div>'

    html_content = re.sub(pattern, replacement, html_content)

    

    final_html = f"""

<!DOCTYPE html>

<html>

<head>

    <meta charset="UTF-8">

    {EMAIL_CSS}

</head>

<body>

    {html_content}

    <footer>

        ğŸ¤– Generated by AI Research Assistant | ğŸ“… {datetime.date.today()}

    </footer>

</body>

</html>

"""

    msg = MIMEMultipart()

    msg["Subject"] = subject

    msg["From"] = EMAIL_USER

    msg["To"] = EMAIL_USER

    msg.attach(MIMEText(final_html, "html", "utf-8"))

    

    if attachment_zip and os.path.exists(attachment_zip):

        try:

            with open(attachment_zip, "rb") as f:

                part = MIMEApplication(f.read(), Name=os.path.basename(attachment_zip))

                part['Content-Disposition'] = f'attachment; filename="{os.path.basename(attachment_zip)}"'

                msg.attach(part)

        except Exception as e:

            print(f"é™„ä»¶æŒ‚è½½å¤±è´¥: {e}")

    

    try:

        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as server:

            server.login(EMAIL_USER, EMAIL_PASS)

            server.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())

        return True

    except Exception as e:

        print(f"å‘é€å¤±è´¥: {e}")

        return False



# --- ğŸš€ 4. ä¸»é€»è¾‘ ---



def main():

    print("ğŸ¬ ç¨‹åºå¯åŠ¨ä¸­...")

    if os.path.exists(DOWNLOAD_DIR):

        shutil.rmtree(DOWNLOAD_DIR)

    os.makedirs(DOWNLOAD_DIR, exist_ok=True)

    

    processed_ids = load_history()

    print(f"ğŸ“§ æ­£åœ¨å°è¯•è¿æ¥ IMAP æœåŠ¡å™¨: {IMAP_SERVER}...")

    

    max_retries = 3

    for attempt in range(max_retries):

        try:

            mail = imaplib.IMAP4_SSL(IMAP_SERVER)

            print(f"ğŸ”‘ æ­£åœ¨ç™»å½•è´¦æˆ·: {EMAIL_USER}...")

            mail.login(EMAIL_USER, EMAIL_PASS)

            break

        except Exception as e:

            if attempt < max_retries - 1:

                wait_time = (attempt + 1) * 5

                print(f"âš ï¸  è¿æ¥å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")

                time.sleep(wait_time)

            else:

                raise e

    

    print("ğŸ“‚ å·²æˆåŠŸç™»å½•ï¼Œæ­£åœ¨æ‰“å¼€æ”¶ä»¶ç®±...")

    mail.select("inbox")

    

    date_str = (datetime.date.today() - datetime.timedelta(days=1)).strftime("%d-%b-%Y")

    print(f"ğŸ” æ­£åœ¨æ£€ç´¢ {date_str} ä¹‹åçš„é‚®ä»¶...")

    _, data = mail.search(None, f'(SINCE "{date_str}")')

    

    pending_sources = []

    email_list = data[0].split()

    print(f"ğŸ“¨ æ£€ç´¢åˆ°å…± {len(email_list)} å°è¿‘æœŸé‚®ä»¶ï¼Œå¼€å§‹è§£æå…³é”®è¯...")

    

    processed_count = 0

    failed_count = 0

    MAX_FAILURES = 5

    DELAY_BETWEEN_EMAILS = 1.5

    DELAY_AFTER_BATCH = 5

    BATCH_SIZE = 10

    OVERQUOTA_COOLDOWN = 30

    

    for idx, e_id in enumerate(email_list, 1):

        try:

            if processed_count > 0:

                print(f"â¸ï¸  ç­‰å¾… {DELAY_BETWEEN_EMAILS} ç§’... ({processed_count}/{len(email_list)})")

                time.sleep(DELAY_BETWEEN_EMAILS)

            

            if processed_count > 0 and processed_count % BATCH_SIZE == 0:

                print(f"ğŸ›‘ å·²å¤„ç† {processed_count} å°ï¼Œä¼‘æ¯ {DELAY_AFTER_BATCH} ç§’é¿å…è§¦å‘é™åˆ¶...")

                time.sleep(DELAY_AFTER_BATCH)

            

            _, header_data = mail.fetch(e_id, "(BODY.PEEK[HEADER])")

            msg_header = email.message_from_bytes(header_data[0][1])

            

            subj, enc = decode_header(msg_header["Subject"])[0]

            subj = subj.decode(enc or 'utf-8') if isinstance(subj, bytes) else subj

            

            if not any(k.lower() in subj.lower() for k in TARGET_SUBJECTS):

                processed_count += 1

                continue

            

            print(f"ğŸ¯ å‘½ä¸­å…³é”®è¯é‚®ä»¶: {subj[:30]}...")

            

            time.sleep(1)

            _, m_data = mail.fetch(e_id, "(RFC822)")

            msg = email.message_from_bytes(m_data[0][1])

            

            # ğŸŸ¢ æå–æ–‡æœ¬å’ŒHTMLé“¾æ¥

            body_text, html_links = extract_body(msg)

            print(f"    ğŸ“ æ‰¾åˆ° {len(html_links)} ä¸ªé“¾æ¥")

            

            # ğŸŸ¢ ä¼ å…¥é“¾æ¥åˆ—è¡¨

            sources = detect_and_extract_all(body_text, html_links)

            print(f"    âœ… è¯†åˆ«å‡º {len(sources)} ä¸ªæ–‡çŒ®æº")

            

            for s in sources:

                if get_unique_id(s) not in processed_ids:

                    pending_sources.append(s)

            

            processed_count += 1

            failed_count = 0

            

        except Exception as e:

            error_msg = str(e)

            print(f"âš ï¸  è§£æé‚®ä»¶ {e_id} æ—¶å‡ºé”™: {error_msg}")

            

            if "OVERQUOTA" in error_msg or "exceeded" in error_msg.lower():

                failed_count += 1

                print(f"âŒ è§¦å‘ Gmail é…é¢é™åˆ¶ï¼({failed_count}/{MAX_FAILURES})")

                

                if failed_count >= MAX_FAILURES:

                    print(f"ğŸ›‘ è¿ç»­å¤±è´¥ {MAX_FAILURES} æ¬¡ï¼Œåœæ­¢æœ¬æ¬¡è¿è¡Œ")

                    print(f"âœ… å·²æˆåŠŸå¤„ç† {processed_count} å°é‚®ä»¶")

                    break

                

                print(f"â° ç­‰å¾… {OVERQUOTA_COOLDOWN} ç§’åç»§ç»­...")

                time.sleep(OVERQUOTA_COOLDOWN)

                

                try:

                    mail.close()

                    mail.logout()

                    time.sleep(5)

                    mail = imaplib.IMAP4_SSL(IMAP_SERVER)

                    mail.login(EMAIL_USER, EMAIL_PASS)

                    mail.select("inbox")

                    print("âœ… é‡æ–°è¿æ¥æˆåŠŸ")

                except:

                    print("âŒ é‡æ–°è¿æ¥å¤±è´¥ï¼Œåœæ­¢è¿è¡Œ")

                    break

            else:

                failed_count += 1

                if failed_count >= MAX_FAILURES:

                    print(f"ğŸ›‘ å…¶ä»–é”™è¯¯å¯¼è‡´è¿ç»­å¤±è´¥ {MAX_FAILURES} æ¬¡ï¼Œåœæ­¢è¿è¡Œ")

                    break

            

            continue

    

    try:

        mail.close()

        mail.logout()

    except:

        pass

    

    MAX_PAPERS = 15

    to_process = pending_sources[:MAX_PAPERS]

    if not to_process:

        print("â˜• æš‚æ— å¾…å¤„ç†çš„æ–°æ–‡çŒ®ï¼Œä»»åŠ¡ç»“æŸã€‚")

        return

    

    print(f"ğŸ“‘ é˜Ÿåˆ—å·²å°±ç»ª: ä»Šæ—¥å°†åˆ†æ {len(to_process)} ç¯‡æ–°æ–‡çŒ®ã€‚")

    report_body, all_files, total_new, failed = "", [], 0, []

    

    for src in to_process:

        print(f"ğŸ“ æ­£åœ¨å¤„ç†ç¬¬ {total_new + len(failed) + 1} ç¯‡: {src.get('id', 'Document')}")

        content, ctype, path = fetch_content(src, save_dir=DOWNLOAD_DIR)

        if path:

            all_files.append(path)

        if content:

            print("ğŸ¤– æ­£åœ¨è°ƒç”¨ LLM è¿›è¡Œå­¦æœ¯åˆ†æ...")

            ans = analyze_with_llm(content, ctype, src.get('url'))

            if "LLM åˆ†æå‡ºé”™" not in ans:

                report_body += f"## ğŸ“‘ {src.get('id', 'Paper')}\n\n{ans}\n\n---\n\n"

                processed_ids.append(get_unique_id(src))

                total_new += 1

                continue

        failed.append(src)

    

    print(f"ğŸ“Š åˆ†æé˜¶æ®µç»“æŸã€‚æˆåŠŸ: {total_new}, å¤±è´¥: {len(failed)}")

    

    final_report = f"# ğŸ“… æ–‡çŒ®æ—¥æŠ¥ {datetime.date.today()}\n\n" + report_body

    if total_new > 0 or failed:

        print("ğŸ“¨ æ­£åœ¨æ‰“åŒ…å¹¶å‘é€é‚®ä»¶...")

        zip_file = "papers.zip" if all_files else None

        if zip_file:

            with zipfile.ZipFile(zip_file, 'w') as zf:

                for f in all_files:

                    zf.write(f, os.path.basename(f))

        

        if send_email_with_attachment(f"ğŸ¤– AI å­¦æœ¯æ—¥æŠ¥ (æ–°:{total_new})", final_report, zip_file):

            print("ğŸ“§ é‚®ä»¶å‘é€æˆåŠŸï¼")

        else:

            print("âŒ é‚®ä»¶å‘é€å¤±è´¥ã€‚")

        

        if zip_file and os.path.exists(zip_file):

            os.remove(zip_file)

    

    save_history(processed_ids)

    print("ğŸ’¾ å†å²è®°å½•å·²ä¿å­˜ã€‚")



if __name__ == "__main__":

    main()
