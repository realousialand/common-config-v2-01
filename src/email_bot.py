import os
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
import time
import hashlib
import json
import shutil
import zipfile
import socket
import imaplib
import email
import smtplib
import datetime
from email.header import decode_header
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
import markdown  # å¿…é¡»ç¡®ä¿ requirements.txt é‡Œæœ‰è¿™ä¸ªåº“

# --- ğŸ› ï¸ 1. æ ¸å¿ƒé…ç½®åŒº ---
LLM_API_KEY = os.environ.get("LLM_API_KEY")
LLM_BASE_URL = "https://api.siliconflow.cn/v1"
LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")

EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"

TARGET_SUBJECTS = [
    "æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", 
    "new research", "Stork", "ScienceDirect", "Chinese politics", 
    "Imperial history", "Causal inference", "new results"
]

HISTORY_FILE = "data/history.json"
DOWNLOAD_DIR = "downloads"
MAX_ATTACHMENT_SIZE = 19 * 1024 * 1024
# ğŸŸ¢ è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼Œé˜²æ­¢æ— é™å¡æ­»
socket.setdefaulttimeout(30) 

client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
cr = Crossref()

# --- ğŸ¨ é‚®ä»¶æ ·å¼ç¾åŒ– (CSS) ---
EMAIL_CSS = """
<style>
    body { font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; max-width: 800px; margin: 0 auto; padding: 20px; }
    h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; font-size: 24px; }
    h2 { color: #e67e22; margin-top: 30px; font-size: 20px; border-left: 5px solid #e67e22; padding-left: 10px; background-color: #fdf2e9; }
    h3 { color: #34495e; font-size: 18px; margin-top: 25px; }
    p { margin-bottom: 15px; text-align: justify; }
    strong { color: #c0392b; font-weight: 700; } /* é‡ç‚¹æ ‡çº¢ */
    blockquote { border-left: 4px solid #bdc3c7; margin: 0; padding-left: 15px; color: #7f8c8d; background-color: #f9f9f9; padding: 10px; }
    li { margin-bottom: 8px; }
    hr { border: 0; height: 1px; background: #eee; margin: 30px 0; }
    code { background-color: #f4f4f4; padding: 2px 5px; border-radius: 3px; font-family: Monaco, monospace; font-size: 0.9em; color: #e74c3c; }
    .image-placeholder { background-color: #e8f6f3; border: 1px dashed #1abc9c; color: #16a085; padding: 15px; text-align: center; border-radius: 5px; margin: 20px 0; font-style: italic; }
</style>
"""

# --- ğŸ§  2. æ ¸å¿ƒæ¨¡å— ---

def get_oa_link_from_doi(doi):
    try:
        email_addr = "bot@example.com"
        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email={email_addr}", timeout=15)
        data = r.json()
        if data.get('is_oa') and data.get('best_oa_location'):
            return data['best_oa_location']['url_for_pdf']
    except: pass
    return None

def detect_and_extract_all(text):
    results = []
    seen_ids = set() 
    for match in re.finditer(r"(?:arXiv:|arxiv\.org/abs/|arxiv\.org/pdf/)\s*(\d{4}\.\d{4,5})", text, re.IGNORECASE):
        aid = match.group(1)
        if aid not in seen_ids:
            results.append({"type": "arxiv", "id": aid, "url": f"https://arxiv.org/pdf/{aid}.pdf"})
            seen_ids.add(aid)
    for match in re.finditer(r"doi:\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.IGNORECASE):
        doi = match.group(1)
        if doi not in seen_ids:
            oa_url = get_oa_link_from_doi(doi)
            results.append({"type": "doi", "id": doi, "url": oa_url})
            seen_ids.add(doi)
    return results

def fetch_content(source_data, save_dir=None):
    if source_data.get("type") == "arxiv":
        print(f"    â³ [ArXiv] è¯·æ±‚é¢‘ç‡ä¿æŠ¤ï¼Œç­‰å¾… 5s...")
        time.sleep(5)

    if source_data.get("url") and source_data["url"].endswith(".pdf"):
        print(f"    ğŸ“¥ [ä¸‹è½½] æ­£åœ¨æŠ“å– PDF: {source_data['url']}")
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
            r = requests.get(source_data["url"], headers=headers, timeout=45)
            if r.status_code == 200:
                file_id = source_data.get('id') or hashlib.md5(source_data['url'].encode()).hexdigest()
                safe_name = re.sub(r'[\\/*?:"<>|]', '_', file_id)
                filename = os.path.join(save_dir, f"{safe_name}.pdf") if save_dir else f"temp_{safe_name}.pdf"
                with open(filename, "wb") as f: f.write(r.content)
                content = pymupdf4llm.to_markdown(filename)
                return content, "PDF Full Text", filename
        except Exception as e:
            print(f"    âš ï¸ ä¸‹è½½ä¸­æ–­: {e}")

    if source_data["type"] == "doi":
        try:
            work = cr.works(ids=source_data["id"])
            title = work['message'].get('title', [''])[0]
            abstract = re.sub(r'<[^>]+>', '', work['message'].get('abstract', 'æ— æ‘˜è¦'))
            content = f"# {title}\n\n## Abstract\n{abstract}"
            return content, "Abstract Only", None
        except: pass
    return None, "Unknown", None

def analyze_with_llm(content, content_type, source_url=""):
    # ğŸŸ¢ ä¿®å¤ï¼šPrompt é‡Œçš„åŒå¼•å·å’Œå•å¼•å·éƒ½å¤„ç†å¥½äº†
    prompt = f"""è¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡çŒ®ã€‚æ¥æºï¼š{content_type}ã€‚åœ¨è§£é‡Šæœºåˆ¶æ—¶æ’å…¥ 

[Image of X]
 æ ‡ç­¾ã€‚è¾“å‡º Markdownã€‚\n---\n{content[:50000]}"""
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"LLM åˆ†æå‡ºé”™: {e}"

# --- ğŸ“§ 3. è¾…åŠ©åŠŸèƒ½ ---

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f: return json.load(f)
        except: return []
    return []

def save_history(history_list):
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f: json.dump(history_list, f, indent=2, ensure_ascii=False)

def get_unique_id(source_data):
    return source_data.get("id") or hashlib.md5(source_data.get("url", "").encode()).hexdigest()

def extract_body(msg):
    body_text = ""
    if msg.is_multipart():
        for part in msg.walk():
            if part.get_content_type() == "text/plain" and "attachment" not in str(part.get("Content-Disposition")):
                try: body_text += part.get_payload(decode=True).decode(errors='ignore') + "\n"
                except: pass
    else:
        try: body_text += msg.get_payload(decode=True).decode(errors='ignore')
        except: pass
    return body_text
def send_email_with_attachment(subject, body_markdown, attachment_zip=None):
    # 1. å°† Markdown è½¬æ¢ä¸º HTML
    try:
        html_content = markdown.markdown(body_markdown, extensions=['extra', 'tables', 'fenced_code'])
    except Exception as e:
        print(f"Markdown è½¬æ¢å¤±è´¥: {e}")
        html_content = body_markdown
    # 2. ä¿®å¤ï¼šæ­£ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼å†™æ³•
    pattern = r"\[Image of ([^\]]+)\]"
    replacement = r'<div class="image-placeholder">ğŸ–¼ï¸ å›¾ç¤ºå»ºè®®ï¼š\1</div>'
    html_content = re.sub(pattern, replacement, html_content)
    # 3. ç»„åˆæœ€ç»ˆçš„ HTML é‚®ä»¶æ­£æ–‡
    final_html = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    {EMAIL_CSS}
</head>
<body>
    {html_content}
    <footer>
        ğŸ¤– Generated by AI Research Assistant | ğŸ“… {datetime.date.today()}
    </footer>
</body>
</html>
"""
    msg = MIMEMultipart()
    msg["Subject"] = subject
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER
    # 4. æŒ‡å®šä¸º html æ ¼å¼
    msg.attach(MIMEText(final_html, "html", "utf-8"))
    # é™„ä»¶å¤„ç†
    if attachment_zip and os.path.exists(attachment_zip):
        try:
            with open(attachment_zip, "rb") as f:
                part = MIMEApplication(f.read(), Name=os.path.basename(attachment_zip))
                part['Content-Disposition'] = f'attachment; filename="{os.path.basename(attachment_zip)}"'
                msg.attach(part)
        except Exception as e:
            print(f"é™„ä»¶æŒ‚è½½å¤±è´¥: {e}")
    # å‘é€é‚®ä»¶
    try:
        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as server:
            server.login(EMAIL_USER, EMAIL_PASS)
            server.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
        return True
    except Exception as e:
        print(f"å‘é€å¤±è´¥: {e}")
        return False

# --- ğŸš€ 4. ä¸»é€»è¾‘ ---

def main():
    print("ğŸ¬ ç¨‹åºå¯åŠ¨ä¸­...")
    if os.path.exists(DOWNLOAD_DIR):
        shutil.rmtree(DOWNLOAD_DIR)
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    
    processed_ids = load_history()
    print(f"ğŸ“§ æ­£åœ¨å°è¯•è¿æ¥ IMAP æœåŠ¡å™¨: {IMAP_SERVER}...")
    
    # ğŸŸ¢ æ·»åŠ é‡è¯•æœºåˆ¶
    max_retries = 3
    for attempt in range(max_retries):
        try:
            mail = imaplib.IMAP4_SSL(IMAP_SERVER)
            print(f"ğŸ”‘ æ­£åœ¨ç™»å½•è´¦æˆ·: {EMAIL_USER}...")
            mail.login(EMAIL_USER, EMAIL_PASS)
            break
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5
                print(f"âš ï¸  è¿æ¥å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                raise e
    
    print("ğŸ“‚ å·²æˆåŠŸç™»å½•ï¼Œæ­£åœ¨æ‰“å¼€æ”¶ä»¶ç®±...")
    mail.select("inbox")
    
    date_str = (datetime.date.today() - datetime.timedelta(days=1)).strftime("%d-%b-%Y")
    print(f"ğŸ” æ­£åœ¨æ£€ç´¢ {date_str} ä¹‹åçš„é‚®ä»¶...")
    _, data = mail.search(None, f'(SINCE "{date_str}")')
    
    pending_sources = []
    email_list = data[0].split()
    print(f"ğŸ“¨ æ£€ç´¢åˆ°å…± {len(email_list)} å°è¿‘æœŸé‚®ä»¶ï¼Œå¼€å§‹è§£æå…³é”®è¯...")
    
    # ğŸŸ¢ å…³é”®ï¼šé€Ÿç‡æ§åˆ¶å‚æ•°
    processed_count = 0
    failed_count = 0
    MAX_FAILURES = 5  # è¿ç»­å¤±è´¥5æ¬¡å°±åœæ­¢
    DELAY_BETWEEN_EMAILS = 1.5  # æ¯å°é‚®ä»¶ä¹‹é—´ç­‰å¾…1.5ç§’
    DELAY_AFTER_BATCH = 5  # æ¯10å°é‚®ä»¶åç­‰å¾…5ç§’
    BATCH_SIZE = 10
    OVERQUOTA_COOLDOWN = 30  # è§¦å‘é™åˆ¶åç­‰å¾…30ç§’
    
    for idx, e_id in enumerate(email_list, 1):
        try:
            # ğŸŸ¢ æ¯å°é‚®ä»¶ä¹‹é—´éƒ½è¦å»¶è¿Ÿ
            if processed_count > 0:
                print(f"â¸ï¸  ç­‰å¾… {DELAY_BETWEEN_EMAILS} ç§’... ({processed_count}/{len(email_list)})")
                time.sleep(DELAY_BETWEEN_EMAILS)
            
            # ğŸŸ¢ æ¯å¤„ç†ä¸€æ‰¹å°±é•¿æ—¶é—´ä¼‘æ¯
            if processed_count > 0 and processed_count % BATCH_SIZE == 0:
                print(f"ğŸ›‘ å·²å¤„ç† {processed_count} å°ï¼Œä¼‘æ¯ {DELAY_AFTER_BATCH} ç§’é¿å…è§¦å‘é™åˆ¶...")
                time.sleep(DELAY_AFTER_BATCH)
            
            # ğŸŸ¢ å…ˆè·å–é‚®ä»¶å¤´éƒ¨ï¼ˆèŠ‚çœé…é¢ï¼‰
            _, header_data = mail.fetch(e_id, "(BODY.PEEK[HEADER])")
            msg_header = email.message_from_bytes(header_data[0][1])
            
            subj, enc = decode_header(msg_header["Subject"])[0]
            subj = subj.decode(enc or 'utf-8') if isinstance(subj, bytes) else subj
            
            # ğŸŸ¢ ä¸åŒ¹é…çš„é‚®ä»¶ç›´æ¥è·³è¿‡ï¼Œä¸è·å–å®Œæ•´å†…å®¹
            if not any(k.lower() in subj.lower() for k in TARGET_SUBJECTS):
                processed_count += 1
                continue
            
            print(f"ğŸ¯ å‘½ä¸­å…³é”®è¯é‚®ä»¶: {subj[:30]}...")
            
            # ğŸŸ¢ åªæœ‰åŒ¹é…çš„é‚®ä»¶æ‰è·å–å®Œæ•´å†…å®¹
            time.sleep(1)  # é¢å¤–å»¶è¿Ÿ
            _, m_data = mail.fetch(e_id, "(RFC822)")
            msg = email.message_from_bytes(m_data[0][1])
            
            sources = detect_and_extract_all(extract_body(msg))
            for s in sources:
                if get_unique_id(s) not in processed_ids:
                    pending_sources.append(s)
            
            processed_count += 1
            failed_count = 0  # é‡ç½®å¤±è´¥è®¡æ•°
            
        except Exception as e:
            error_msg = str(e)
            print(f"âš ï¸  è§£æé‚®ä»¶ {e_id} æ—¶å‡ºé”™: {error_msg}")
            
            # ğŸŸ¢ ä¸“é—¨å¤„ç† OVERQUOTA é”™è¯¯
            if "OVERQUOTA" in error_msg or "exceeded" in error_msg.lower():
                failed_count += 1
                print(f"âŒ è§¦å‘ Gmail é…é¢é™åˆ¶ï¼({failed_count}/{MAX_FAILURES})")
                
                if failed_count >= MAX_FAILURES:
                    print(f"ğŸ›‘ è¿ç»­å¤±è´¥ {MAX_FAILURES} æ¬¡ï¼Œåœæ­¢æœ¬æ¬¡è¿è¡Œ")
                    print(f"âœ… å·²æˆåŠŸå¤„ç† {processed_count} å°é‚®ä»¶")
                    break
                
                print(f"â° ç­‰å¾… {OVERQUOTA_COOLDOWN} ç§’åç»§ç»­...")
                time.sleep(OVERQUOTA_COOLDOWN)
                
                # ğŸŸ¢ å°è¯•é‡æ–°è¿æ¥
                try:
                    mail.close()
                    mail.logout()
                    time.sleep(5)
                    mail = imaplib.IMAP4_SSL(IMAP_SERVER)
                    mail.login(EMAIL_USER, EMAIL_PASS)
                    mail.select("inbox")
                    print("âœ… é‡æ–°è¿æ¥æˆåŠŸ")
                except:
                    print("âŒ é‡æ–°è¿æ¥å¤±è´¥ï¼Œåœæ­¢è¿è¡Œ")
                    break
            else:
                failed_count += 1
                if failed_count >= MAX_FAILURES:
                    print(f"ğŸ›‘ å…¶ä»–é”™è¯¯å¯¼è‡´è¿ç»­å¤±è´¥ {MAX_FAILURES} æ¬¡ï¼Œåœæ­¢è¿è¡Œ")
                    break
            
            continue
    
    # ğŸŸ¢ å…³é—­è¿æ¥
    try:
        mail.close()
        mail.logout()
    except:
        pass
    
    # ... åç»­å¤„ç† pending_sources çš„é€»è¾‘ä¿æŒä¸å˜ ...
    
    MAX_PAPERS = 15
    to_process = pending_sources[:MAX_PAPERS]
    if not to_process:
        print("â˜• æš‚æ— å¾…å¤„ç†çš„æ–°æ–‡çŒ®ï¼Œä»»åŠ¡ç»“æŸã€‚")
        return
    
    print(f"ğŸ“‘ é˜Ÿåˆ—å·²å°±ç»ª: ä»Šæ—¥å°†åˆ†æ {len(to_process)} ç¯‡æ–°æ–‡çŒ®ã€‚")
    report_body, all_files, total_new, failed = "", [], 0, []
    
    for src in to_process:
        print(f"ğŸ“ æ­£åœ¨å¤„ç†ç¬¬ {total_new + len(failed) + 1} ç¯‡: {src.get('id', 'Document')}")
        content, ctype, path = fetch_content(src, save_dir=DOWNLOAD_DIR)
        if path:
            all_files.append(path)
        if content:
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨ LLM è¿›è¡Œå­¦æœ¯åˆ†æ...")
            ans = analyze_with_llm(content, ctype, src.get('url'))
            if "LLM åˆ†æå‡ºé”™" not in ans:
                report_body += f"## ğŸ“‘ {src.get('id', 'Paper')}\n\n{ans}\n\n---\n\n"
                processed_ids.append(get_unique_id(src))
                total_new += 1
                continue
        failed.append(src)
    
    print(f"ğŸ“Š åˆ†æé˜¶æ®µç»“æŸã€‚æˆåŠŸ: {total_new}, å¤±è´¥: {len(failed)}")
    
    final_report = f"# ğŸ“… æ–‡çŒ®æ—¥æŠ¥ {datetime.date.today()}\n\n" + report_body
    if total_new > 0 or failed:
        print("ğŸ“¨ æ­£åœ¨æ‰“åŒ…å¹¶å‘é€é‚®ä»¶...")
        zip_file = "papers.zip" if all_files else None
        if zip_file:
            with zipfile.ZipFile(zip_file, 'w') as zf:
                for f in all_files:
                    zf.write(f, os.path.basename(f))
        
        if send_email_with_attachment(f"ğŸ¤– AI å­¦æœ¯æ—¥æŠ¥ (æ–°:{total_new})", final_report, zip_file):
            print("ğŸ“§ é‚®ä»¶å‘é€æˆåŠŸï¼")
        else:
            print("âŒ é‚®ä»¶å‘é€å¤±è´¥ã€‚")
        
        if zip_file and os.path.exists(zip_file):
            os.remove(zip_file)
    
    save_history(processed_ids)
    print("ğŸ’¾ å†å²è®°å½•å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    main()
