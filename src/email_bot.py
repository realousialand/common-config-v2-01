import os
import re
import requests
import pymupdf4llm
from openai import OpenAI
from habanero import Crossref
import time
import hashlib
import json
import shutil
import zipfile
import socket
import imaplib
import email
import smtplib
import datetime
import random
from email.header import decode_header
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from urllib.parse import unquote, urlparse
import markdown

# --- ğŸ› ï¸ 1. æ ¸å¿ƒé…ç½®åŒº ---
LLM_API_KEY = os.environ.get("LLM_API_KEY")
LLM_BASE_URL = "https://api.siliconflow.cn/v1"
LLM_MODEL_NAME = os.environ.get("LLM_MODEL_NAME", "deepseek-ai/DeepSeek-R1-distill-llama-70b")

EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
IMAP_SERVER = "imap.gmail.com"
SMTP_SERVER = "smtp.gmail.com"

# ç›‘æ§å…³é”®è¯
TARGET_SUBJECTS = [
    "æ–‡çŒ®é¸Ÿ", "Google Scholar Alert", "ArXiv", "Project MUSE", 
    "new research", "Stork", "ScienceDirect", "Chinese politics", 
    "Imperial history", "Causal inference", "new results", "The Accounting Review",
    "recommendations available", "Table of Contents"
]

HISTORY_FILE = "data/history.json"
DOWNLOAD_DIR = "downloads"
MAX_ATTACHMENT_SIZE = 19 * 1024 * 1024
socket.setdefaulttimeout(30)

client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
cr = Crossref()

DOMAIN_LAST_ACCESSED = {}

# --- ğŸ¨ é‚®ä»¶æ ·å¼ ---
EMAIL_CSS = """
<style>
    body { font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; max-width: 800px; margin: 0 auto; padding: 20px; }
    h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; font-size: 24px; }
    h2 { color: #e67e22; margin-top: 30px; font-size: 20px; border-left: 5px solid #e67e22; padding-left: 10px; background-color: #fdf2e9; }
    .image-placeholder { background-color: #e8f6f3; border: 1px dashed #1abc9c; color: #16a085; padding: 15px; text-align: center; border-radius: 5px; margin: 20px 0; font-style: italic; }
</style>
"""

# --- ğŸ§  2. æ ¸å¿ƒæ¨¡å— ---

def get_oa_link_from_doi(doi):
    try:
        email_addr = "bot@example.com"
        r = requests.get(f"https://api.unpaywall.org/v2/{doi}?email={email_addr}", timeout=15)
        data = r.json()
        if data.get('is_oa') and data.get('best_oa_location'):
            return data['best_oa_location']['url_for_pdf']
    except: pass
    return None

def extract_titles_from_text(text):
    print("    ğŸ§  [æ™ºèƒ½æå–] æ­£åœ¨åˆ†æé‚®ä»¶æ­£æ–‡æå–æ ‡é¢˜...")
    prompt = f"""
    You are a research assistant. Extract the titles of academic papers from the email text below.
    Rules:
    1. Ignore generic text like "Table of Contents", "Read the full article", "Unsubscribe".
    2. Return ONLY a JSON list of strings. Example: ["Title 1", "Title 2"].
    3. Do not output Markdown.
    
    Email Text:
    {text[:5000]}
    """
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        content = completion.choices[0].message.content.strip()
        content = content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        print(f"    âš ï¸ æ ‡é¢˜æå–å¤±è´¥: {e}")
        return []

def search_doi_by_title(title):
    print(f"    ğŸ” [Crossref] æœç´¢ DOI: {title[:40]}...")
    try:
        results = cr.works(query=title, limit=1)
        if results['message']['items']:
            item = results['message']['items'][0]
            return item.get('DOI')
    except Exception as e:
        print(f"    âŒ DOI æœç´¢å¤±è´¥: {e}")
    return None

def extract_body(msg):
    """æå–çº¯æ–‡æœ¬å’Œæ‰€æœ‰ http é“¾æ¥"""
    body_text = ""
    extracted_urls = set()
    
    def find_urls_in_text(text):
        urls = re.findall(r'(https?://[^\s"\'<>]+)', text)
        return [u.rstrip('.,;)]}') for u in urls]

    if msg.is_multipart():
        for part in msg.walk():
            content_type = part.get_content_type()
            disposition = str(part.get("Content-Disposition"))
            try:
                payload = part.get_payload(decode=True)
                if not payload: continue
                part_text = payload.decode(errors='ignore')
                
                if "attachment" not in disposition:
                    if content_type == "text/html":
                        hrefs = re.findall(r'href=["\']([^"\']+)["\']', part_text, re.IGNORECASE)
                        extracted_urls.update(hrefs)
                        clean_text = re.sub('<[^<]+?>', ' ', part_text)
                        body_text += clean_text + "\n"
                    else:
                        body_text += part_text + "\n"
                
                raw_urls = find_urls_in_text(part_text)
                extracted_urls.update(raw_urls)
            except: continue
    else:
        try:
            payload = msg.get_payload(decode=True)
            if payload:
                text = payload.decode(errors='ignore')
                body_text += text
                extracted_urls.update(find_urls_in_text(text))
        except: pass
    
    return body_text, list(extracted_urls)

def detect_and_extract_all(text, all_links=None):
    results = []
    seen_ids = set()
    
    # 1. æ£€æµ‹ ArXiv
    for match in re.finditer(r"(?:arXiv:|arxiv\.org/abs/|arxiv\.org/pdf/)\s*(\d{4}\.\d{4,5})", text, re.IGNORECASE):
        aid = match.group(1)
        if aid not in seen_ids:
            results.append({"type": "arxiv", "id": aid, "url": f"https://arxiv.org/pdf/{aid}.pdf"})
            seen_ids.add(aid)
    
    # 2. æ£€æµ‹ DOI
    for match in re.finditer(r"(?:doi:|doi\.org/)\s*(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", text, re.IGNORECASE):
        doi = match.group(1)
        if doi not in seen_ids:
            oa_url = get_oa_link_from_doi(doi)
            results.append({"type": "doi", "id": doi, "url": oa_url})
            seen_ids.add(doi)
    
    # 3. å¢å¼ºç‰ˆé“¾æ¥åŒ¹é…
    ACADEMIC_DOMAINS = [
        'emerald.com', 'researchgate.net', 'wiley.com', 'sciencedirect.com', 
        'springer.com', 'tandfonline.com', 'sagepub.com', 'jstor.org', 'oup.com', 
        'cambridge.org', 'egrove.olemiss.edu'
    ]
    
    BLOCKED_DOMAINS = ['muse.jhu.edu', 'sciencedirect.com/science/article/pii']
    
    if all_links:
        for link in all_links:
            try:
                link = unquote(link)
                link_lower = link.lower()

                if any(x in link_lower for x in ['unsubscribe', 'privacy', 'manage', 'twitter', 'facebook']):
                    continue
                
                if any(blk in link_lower for blk in BLOCKED_DOMAINS):
                    continue

                is_pdf = link_lower.endswith('.pdf') or '/pdf/' in link_lower
                is_repo_pdf = 'viewcontent.cgi' in link_lower
                is_content_pdf = 'content/pdf' in link_lower
                is_academic_web = any(d in link_lower for d in ACADEMIC_DOMAINS)
                is_edu = '.edu' in urlparse(link).netloc
                
                if is_pdf or is_repo_pdf or is_content_pdf or is_academic_web or is_edu:
                    link_hash = hashlib.md5(link.encode()).hexdigest()[:10]
                    if link_hash not in seen_ids:
                        if "scholar_url?url=" in link:
                            match = re.search(r'url=([^&]+)', link)
                            if match: link = unquote(match.group(1))
                        
                        source_type = "direct_pdf" if (is_pdf or is_repo_pdf) else "academic_web"
                        results.append({
                            "type": source_type,
                            "id": f"link_{link_hash}",
                            "url": link
                        })
                        seen_ids.add(link_hash)
            except: continue
    
    return results

def polite_wait(url):
    try:
        if not url: return
        domain = urlparse(url).netloc
        last_time = DOMAIN_LAST_ACCESSED.get(domain, 0)
        cooldown = 5 + random.uniform(1, 3)
        if time.time() - last_time < cooldown:
            time.sleep(cooldown)
        DOMAIN_LAST_ACCESSED[domain] = time.time()
    except: pass

def fetch_content(source_data, save_dir=None):
    if source_data.get("type") == "arxiv":
        time.sleep(3)

    url = source_data.get("url")
    if not url: 
        if source_data.get("type") == "doi": return fetch_abstract_only(source_data)
        return None, "No URL", None

    polite_wait(url)
    print(f"    ğŸ” [æ¢æµ‹] æ­£åœ¨è®¿é—®: {url[:50]}...")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
    }

    try:
        r = requests.get(url, headers=headers, timeout=30, allow_redirects=True, stream=True)
        
        if r.status_code == 429:
            print("    ğŸ›‘ [429] è¯·æ±‚è¿‡å¤šï¼Œå†·å´ 60ç§’...")
            time.sleep(60)
            return None, "Rate Limited", None
            
        final_url = r.url
        content_type = r.headers.get('Content-Type', '').lower()
        
        is_pdf_response = (
            'application/pdf' in content_type or 
            final_url.endswith('.pdf') or 
            'viewcontent.cgi' in final_url
        )

        if is_pdf_response:
            print("    ğŸ“¥ ç¡®è®¤ PDF å†…å®¹ï¼Œå¼€å§‹ä¸‹è½½...")
            file_id = source_data.get('id') or hashlib.md5(url.encode()).hexdigest()[:10]
            safe_name = re.sub(r'[\\/*?:"<>|]', '_', file_id)
            filename = os.path.join(save_dir, f"{safe_name}.pdf")
            
            with open(filename, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # ğŸŸ¢ [æ–°å¢è¿‡æ»¤] æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œè¿‡æ»¤â€œå‡ PDFâ€
            file_size = os.path.getsize(filename)
            if file_size < 2000: # å°äº 2KB çš„é€šå¸¸æ˜¯é”™è¯¯é¡µé¢
                print(f"    âš ï¸ æ–‡ä»¶è¿‡å° ({file_size} bytes)ï¼Œç–‘ä¼¼æ— æ•ˆç½‘é¡µ/åçˆ¬æ‹¦æˆªï¼Œè·³è¿‡ã€‚")
                os.remove(filename)
                return None, "Fake PDF", None

            try:
                content = pymupdf4llm.to_markdown(filename)
                print(f"    âœ… PDF è§£ææˆåŠŸï¼Œé•¿åº¦: {len(content)}")
                return content, "PDF Full Text", filename
            except: 
                return None, "PDF Error", None

        elif 'text/html' in content_type:
            print("    ğŸŒ æ£€æµ‹åˆ°ç½‘é¡µï¼Œå°è¯•æå–æ­£æ–‡...")
            html_content = ""
            for chunk in r.iter_content(chunk_size=8192):
                html_content += chunk.decode(errors='ignore')
                if len(html_content) > 200000: break 
            
            text_content = re.sub(r'<script.*?>.*?</script>', '', html_content, flags=re.DOTALL)
            text_content = re.sub(r'<style.*?>.*?</style>', '', text_content, flags=re.DOTALL)
            text_content = re.sub(r'<[^<]+?>', '\n', text_content)
            text_content = re.sub(r'\n+', '\n', text_content).strip()
            
            # ç½‘é¡µå†…å®¹å¤ªçŸ­ä¹Ÿè¿‡æ»¤
            if len(text_content) < 500:
                print(f"    âš ï¸ ç½‘é¡µå†…å®¹è¿‡çŸ­ ({len(text_content)} chars)ï¼Œè·³è¿‡ã€‚")
                return None, "Content Too Short", None
            
            print(f"    âœ… ç½‘é¡µæ–‡æœ¬æå–æˆåŠŸï¼Œé•¿åº¦: {len(text_content)}")
            return text_content, "Web Page Text", None

    except Exception as e:
        print(f"    âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
        if source_data.get("type") == "doi": return fetch_abstract_only(source_data)

    if source_data.get("type") == "doi": return fetch_abstract_only(source_data)
    return None, "Unknown", None

def fetch_abstract_only(source_data):
    try:
        print(f"    ğŸ“š [ä¿åº•] æ­£åœ¨ä» Crossref è·å–æ‘˜è¦...")
        work = cr.works(ids=source_data["id"])
        title = work['message'].get('title', [''])[0]
        abstract = re.sub(r'<[^>]+>', '', work['message'].get('abstract', 'æ— æ‘˜è¦'))
        content = f"# {title}\n\n## Abstract\n{abstract}"
        return content, "Abstract Only", None
    except: return None, "Error", None

def analyze_with_llm(content, content_type, source_url=""):
    prompt = f"""è¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡çŒ®ã€‚æ¥æºï¼š{content_type}ã€‚åœ¨è§£é‡Šæœºåˆ¶æ—¶æ’å…¥ 

[Image of X]
 æ ‡ç­¾ã€‚è¾“å‡º Markdownã€‚\n---\n{content[:50000]}"""
    try:
        completion = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"LLM åˆ†æå‡ºé”™: {e}"

# --- ğŸ“§ 3. è¾…åŠ©åŠŸèƒ½ ---

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f: return json.load(f)
        except: return []
    return []

def save_history(history_list):
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f: json.dump(history_list, f, indent=2, ensure_ascii=False)

def get_unique_id(source_data):
    return source_data.get("id") or hashlib.md5(source_data.get("url", "").encode()).hexdigest()

def send_email_with_attachment(subject, body_markdown, attachment_zip=None):
    try:
        html_content = markdown.markdown(body_markdown, extensions=['extra', 'tables', 'fenced_code'])
    except:
        html_content = body_markdown
    
    # ğŸŸ¢ ä¿®å¤äº†ä½ è´´çš„ä»£ç é‡Œåæ‰çš„æ­£åˆ™
    try:
        def replacer(match):
            return f'<div class="image-placeholder">ğŸ–¼ï¸ å›¾ç¤ºå»ºè®®ï¼š{match.group(1)}</div>'
        html_content = re.sub(r'\]+)\]', replacer, html_content)
    except Exception as e:
        print(f"âš ï¸ ç¾åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ ¼å¼: {e}")
        pass
    
    final_html = f"<!DOCTYPE html><html><head><meta charset='UTF-8'>{EMAIL_CSS}</head><body>{html_content}<hr><p style='text-align:center;color:#888;font-size:12px;'>Generated by AI Research Assistant | {datetime.date.today()}</p></body></html>"
    msg = MIMEMultipart()
    msg["Subject"] = subject
    msg["From"] = EMAIL_USER
    msg["To"] = EMAIL_USER
    msg.attach(MIMEText(final_html, "html", "utf-8"))
    
    if attachment_zip and os.path.exists(attachment_zip):
        try:
            with open(attachment_zip, "rb") as f:
                part = MIMEApplication(f.read(), Name=os.path.basename(attachment_zip))
                part['Content-Disposition'] = f'attachment; filename="{os.path.basename(attachment_zip)}"'
                msg.attach(part)
        except: pass
    
    try:
        with smtplib.SMTP_SSL(SMTP_SERVER, 465) as server:
            server.login(EMAIL_USER, EMAIL_PASS)
            server.sendmail(EMAIL_USER, EMAIL_USER, msg.as_string())
        return True
    except Exception as e:
        print(f"å‘é€å¤±è´¥: {e}")
        return False

# --- ğŸš€ 4. ä¸»é€»è¾‘ ---

def main():
    print("ğŸ¬ ç¨‹åºå¯åŠ¨ä¸­...")
    if os.path.exists(DOWNLOAD_DIR): shutil.rmtree(DOWNLOAD_DIR)
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    
    processed_ids = load_history()
    mail = imaplib.IMAP4_SSL(IMAP_SERVER)
    mail.login(EMAIL_USER, EMAIL_PASS)
    mail.select("inbox")
    
    date_str = (datetime.date.today() - datetime.timedelta(days=1)).strftime("%d-%b-%Y")
    _, data = mail.search(None, f'(SINCE "{date_str}")')
    email_list = data[0].split()
    print(f"ğŸ“¨ æ£€ç´¢åˆ° {len(email_list)} å°é‚®ä»¶")
    
    pending_sources = []
    processed_count = 0
    
    for idx, e_id in enumerate(email_list):
        try:
            _, header_data = mail.fetch(e_id, "(BODY.PEEK[HEADER])")
            msg_header = email.message_from_bytes(header_data[0][1])
            subj, enc = decode_header(msg_header["Subject"])[0]
            subj = subj.decode(enc or 'utf-8') if isinstance(subj, bytes) else subj
            
            if not any(k.lower() in subj.lower() for k in TARGET_SUBJECTS):
                continue
            
            print(f"ğŸ¯ å‘½ä¸­: {subj[:30]}...")
            _, m_data = mail.fetch(e_id, "(RFC822)")
            msg = email.message_from_bytes(m_data[0][1])
            
            body_text, all_urls = extract_body(msg)
            print(f"    ğŸ“ æ‰«æåˆ° {len(all_urls)} ä¸ªé“¾æ¥")
            
            sources = detect_and_extract_all(body_text, all_urls)
            
            if not sources:
                print("    ğŸ’¡ æ— ç›´æ¥é“¾æ¥ï¼Œå°è¯• LLM æ ‡é¢˜æå–...")
                titles = extract_titles_from_text(body_text)
                for t in titles:
                    found_doi = search_doi_by_title(t)
                    if found_doi:
                        print(f"    âœ… åæŸ¥ DOI: {found_doi}")
                        oa_url = get_oa_link_from_doi(found_doi)
                        sources.append({"type": "doi", "id": found_doi, "url": oa_url})
                        time.sleep(1)

            for s in sources:
                if get_unique_id(s) not in processed_ids:
                    pending_sources.append(s)
            
            processed_count += 1
            if processed_count % 10 == 0:
                print("ğŸ›‘ æ‰¹æ¬¡ä¼‘æ¯ 5ç§’...")
                time.sleep(5)
                
        except Exception as e:
            print(f"âš ï¸ é”™è¯¯: {e}")
            continue

    MAX_PAPERS = 15
    to_process = pending_sources[:MAX_PAPERS]
    
    if not to_process:
        print("â˜• æ— æ–°æ–‡çŒ®ã€‚")
        return

    print(f"ğŸ“‘ å‡†å¤‡åˆ†æ {len(to_process)} ç¯‡æ–‡çŒ®...")
    report_body, all_files, total_new, failed = "", [], 0, []
    
    for src in to_process:
        print(f"ğŸ“ å¤„ç†: {src.get('id', 'Doc')}")
        content, ctype, path = fetch_content(src, save_dir=DOWNLOAD_DIR)
        
        if path: all_files.append(path)
        
        if content:
            print("ğŸ¤– AI åˆ†æä¸­...")
            ans = analyze_with_llm(content, ctype, src.get('url'))
            if "LLM åˆ†æå‡ºé”™" not in ans:
                report_body += f"## ğŸ“‘ {src.get('id', 'Paper')}\n\n{ans}\n\n---\n\n"
                processed_ids.append(get_unique_id(src))
                total_new += 1
                continue
        failed.append(src)
    
    final_report = f"# ğŸ“… æ–‡çŒ®æ—¥æŠ¥ {datetime.date.today()}\n\n" + report_body
    if total_new > 0 or failed:
        print("ğŸ“¨ å‘é€é‚®ä»¶...")
        zip_file = "papers.zip" if all_files else None
        if zip_file:
            with zipfile.ZipFile(zip_file, 'w') as zf:
                for f in all_files: zf.write(f, os.path.basename(f))
        
        send_email_with_attachment(f"ğŸ¤– AI å­¦æœ¯æ—¥æŠ¥ (æ–°:{total_new})", final_report, zip_file)
        if zip_file and os.path.exists(zip_file): os.remove(zip_file)
    
    save_history(processed_ids)
    print("ğŸ’¾ å†å²è®°å½•å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    main()
